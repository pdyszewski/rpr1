[{"path":"index.html","id":"section","chapter":"","heading":"","text":"Notatki te powstały na potrzeby kursu o tej samej nazwie, prowadzonego w Instytucie Matematycznym Uniwersytetu\nWrocławskiego w semestrze letnim 2024/2025.\nPierwsza wersja tych notatek jak plan merytoryczny wykładu zostały przygotowane przez DB w latach 2020-2024.\nInteraktywna wersja notatek została przygotowana przez PD w semestrze letnim roku akademickiego 2024/2025.Interaktywny charakter notatek był zainspirowany książkami Complex Analysis autorstwa Juana Carlosa Ponce Campuzano oraz Collision Detection Jeffreya Thompsona.Notatki te wciąż znajdują się wstępnej fazie mogą zawierać błędy. Jeśli zauważysz jakiekolwiek nieścisłości, niedociągnięcia lub inne problemy, gorąco zachęcam ich zgłaszania na\nGitHubie.napisania książki wykorzystałem bibliotekę bookdown z pakietu R, symulacje zostały stworzone przy użyciu javascript (p5.js). Rysunki w przeważającej większości są generowane przy użyciu TikZ.Wrocław, luty 2025Piotr Dyszewski","code":""},{"path":"sylabus.html","id":"sylabus","chapter":"Sylabus","heading":"Sylabus","text":"","code":""},{"path":"sylabus.html","id":"dane-dotyczące-przedmiotu","chapter":"Sylabus","heading":"Dane dotyczące przedmiotu","text":"Nazwa przedmiotu: Rachunek Prawdopodobieństwa 1RJednostka oferująca przedmiot: Instytut MatematycznyZałożenia: Analiza topologia R (28-MT-S-oAnTopR), (Kombinatoryka 28-MT-S-oKomb)Strona www: https://sites.google.com/site/piotrdyszewski/teaching/RPR1Forma zajęć: wykład + ćwiczeniaPunkty ECTS: 7Sprawdziany pisemne: 7.04 9.06","code":""},{"path":"sylabus.html","id":"skrócony-plan-wykładu","chapter":"Sylabus","heading":"Skrócony plan wykładu","text":"W trakcie wykładu poruszymy następujące zagadnienia:) Przestrzeń probabilistyczna: aksjomatyka rachunku prawdopodobieństwa, miara probabilistyczna,\nwłasności miary probabilistycznej, prawdopodobieństwo warunkowe,\nprawdopodobieństwo całkowite, wzór Bayesa, niezależność zdarzeń, lemat Borela-Cantellego;B) elementy losowe: zmienne losowe, wektory losowe, niezależne zmienne losowe;C) rozkłady prawdopodobieństwa:\nrozkłady zmiennych losowych, dystrybuanty, rozkłady dyskretne absolutnie ciągłe;D) parametry rozkładów: wartość oczekiwana, wariancja, kowariancja, funkcje charakterystyczne;E) twierdzenia graniczne: zbieżność zmiennych losowych, prawa wielkich liczb,\nprawo 0-1 Kołmogorowa mocne słabe prawo wielkich liczb, twierdzenie de Moivre’-Laplace’,\ncentralne twierdzenie graniczne.Podstawowa literatura wykładu:Durrett, R. (2019). Probability: theory examples (Vol. 49). Cambridge university press.Billingsley, P. (2017). Probability measure. John Wiley & Sons.Jakubowski, J., & Sztencel, R. (2001). Wstęp teorii prawdopodobieństwa. Script.","code":""},{"path":"sylabus.html","id":"szczegółowy-plan-wykładu","chapter":"Sylabus","heading":"Szczegółowy plan wykładu","text":"Wstępny plan tematów poruszanych na poszczególnych wykładach:Aksjomatyka rachunku prawdopodobieństwaPrawdopodobieństwo warunkowe, niezależność zdarzeńLemat Borela-CantellegoZmienne losoweRozkłady zmiennych losowychWektory losowe, niezależność zmiennych losowychParametry rozkładówNierówności związane z momentami, słabe prawo wielkich liczbZbieżność zmiennych losowych, prawo \\(0-1\\) KołmogorowaMocne prawo wielkich liczbTwierdzenie de Moivre’-Laplace’aZbieżność według rozkładuFunkcje charakterystyczneCentralne twierdzenie graniczneZastosowania CTG, rozkłady stabilne","code":""},{"path":"sylabus.html","id":"efekty-kształcenia","chapter":"Sylabus","heading":"Efekty kształcenia","text":"Po wykładzie student:Wymienia definiuje podstawowe obiekty teorii prawdopodobieństwa (, B, C, D );Podaje związki między podstawowymi obiektami teorii prawdopodobieństwa (, B, C, D );Wykorzystuje narzędzia teorii prawdopodobieństwa opisu zmiennych losowych w terminach ich rozkładu ich parametrów (B, C, D);Formułuje twierdzenia graniczne (D);Bada zmienne losowe pod kątem zależności (, B);Analizuje ciąg zmiennych losowych pod kątem różnych rodzajów zbieżności (B, E);Stosuje twierdzenia graniczne analizy ciągów zmiennych losowych (B, E);Stosuje nierówności lemat Borela-Cantelliego w analizie ciągów zmiennych losowych (, B, E);Stosuje metodę funkcji charakterystycznej dowodzenia twierdzeń granicznych (D, E).","code":""},{"path":"sylabus.html","id":"sposób-weryfikacji-efektów-kształcenia","chapter":"Sylabus","heading":"Sposób weryfikacji efektów kształcenia","text":"Na zaliczenie składać się będą:Aktywność na ćwiczeniach;Dwa sprawdziany pisemne (7.04 9.06).","code":""},{"path":"sylabus.html","id":"metody-i-kryteria-oceniania","chapter":"Sylabus","heading":"Metody i kryteria oceniania","text":"Zaliczenie ćwiczeń na sprawdzianów pisemnych\naktywności w czasie zajęć.\nOcena z egzaminu wystawiona jest na podstawie egzaminu pisemnego.","code":""},{"path":"sylabus.html","id":"warunkiem-zaliczenia-przedmiotu-jest","chapter":"Sylabus","heading":"Warunkiem zaliczenia przedmiotu jest:","text":"Uzyskanie 30% punktów za zadania stanowiące bieżącą weryfikację efektów kształcenia;Uzyskanie pozytywnej oceny z egzaminu stanowiącego końcową weryfikację efektów kształcenia.","code":""},{"path":"sylabus.html","id":"kryteria-ocen","chapter":"Sylabus","heading":"Kryteria ocen:","text":"(dst) student realizuje punkty 1-4 efektów kształcenia(db) student realizuje punkty 1-7 efektów kształcenia(bdb) student realizuje punkty 1-9 efektów kształceniaWrocław, luty 2025\nPiotr Dyszewski","code":""},{"path":"wprowadzenie.html","id":"wprowadzenie","chapter":"1 Wprowadzenie","heading":"1 Wprowadzenie","text":"Każdy wykład z rachunku prawdopodobieństwa zaczyna się ogólnym stwierdzeniem, że jest dział\nmatematyki zajmujący się badaniem zdarzeń losowych. Ważne jednak jest doprecyzowanie\no jaką analizę zdarzeń losowych chodzi jakiego rodzaju aparat matematyczny jest tego potrzebny.\nJeżeli ograniczymy się jedynie badania prawdopodobieństwa poszczególnych zdarzeń\ntakich jak jedenaście osób trafiło “szóstkę” w Lotto. Znajomość samej wartości liczbowej, na dodatek\nbardzo małej trudnej wyobrażenia, niezbyt wiele użytecznych informacji nie pozwoli udzielić\nodpowiedzi na wiele naturalnych pytań. Jak często dochodzi takich zdarzeń?\nJakie powinny być stawki za poszczególne zakłady aby Lotto było opłacalne dla Totalizatora\nSportowego?Okazuje się, że mimo iż nie mamy sposobu na przewidzenie wyniku zdarzenia czy eksperymentu\nlosowego, jesteśmy w stanie opisać pewne deterministyczne zależności między nimi.\nZależności te pozwalają między innymi dobrze zaplanować cennik Lotto.\nOmówimy teraz pokrótce kilka konkretnych przykładów aby dokładniej zobrazować zależności\no których będziemy w trakcie wykładu mówić.","code":""},{"path":"wprowadzenie.html","id":"krzywa-dzwonowa","chapter":"1 Wprowadzenie","heading":"1.1 Krzywa dzwonowa","text":"Pierwszy przykład pochodzi z Tokio, gdzie w październiku 2024 roku odbył\nsię Hakone Ekiden Yosenkai - półmaraton (bieg na 21.0975 km) kwalifikacyjny wyścigu\nHakone Ekiden (bieg sztafetowy). Oczywiście spodziewamy się, że znakomita większość\nzawodników uplasuje się w połowie stawki kilka najszybszych lub najwolniejszych osób będzie\nodstawało odpowiednio na jej początku końcu.\nOkazuje się jednak, że dokładny rozkład biegaczy jest bardzo regularny.\nZaobserwowana krzywa funkcja dzwonowa, która\njest równa\n\\[\\begin{equation*}\n    \\exp \\left(-x^2/(2\\sigma^2)\\right)/\\sqrt{2\\pi\\sigma^2}\n\\end{equation*}\\]\ndla pewnego parametru \\(\\sigma>0\\).\nW trakcie wykładu zobaczymy dokładnie skąd bierze się\nzaobserwowany kształt. Poniżej przedstawiamy\nwykres dla \\(\\sigma=1/2\\).","code":""},{"path":"wprowadzenie.html","id":"deska-galtona","chapter":"1 Wprowadzenie","heading":"1.2 Deska Galtona","text":"Istnieje wiele innych przykładów, w których obserwujemy krzywą dzwonową.\nAby zrozumieć jej pochodzenie warto przyjrzeć się najprostszym przykładom.\nJednym z nich jest deska Galtona zaprezentowana poniżej.\nKule spadają po kołkach za każdym razem odbijając się losowo w lewo bądź prawo.\nPoniżej widzimy histogram (znormalizowany wykres słupkowy)\nliczby kul, które opuściły deskę przez dane miejsce.Numery na histogramie mówią ile razy kula odbiła się od kołka w prawo.\nZauważmy, że po dłuższym czasie nasz histogram zaczyna przypominać krzywą dzwonową.\nPowyższe stanowi przykład Twierdzenia granicznego. Mimo, że nie jesteśmy w stanie przewidzieć wyniku\npojedynczego eksperymentu (nie wiemy gdzie dokładnie wyląduje ustalona kula),\nwiemy jak po uśrednieniu będzie zachowywał się cały system. W trakcie wykładu będziemy\nzajmować się opisem tego typu zjawisk.","code":""},{"path":"wprowadzenie.html","id":"jeszcze-jeden-przykład","chapter":"1 Wprowadzenie","heading":"1.3 Jeszcze jeden przykład","text":"Ideą twierdzeń granicznych jest wyodrębnienie deterministycznego\nstwierdzenia o losowym układzie.\nNie wszystkie twierdzenia graniczne muszą się wiązać z krzywą dzwonową.\nLosowe parkietaże, które teraz krótko omówimy, wiążą się z innym dobrze znanym kształtem.Dla naturalnego \\(n\\) rozważmy szachownicę wymiarów \\(2n\\times 2n\\) z której usunięto\ncztery rogi (równoramienne trójkąty prostokątne o ramieniu \\(n-1\\)).\nDiament dla \\(n=4\\) wygląda następująco.Dla naturalnego \\(n\\) chcemy pokryć diament kostkami domina o wymiarach\n\\(2\\times 1\\) oraz \\(1\\times 2\\). Poniżej znajduje się skrypt\ngenerujący losowy parkietaż diamentu rzędu \\(n\\).Widzimy, że dużych wartości \\(n\\) parkietaż wygląda bardzo regularnie. Rogi parkietażu są\nmonochromatyczne, co oznacza, że północny południowy róg są wyłożone kostkami poziomymi \nwschodni zachodni pionowymi. Widzimy też. że kolorowy fragment parkietażu\n(tam, gdzie widzimy zarówno kostki poziome jak kostki pionowe) zbiega okręgu.Dokładne opisanie powyższego zjawiska wykracza poza ramy tego wykładu. Zainteresowanym polecamy przeanalizowanie algorytmu generującego losowe permutacje (przycisk Auto). Algorytm ten tłumaczy dlaczego rogi\ndiamentu są monochromatyczne.","code":""},{"path":"wprowadzenie.html","id":"quiz","chapter":"1 Wprowadzenie","heading":"1.4 Quiz","text":"Niektóre sekcje w notatkach będą zakończone krótkim quizem aby czytelnik\nmógł sprawdzić swój poziom zrozumienia materiału.\nPrezentowane pytania będą zazwyczaj proste. Aby dokładnie zrozumieć materiał\nnależy przerobić listy zadań.","code":""},{"path":"aksjomatyka-rachunku-prawdopodobieństwa.html","id":"aksjomatyka-rachunku-prawdopodobieństwa","chapter":"2 Aksjomatyka rachunku prawdopodobieństwa","heading":"2 Aksjomatyka rachunku prawdopodobieństwa","text":"Omówimy podstawowe aksjomaty teorii prawdopodobieństwa. Zaczniemy jednak od przykładu\nilustrującego konieczność wprowadzenia matematycznego formalizmu.","code":""},{"path":"aksjomatyka-rachunku-prawdopodobieństwa.html","id":"paradoks-bertranda","chapter":"2 Aksjomatyka rachunku prawdopodobieństwa","heading":"2.1 Paradoks Bertranda","text":"W rachunku prawdopodobieństwa można wiele powiedzieć na poziomie intuicyjnym.\nOkazuje się jednak, że bez odpowiedniego formalizmu łatwo jest popaść w kłopoty.\nAby zilustrować rozważmy klasyczne zjawisko zwane paradoksem Bertranda.\nW okręgu o promieniu \\(1\\) wybrano losowo cięciwę \\(AB\\). Jakie jest prawdopodobieństwo,\nże będzie ona dłuższa niż bok trójkąta\nrównobocznego wpisanego w ten okrąg?Cięciwa wyznaczona jest przez dwa swoje końce. Skoro\nokrąg jest niezmienniczy na obroty, zawsze możemy umieścić\ninteresujący nas trójkąt równoboczny tak, aby jeden w końców\ncięciwy \\(\\) znajdował się dokładnie w wierzchołku trójkąta.\nWylosowanie cięciwy można więc utożsamić z wylosowaniem\ndrugiego punktu na okręgu.Cięciwa \\(AB\\) spełnia zadany warunek wtedy tylko wtedy, gdy\npunkt \\(B\\) trafia w łuk wyznaczony przez bok trójkąta\nprzeciwległy \\(\\). Skoro trójkąt dzieli okrąg na\ntrzy łuki równej długości, szukane prawdopodobieństwo wynosi \\(1/3\\).Istnieje inny sposób podejścia tego problemu. Cięciwa jest jednoznacznie wyznaczona przez\npołożenie swojego środka, więc wylosowanie jej jest równoważne z wylosowaniem jej środka.Cięciwa spełnia warunki zadania, gdy jej środek leży wnętrzu\nkoła o promieniu \\(1/2\\) tym samym środku. Prawdopodobieństwo, że\ncięciwa \\(AB\\) spełnia warunki zadania jest zatem\nrówne ilorazowi pół obu kół\n\\[\n    \\frac{|B(0,1/2)|}{|B(0,1)|}  = \\frac 14.\n\\]\nDokładne uzasadnienie słuszności powyższego wyrażenia wymaga wprowadzenia odpowiedniego\nformalizmu. Widzimy jednak, że o wiele większym problemem jest , że otrzymaliśmy\ninny wynik niż poprzednio.\nOkazuje się jednak, że można wprowadzić więcej chaosu.\nPodobnie jak w poprzednim przypadku chcemy wylosować środek cięciwy.\nZauważmy, że istotna jest jedynie jej długość, ta zależy tylko od odległości środka\ncięciwy od środka okręgu.\nPodobnie jak poprzednio wnioskujemy więc, że\n\\[\n    \\frac{|(0,1/2)|}{|(0,1)|}  = \\frac 12.\n\\]\nOtrzymaliśmy trzy różne odpowiedzi, co tłumaczy dlaczego zjawisko nazywane jest niekiedy\nparadoksem. Sprzeczność jest jednak pozorna. Jak się okazuje powyższe trzy metody losowania nie są\nrównoważne. Aby się o tym przekonać możemy powtórzyć powyższe eksperymenty wiele razy porównać rezultaty.\nJeżeli sto razy wylosujemy cięciwę przez wylosowanie jej końców na okręgu otrzymamy następujący rysunek.Widzimy, że w tym przypadku cięciwy umieszczone są równomiernie. Jeżeli natomiast wylosujemy sto\ncięciw poprzez wylosowanie ich środka otrzymamy więcej cięciw bliżej okręgu.Jeżeli wreszcie wylosujemy sto cięciw losując odległość od środka\nokręgu w sposób jednostajny, otrzymamy wiele cięciw blisko\nśrodka okręgu.Widzimy więc, że wspomniany paradoks jest pozorny, ponieważ przedstawione\nsposoby losowania cięciw nie są równoważne. Aby uniknąć tego typu niejasności\njak formalnie uzasadnić wyniki teoretyczne otrzymane przy trzech sposobach losowania,\nmusimy wprowadzić odpowiedni język opisu zdarzeń losowych.","code":""},{"path":"aksjomatyka-rachunku-prawdopodobieństwa.html","id":"przestrzeń-probabilistyczna","chapter":"2 Aksjomatyka rachunku prawdopodobieństwa","heading":"2.2 Przestrzeń probabilistyczna","text":"Za każdym razem kiedy rzucamy kością czy losujemy kartę z talii wykonujemy pewnego rodzaju\neksperyment losowy. W pierwszej części wykładu zajmiemy się ich badaniem.\nZaczniemy od sposobu matematycznej reprezentacji wyniku naszego eksperymentu.\nKażdy możliwy wynik eksperymentu nazywać będziemy zdarzeniem elementarnym.\nPrzykładowo dla rzutu kością sześcienną zdarzeniem elementarnym będzie\nliczba wyrzuconych oczek, czyli dowolna z liczb\n\\(1,2, \\ldots 6\\).\nZbiór wszystkich zdarzeń elementarnych oznaczać będziemy przez \\(\\Omega\\) \nbędziemy nazywać przestrzenią zdarzeń elementarnych, jego elementy,\nczyli zdarzenia elementarne będziemy oznaczać przez \\(\\omega\\).Przykład 2.1  Powiedzmy, że nasz eksperyment losowy polega na rzucie parą kości sześciennych.\nWynikiem naszego eksperymentu jest para elementów zbioru ze\n\\([6] = \\{1, 2, \\ldots , 6\\}\\).\nIstotne jest aby rozróżnić wyniki\notrzymane na obu kościach,\nwszak kości nie muszą być symetryczne. Zbiorem zdarzeń elementarnych jest\n\\[\n\\Omega = [6]^2=  \n\\{ (1,1), (1,2),\n\\ldots , (6,6) \\}.\n\\]\nW tym przypadku przestrzeń zdarzeń elementarnych jest skończona \nskłada się z \\(|\\Omega| = 36\\) elementów.Przykład 2.2  Tym razem rzucamy monetą momentu otrzymania orła.\nPrzestrzeń zdarzeń elementarnych \\(\\Omega\\) składa się ze skończonych ciągów \\(\\{O, R\\}\\)\nkończących się znakiem \\(O\\) jednego nieskończonego ciągu \\(R\\). Dokładniej\n\\[\\begin{align*}\n\\Omega & = \\{R\\}^{\\mathbb{N}} \\cup \\bigcup_{k=0}^\\infty\\{R \\}^k \\times \\{O\\}\n\\\\ & = \\{ RRRR \\ldots\\} \\cup \\{ O, RO, RRO, \\ldots  \\}.\n\\end{align*}\\]\nTutaj niekończony ciąg \\(RRR\\ldots\\) opisuje scenariusz, w którym nigdy nie wyrzucimy orła.Przykład 2.3  Losujemy liczbę z przedziału \\([0,1]\\). Wówczas wynikiem naszego losowania jest dokładnie\notrzymana liczba. Stąd \\(\\Omega=[0,1]\\).W większości eksperymentów losowych nie interesują nas poszczególne jego wyniki,\nlecz pewien ich wspólny aspekt. Dla przykładu w rzucie kością sześcienną istotna\ndla nas może być tylko parzystość uzyskanego wyniku.\nInnymi słowy bardziej od poszczególnych zdarzeń elementarnych interesować nas będę ich wyróżnione\nzbiory \\(\\subset \\Omega\\).\nTe interesujące nas zbiory nazywać będziemy zdarzeniami będziemy\noznaczać przez\n\\(,B,C\\) itd.\nDla określonego zbioru zdarzeń elementarnych \\(\\Omega\\)\nchcemy określić prawdopodobieństwo (miarę) \\(\\mathbb{P}\\).\nJeżeli \\(\\Omega\\) jest zbiorem skończonym lub przeliczalnym,\nzazwyczaj możemy przypisać prawdopodobieństwo dowolnemu podzbiorowi\n\\(\\Omega\\), więc dowolnemu elementowi\n\\(2^\\Omega\\), rodziny wszystkich podzbiorów \\(\\Omega\\).\nJeżeli \\(\\Omega\\) jest zbiorem nieprzeliczalnym, \\(2^\\Omega\\) jest zbyt duże\npojawiają się problemy ze zdefiniowaniem na \\(\\Omega\\) prawdopodobieństwa.\nJesteśmy zatem zmuszeni ograniczenia się pewnej\nszczególnej rodziny \\(\\mathcal{F} \\subseteq 2^\\Omega\\) zbiorów, dla których\njesteśmy w stanie określić prawdopodobieństwo.\nRodzina \\(\\mathcal{F}\\) powinna być zamknięta na wykonywanie podstawowych operacji na zbiorach.\nOkazuje się, że należy założyć, że \\(\\mathcal{F}\\) jest \\(\\sigma\\)-ciałem.Definicja 2.1  Rodzinę \\(\\mathcal{F}\\) podzbiorów \\(\\Omega\\) nazywamy \\(\\sigma\\)-ciałem, jeżeli\\(\\emptyset\\\\mathcal{F}\\);jeżeli \\(\\\\mathcal{F}\\), \\(^c\\\\mathcal{F}\\);jeżeli \\(A_1,A_2,\\ldots\\\\mathcal{F}\\), \\(\\bigcup_{=1}^\\infty A_i \\\\mathcal{F}\\).Parę \\((\\Omega,\\mathcal{F})\\) nazywamy przestrzenią mierzalną. Elementy \\(\\mathcal{F}\\)\nnazywamy zdarzeniami.Przykład 2.4  Wróćmy rzutu parą kości, gdzie przestrzenią zdarzeń elementarnych jest \\(\\Omega = [6]^2\\).\nWówczas zdarzeniem jest\n\\[\\begin{align*}\n& = \\{ \\mbox{suma oczek jest równa 7} \\} \\\\\n& = \\{ (1,6), (2,5),  \\ldots , (6,1)  \\} \\subseteq \\Omega.\n\\end{align*}\\]\nNatomiast zbiór\n\\[\n\\{ (1,1), (2,2) , \\ldots , (6,6)\\}\n\\]\njest zdarzeniem \\(\\{\\mbox{wypadł dublet}\\}\\).Przykład 2.5  Alex układa swoją dwudziestosześciotomową encyklopedię na półce.\nEksperyment polega na umieszczeniu poszczególnych tomów encyklopedii w losowej kolejności.\nWtedy zbiór zdarzeń elementarnych możemy utożsamić ze zbiorem permutacji. Dla \\(n \\\\mathbb{N}\\)\ndefiniujemy\n\\[\\begin{equation*}\n    S_{n} = \\left\\{ \\pi \\colon [n] \\[n]\\: : \\: \\mbox{$\\pi$ jest $1-1$ ''na''} \\right\\}.\n        \\end{equation*}\\]\nKażdy element \\(\\omega \\S_n\\) możemy utożsamić z macierzą\n\\[\\begin{equation*}\n\\omega = \\left( \\begin{array}{ccccc}\n1 & 2 & 3 & \\ldots & n\\\\\n\\omega(1) & \\omega(2) & \\omega(3) & \\ldots & \\omega(n) \\end{array} \\right),\n\\end{equation*}\\]\ngdzie \\(\\omega(1), \\omega(2), \\ldots , \\omega(n)\\) są parami różnymi liczbami ze zbioru\n\\([n]=\\{1,2,\\ldots n\\}\\).\nWówczas w naszym przykładzie \\(\\Omega=S_{26}\\).\nPrzykładowo ułożenie tomów w kolejności alfabetycznej odpowiada permutacji\nidentycznościowej \\({\\rm id} \\\\Omega\\) czyli takiej, że\\({\\rm id}()=\\) dla \\(\\[26]\\).\nInnymi słowy\n\\[\\begin{equation*}\n{\\rm id} =\n\\left( \\begin{array}{ccccc} 1 & 2 & 3 & \\ldots & n\\\\ 1 & 2 & 3 & \\ldots & n \\end{array} \\right).\n\\end{equation*}\\]Przykład 2.6  Andrzej układa sagę o Wiedźminie na półce.\nWówczas tomy możemy ponumerować liczbami \\(1,2,3,4,5\\) według daty wydania.\nZbiorem zdarzeń elementarnych jest zatem \\(\\Omega = S_5\\).\nRozważmy zdarzenie \\(\\) polegające na tym, że\npierwsze dwa tomy ułożone są chronologicznie.\nWówczas \\(\\) składa się z permutacji o reprezentacji\n\\[\\begin{equation*}\n\\left( \\begin{array}{ccccc} 1 & 2 & 3 & 4 & 5\\\\ 1 & 2 & * & * & * \\end{array} \\right).\n\\end{equation*}\\]Wprowadzone przez nas Definicja 2.1 zdarzenia jest niezwykle pojemna.\nWskutek czego jeżeli \\(A_1, A_2 \\ldots\\) jest ciągiem zdarzeń, czyli dla każdego \\(k \\\\mathbb{N}\\),\n\\(A_k\\) jest zdarzeniem, zdarzeniem jest również \\(\\bigcup_{k=1}^{\\infty} A_k\\)\noraz \\(\\bigcap_{k=1}^\\infty A_k\\).\nDodatkowo, jeżeli \\(\\) jest zdarzeniem, jest nim również\nzdarzenie przeciwne zdarzenia \\(\\), zadane wzorem\n\\[\n    ^c = \\Omega \\setminus .\n\\]\nZdarzenie \\(^c\\) polega na tym, że nie zachodzi zdarzenie \\(\\).\nJest jeszcze jedna relacja teoriomnogościowa, z której będziemy korzystać.\nPowiemy, że zdarzenie \\(B\\) pociąga zdarzenie \\(\\), jeżeli \\(B \\subseteq \\).Przykład 2.7  Powiedzmy, że rzucamy tylko jedną kością.\nWówczas \\(\\Omega = D = \\{ 1, 2, \\ldots , 6\\}\\). Niech\n\\[\\begin{align*}\n& = \\{\\mbox{wypadła liczba parzysta} \\} \\\\ & = \\{ 2, 4, 6\\}.\n\\end{align*}\\]\nZdarzenie przeciwne zdarzenia \\(\\) \n\\[\\begin{align*}\n^c & = \\{\\mbox{wypadła liczba nieparzysta} \\} \\\\ &= \\{ 1, 3, 5\\}.\n\\end{align*}\\]\nZ kolei zdarzenie\n\\[\nB = \\{\\mbox{wypadła szóstka} \\} = \\{ 6\\}\n\\]\npociąga zdarzenie \\(\\), czyli \\(B \\subseteq \\).Przykład 2.8  Losujemy punkt z przedziału \\([0,1]\\). Tak jak przyjęliśmy wcześniej \\(\\Omega=[0,1]\\).\nW przyszłości będziemy chcieli określić prawdopodobieństwo, że wylosowany punkt wpada \nustalonego przedziału. Przedziały postaci \\((,b)\\) będą więc dla nas zdarzeniami.\nZ kursu teorii miary wiemy, że wówczas będziemy w stanie zmierzyć każdy zbiór, który jest\nrezultatem przeliczalnych operacji mnogościowych na przedziałach otwartych. Oznacza , że\n\\(\\mathcal{F} =\\mathcal{B}([0,1])\\) jest \\(\\sigma\\)-ciałem zbiorów borelowskich na \\([0,1]\\),\nczyli najmniejszym \\(\\sigma\\)-ciałem zawierającym wszystkie zbiory otwarte.\nPamiętajmy, że \\(\\mathcal{B}([0,1]) \\neq 2^{[0,1]}\\). Istnieją zatem \\(\\subseteq \\Omega\\) takie, że\n\\(\\notin \\mathcal{F}\\). Są zbiory, których nie będziemy w stanie zmierzyć. Dla zdarzenia\n\\(\\\\mathcal{F}\\) mamy\n\\[\\begin{equation*}\n    = \\{ \\omega \\\\Omega \\: : \\: \\omega \\\\}.\n\\end{equation*}\\]\nJest więc zdarzenie polegające na tym, że wylosowana liczba \\(\\omega\\) pochodzi ze zbioru \\(\\).","code":""},{"path":"aksjomatyka-rachunku-prawdopodobieństwa.html","id":"miara-probabilistyczna","chapter":"2 Aksjomatyka rachunku prawdopodobieństwa","heading":"2.3 Miara probabilistyczna","text":"Jesteśmy przyzwyczajeni intuicyjnego myślenia o prawdopodobieństwie.\nDla przykładu w eksperymencie polegającym na rzucie dwoma kostkami od razu napiszemy, że\n\\[\\begin{equation*}\n    \\mathbb{P}\\left[ \\mbox{na obu kościach wypadnie $1$} \\right] =\n    \\frac 1{36}.\n\\end{equation*}\\]\nW pewnym sensie o liczbie \\(\\mathbb{P}[]\\) myślimy jak o prawdopodobieństwie zdarzenia \\(\\)\nopierając się na słownym opisie zdarzenia \\(\\). Nie ma w tym nic złego,\nlecz w wielu przypadkach wygodniej jest myśleć o zdarzeniu \\(\\) jak o podzbiorze \\(\\Omega\\) \no liczbie \\(\\mathbb{P}[]\\) jak o masie zbioru \\(\\).Definicja 2.2  Niech \\((\\Omega,\\mathcal{F})\\) będzie przestrzenią mierzalną.\nFunkcję \\(\\mathbb{P}:\\mathcal{F}\\[0,1]\\) nazywamy prawdopodobieństwem\n(miarą probabilistyczną) jeżeli\\(\\mathbb{P}[\\Omega] = 1\\);\\(\\mathbb{P}\\left[\\bigcup_{=1}^\\infty A_i\\right] = \\sum_{=1}^\\infty \\mathbb{P}[A_i]\\)\ndla dowolnych parami rozłącznych zdarzeń \\(A_1,A_2,\\ldots \\\\mathcal{F}\\).Trójkę \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) nazywamy przestrzenią probabilistyczną.Aksjomatyzacja rachunku prawdopodobieństwa była jednym z problemów Hilberta.\nPowyższa definicja została podana przez Kołmogorowa w 1933 roku zapoczątkowała\nwspółczesny rachunek prawdopodobieństwa.\nWcześniej badano głównie dyskretne przestrzenie probabilistyczne\n(np. w celu zrozumienia gier hazardowych).\nZ biegiem czasu, na przełomie dziewiętnastego \ndwudziestego wieku, zaczęto badać jednak znacznie bardziej skomplikowane modele\n(np. pochodzące z fizyki statystycznej) brakowało matematycznych narzędzi\nich precyzyjnego opisu. Okazało się, że powyższa definicja jest właściwym punktem wyjścia.\nNależy jednak mieć świadomość, że np. pokazanie istnienia miary probabilistycznej\njest nietrywialne (odpowiednia konstrukcja pokazywana jest podczas kursu z teorii miary bazuje na tw. Caratheodory’ego).Przykład 2.9  (Prawdopodobieństwo klasyczne) Niech \\(\\Omega\\)\nbędzie skończonym zbiorem, \\(\\mathcal{F} = 2^\\Omega\\).\nWówczas wystarczy zdefiniować prawdopodobieństwo na zdarzeniach elementarnych\n(zbiorach jednoelementowych). Zazwyczaj zakłada się, że są one jednakowo prawdopodobne,\nco prowadzi \n\\[\n    \\mathbb{P}[\\{\\omega\\}] = \\frac 1{|\\Omega|},\n\\]\ngdzie \\(|\\Omega|\\) oznacza liczbę elementów zbioru \\(\\Omega\\).\nWówczas dla dowolnego \\(\\subseteq \\Omega\\):\n\\[\\begin{equation}\n\\mathbb{P}[] = \\frac{||}{|\\Omega|}.\n\\tag{2.1}\n\\end{equation}\\]Przykład 2.10  Rzucamy trzykrotnie symetryczną monetą. Interesuje nas prawdopodobieństwo, że\ndokładnie \\(2\\) razy wypadła reszka.\nMamy\n\\[\\begin{multline*}\n  \\Omega=\\left\\{ (O,O,O), (O,O,R), (O,R,O), (R,O,O),\\right. \\\\\n  \\left. (O,R,R), (R,O,R), (R,R,O), (R,R,R)   \\right\\}\n\\end{multline*}\\]\noraz \\(\\mathcal{F}= 2^{\\Omega}\\).\nSkoro moneta jest symetryczna, każdy wynik jest jednakowo prawdopodobny.\nZa miarę probabilistyczną \\(\\mathbb{P}\\)\nprzyjmujemy zatem prawdopodobieństwo klasyczne dane wzorem (2.1).\nInteresuje nas prawdopodobieństwo zdarzenia\n\\[\\begin{equation*}\n  = \\left\\{  (O,R,R), (R,O,R), (R,R,O)   \\right\\}.\n\\end{equation*}\\]\nSkoro \\(\\) jest trójelementowy, \\(\\mathbb{P}() = 3/8\\).Przykład 2.11  Jeśli rzucamy kością sześcienną, zbiorem zdarzeń elementarnych jest\n\\(\\Omega =[6]= \\{ 1, 2, \\ldots , 6\\}\\). Jeżeli kość jest dobrze wyważona, \nopisu tego doświadczenia użyjemy prawdopodobieństwa klasycznego \\(\\mathbb{P}_1\\) takiego, że\n\\[\n\\mathbb{P}_0[\\{ 1 \\}] = \\mathbb{P}_0[\\{ 2 \\}] = \\ldots = \\mathbb{P}_0[\\{ 6\\}] = 1/6.\n\\]\nMożemy też zakładać, że jedna kość jest niewyważona, przykładowo\n\\[\\begin{align*}\n\\mathbb{P}_1[\\{2\\}] & = \\mathbb{P}_1[\\{6\\}]= 1/4 \\\\\n\\mathbb{P}_1[\\{ 1\\}] & = \\mathbb{P}_1[\\{ 3\\}] = \\mathbb{P}_1[\\{4\\}] = \\mathbb{P}_1[\\{ 5 \\}] = 1/8.\n\\end{align*}\\]\nMiary \\(\\mathbb{P}_0\\) \\(\\mathbb{P}_1\\) są różne te zdarzenia będą miały względem nich\nróżne prawdopodobieństwa. Dla przykładu dla zdarzenia\n\\[\n= \\{\\mbox{wypadła liczba parzysta}\\} = \\{2, 4, 6\\}\n\\]\notrzymujemy \\(\\mathbb{P}_0[] = 1/2\\) oraz \\(\\mathbb{P}_1[]=5/8\\).Przykład 2.12  Załóżmy, że \\(\\Omega = \\{\\omega_1,\\omega_2,\\ldots,\\}\\)\njest zbiorem przeliczalnym niech \\(p_1,p_2,\\ldots\\) będą\nliczbami nieujemnymi o sumie \\(1\\).\nWtedy wybór \\(\\mathcal{F}=2^\\Omega\\) oraz \\(\\mathbb{P}[\\{\\omega_i\\}] = p_i\\) dla \\(\\\\mathbb{N}\\)\njednoznacznie definiuje miarę probabilistyczną. Mianowicie dla każdego\n\\(\\\\mathcal{F}\\) mamy\n\\[\\begin{equation}\n  \\mathbb{P}[] = \\sum_{: \\omega_i\\} p_i = \\sum_i p_i {\\bf 1}_A(\\omega_i).\n  \\tag{2.2}\n  \\end{equation}\\]\nTutaj \\({\\bf 1}_A\\) jest indykatorem zbioru \\(\\) zadanym przez\n\\[\\begin{equation*}\n{\\bf 1}_A(\\omega) = \\left\\{ \\begin{array}{cc} 1 & \\omega \\\\\\ 0 & \\omega\\notin \\end{array}\n\\right. .\n\\end{equation*}\\]\nNa tym przykładzie widać, że o prawdopodobieństwie \\(\\mathbb{P}\\) można (czasami nawet trzeba)\nmyśleć jak o masie. Każdy z punktów \\(\\Omega\\) ma pewną masę (punkt \\(\\omega_k\\) ma masę \\(p_k\\)).\nWówczas (2.2) jest po prostu zliczeniem masy wszystkich punktów znajdujących się\nw zbiorze \\(\\).Omówione tej pory przykłady dotyczą skończonych bądź przeliczalnych przestrzeni probabilistycznych.\nZdefiniowanie na nich prawdopodobieństwa jest stosunkowo łatwe, ponieważ można zrobić\ndla każdego punktu z osobna. Wprowadzona przez nas Definicja 2.2\njest na tyle pojemna, aby dopuścić przypadki większych przestrzeni. Z takimi\nprzestrzeniami będziemy się spotykali niejednokrotnie w trakcie wykładu. Najprostszą z\nnich jest odcinek jednostkowy.Przykład 2.13  (Wybór losowego punktu z odcinka [0,1]) Powiedzmy, że chcemy zdefiniować przestrzeń probabilistyczną \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\)\nopisującą losowanie punktu z odcinka \\([0,1]\\). Wówczas możemy przyjąć\n\\(\\Omega=[0,1]\\) oraz \\(\\mathcal{F}=\\mathcal{B}[0,1]\\) rodzinę zbiorów borelowskich \\([0,1]\\).\nJest najmniejsze \\(\\sigma\\)-ciało podzbiorów \\([0,1]\\) zawierające wszystkie otwarte podzbiory \\([0,1]\\).\nChcemy na \\((\\Omega, \\mathcal{F})\\) wprowadzić takie prawdopodobieństwo \\(\\mathbb{P}\\) aby (luźno mówiąc)\nwylosowanie każdego punktu było jednakowo prawdopodobne. Formalnie nie można\ntego zrobić wprost. Dokładniej nie jest możliwe zdefiniowanie prawdopodobieństwa tak,\naby punkty należące odcinka \\([0,1]\\) miały takie samo, niezerowe prawdopodobieństwo.\nWłaściwe jest pytanie o wylosowanie zbioru (borelowskiego).\nSzukamy prawdopodobieństwa takiego aby prawdopodobieństwo wylosowania (miara zbioru) postaci\n\\([, +\\epsilon)\\) nie zależała od \\(\\[0,1-\\epsilon)\\) (lub równoważnie prawdopodobieństwo było\nniezmiennicze na przesunięcia).\nJedynym wyborem jest \\(\\mathbb{P}\\) będące miarą Lebesgue’na \\([0,1]\\).\nJak widzimy na symulacji poniżej, rozrzut punktów w takim modelu\nrzeczywiście jest jednostajny na całym odcinku.Powyżej widzimy symulacje \\(100\\) punktów wylosowanych z \\([0,1]\\)\nw sposób jednostajny.\nZauważmy, że wówczas dla każdego \\(x\\[0,1]\\), \\(\\mathbb{P}[\\{x\\}] = 0\\).Przykład 2.14  Możemy powtórzyć poprzedni przykład dla równoramiennego trójkąta prostokątnego\n\\(T \\subseteq \\mathbb{R}^2\\). Wówczas \\(\\Omega=T\\), \\(\\mathcal{F}=\\mathcal{B}(T)\\).\nChcąc ponownie losować w sposób jednostajny za \\(\\mathbb{P}\\) wybierzemy\nodpowiednio unormowaną dwuwymiarową miarę Lebesgue’(tak aby \\(\\mathbb{P}[\\Omega]=1\\)).Przykład 2.15  Wróćmy jeszcze na chwilę, losowania liczb z przedziału \\([0,1]\\). Przypuśćmy jednak, że\n\\[\\begin{equation*}\n\\mathbb{P}[] = 2 \\int_A s \\: \\mathrm{d}s\n\\end{equation*}\\]\ndla dowolnego \\(\\\\mathcal{F}\\).Przy takim wyborze prawdopodobieństwa faworyzujemy\npunkty bliżej prawego końca odcinka. Dla \\(x, \\epsilon \\(0,1)\\) takich, że \\(x+\\epsilon<1\\)\nmamy bowiem\n\\[\\begin{equation*}\n    \\mathbb{P}[(x, x+\\epsilon)] = (x+\\epsilon)^2-x^2 = 2x\\epsilon +\\epsilon\n\\end{equation*}\\]\nco jest rosnącą funkcją \\(x\\).\nWidzimy też na poniższej symulacji.Twierdzenie 2.1  (Podstawowe własności prawdopodobieństwa) Niech \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) będzie przestrzenią\nprobabilistyczną oraz \\(,B,A_1,A_2,\\ldots \\\\mathcal{F}\\). Wtedy\\(\\mathbb{P}[\\emptyset] = 0\\).\\(\\mathbb{P}[^c] = 1-\\mathbb{P}[]\\).Jeżeli \\(\\subset B\\), \\(\\mathbb{P}[B\\setminus ] = \\mathbb{P}[B] - \\mathbb{P}[]\\),\nw szczególności \\(\\mathbb{P}[]\\le \\mathbb{P}[B]\\).\\(\\mathbb{P}[\\cup B] = \\mathbb{P}[] + \\mathbb{P}[B] - \\mathbb{P}[\\cap B]\\).\\(\\mathbb{P}[\\bigcup_{=1}^\\infty A_i] \\le \\sum_{=1}^\\infty \\mathbb{P}[A_i]\\).Proof. Pozostawiamy jako zadaniePrzykład 2.16  Romeo Julia umówili się na spotkanie między \\(12\\) \\(1\\) w nocy. Nie ustalili jednak\ndokładnej godziny spotkania. Osoba, która przyjdzie jako pierwsza może czekać co najwyżej\n\\(15\\) minut na drugą. Jakie jest prawdopodobieństwo, że dojdzie spotkania?\nKładziemy \\(\\Omega = [0,1]^2\\). Wówczas \\(\\omega = (x,y) \\\\Omega\\) reprezentuje czas\nprzyjścia pierwszej osoby przez \\(x\\) drugiej przez \\(y\\). \\(\\mathcal{F} = \\mathcal{B}([0,1]^2)\\).\nZdarzenie, że dojdzie spotkania \n\\[\\begin{equation*}\n    = \\{ \\omega= (x,y) \\\\Omega \\: : \\: |x-y|\\leq 1/4\\}.\n\\end{equation*}\\]\nZ reprezentacji graficznejłatwo znajdujemy prawdopodobieństwo zdarzenia przeciwnego \\(\\mathbb{P}[^c] = 9/16\\).\nStąd \\(\\mathbb{P}[] = 1-\\mathbb{P}[^c]=7/16\\).Przykład 2.17  Losujemy jedną z liczb \\(1, 2, \\ldots 1000\\) w taki sposób, że wylosowanie każdej z\nnich jest jednakowo prawdopodobne. Wówczas przestrzenią zdarzeń elementarnych jest\n\\[\\begin{equation*}\n\\Omega = \\{ 1, 2, \\ldots , 1000\\}.\n\\end{equation*}\\]\nJakie jest prawdopodobieństwo, że wylosowana liczba dzieli się przez \\(3\\) lub \\(5\\)?\nDla liczby \\(p\\) oznaczmy zdarzenie\n\\[\\begin{equation*}\n    D_{p} = \\{ n \\[1000] \\: | \\: \\mbox{$n$ dzieli się przez $p$} \\} \\subseteq \\Omega.\n\\end{equation*}\\]\nSzukamy \\(\\mathbb{P}[D_3\\cup D_5]\\). Zauważmy, że\n\\[\\begin{equation*}\n    D_{3} = \\{ 3m \\: | \\: 1\\leq m \\leq 333 \\},\n\\end{equation*}\\]\nwięc \\(\\mathbb{P}[D_3] = 333/1000\\). Podobnie \\(\\mathbb{P}[D_5] =1/5\\).\nSkoro \\(D_3 \\cap D_5 = D_{15}\\), \\(\\mathbb{P}[D_3 \\cap D_5] = 33/500\\).\nOstatecznie\n\\[\\begin{align*}\n    \\mathbb{P}[D_3 \\cup D_5] & = \\mathbb{P}[D_3]+\\mathbb{P}[D_5]-\\mathbb{P}[D_3\\cap D_5] \\\\&=\n    467/1000.\n\\end{align*}\\]Punkt 4. powyższego twierdzenia szczególny przypadek zasady włączeń wyłączeń.\nTen punkt można uogólnić na dowolną skończoną liczbę zbiorów. Dla trzech zbiorów mamy:\n\\[\\begin{align*}\n\\mathbb{P}[\\cup B\\cup C] & = \\mathbb{P}[]  +\\mathbb{P}[B] +\\mathbb{P}[C] + \\\\\n& - \\mathbb{P}[\\cap B] - \\mathbb{P}[\\cap C] - \\mathbb{P}[B\\cap C]\\\\ & + \\mathbb{P}[\\cap B\\cap C].\n\\end{align*}\\]Twierdzenie 2.2  (Zasada włączeń wyłączeń) Niech \\(n \\\\mathbb{N}\\). Jeżeli \\(A_1,A_2,\\ldots, A_n\\\\mathcal{F}\\), \n\\[\\begin{multline*}\n  \\mathbb{P}[A_1\\cup A_2\\cup \\ldots \\cup A_n] = \\sum_{=1}^n\\mathbb{P}[A_i]+ \\\\\n- \\sum_{<j}\\mathbb{P}[A_i\\cap A_j] + \\sum_{<j<k}\\mathbb{P}[A_i\\cap A_j\\cap A_k]\n  \\\\ + \\ldots +(-1)^{n+1} \\mathbb{P}[A_1\\cap A_2\\cap \\ldots \\cap A_n]\n  \\end{multline*}\\]Proof. Pozostawiamy jako ćwiczenie.Twierdzenie 2.3  (Twierdzenie o ciągłości) Niech \\((\\Omega, \\mathcal{F}, \\mathbb{P},\\) będzie przestrzenią probabilistyczną.\nRozważmy nieskończony ciąg zdarzeń \\(A_1,A_2,\\ldots \\\\mathcal{F}\\).Jeżeli ciąg \\(\\{A_n\\}_{n \\\\mathbb{N}}\\) jest wstępujący (tzn. \\(A_1\\subset A_2 \\subset \\ldots\\))\n\\(=\\bigcup_{=1}^\\infty A_i\\), \n\\[\n\\mathbb{P}[] = \\lim_{n\\\\infty} \\mathbb{P}[A_n].\n\\]Jeżeli ciąg \\(\\{A_n\\}_{n \\\\mathbb{N}}\\) jest zstępujący\n(tzn. \\(A_1\\supset A_2 \\supset \\ldots\\)) \\(=\\bigcap_{=1}^\\infty A_i\\), \n\\[\n\\mathbb{P}[] = \\lim_{n\\\\infty} \\mathbb{P}[A_n].\n\\]Proof. Punkt 1. Niech \\(B_1 = A_1\\), \\(B_2 = A_2\\setminus A_1\\), …, \\(B_n = A_n\\setminus A_{n-1}\\),\nwtedy zdarzenia \\(B_n\\) są rozłączne,\n\\[\n\\bigcup_{=1}^n B_i = A_n \\quad \\mbox{oraz } \\quad \\bigcup_{=1}^\\infty B_i =  .\n\\]\nNa mocy punktu drugiego Definicji 2.2 otrzymujemy\n\\[\\begin{align*}\n\\mathbb{P}[] & = \\mathbb{P}\\left[\\bigcup_{=1}^\\infty B_i\\right] =\n\\sum_{=1}^\\infty \\mathbb{P} [ B_i ] \\\\&= \\lim_{n\\\\infty} \\sum_{=1}^n \\mathbb{P}[B_i]\n= \\lim_{n\\\\infty} \\mathbb{P}[A_n].\n\\end{align*}\\]\nPunkt 2. Niech \\(C_i = A_i^c\\). Wtedy rodzina zdarzeń ciąg \\(\\{C_n\\}_{n \\\\mathbb{N}}\\)\njest wstępujący. Z prawa De Morgana\n\\[\n    \\bigcup_{=1}^\\infty C_i = \\bigcup_{=1}^\\infty ^c_i\n    = \\left( \\bigcap_{=1}^\\infty A_i \\right)^c = ^c.\n\\]\nKorzystając zatem z udowodnionego już punktu 1 otrzymujemy\n\\[\\begin{multline*}\n\\mathbb{P}[] = 1 - \\mathbb{P}[^c] = 1 - \\mathbb{P}[C] = \\\\\n1 - \\lim_{n\\\\infty} \\mathbb{P}[C_n] = \\lim_{n\\\\infty}\\mathbb{P}[\\Omega\\setminus C_n]\n= \\lim_{n\\\\infty} \\mathbb{P}[A_n].\n\\end{multline*}\\]Przykład 2.18  urny wrzucamy nieskończenie wiele kul o numerach \\(1,2,\\ldots\\) w\nnastępujący sposób (w każdym kroku wybieramy ustaloną opcję ), b) lub c)):o godz. (\\(12.00 - 1\\) min.) wrzucamy kule \\(1,2,\\ldots, 10\\);o godz. (\\(12.00 - 1/2\\) min.) wyciągamy ) kulę 10; b) kulę 1; c) losową kulę,\nnastępnie wrzucamy kule \\(11,12,\\ldots, 20\\);o godz. (\\(12.00 - 1/4\\) min.) wyciągamy ) kulę 20; b) kulę 2; c) losową kulę,\n~następnie\nwrzucamy kule \\(21,22,\\ldots, 30\\);…Ile kul będzie w urnie o godz. 12?\nJasne jest, że jeżeli zastosujemy opcję ), o 12 w urnie będzie nieskończenie wiele kul.\nTroszeczkę bardziej niepokojący jest fakt, że jeżeli zastosujemy opcję b) urna będzie pusta.Rozpatrzmy przypadek c). Niech \\(A_n\\) będzie zdarzeniem, że po \\(n\\) krokach kula \\(1\\)\njest wciąż w urnie (oczywiście \\(A_{n+1}\\subset A_n\\)). Wówczas\n\\[\n\\mathbb{P}[A_{n+1}]\n= \\frac{9}{10}\\cdot\\frac{18}{19}\\cdot\\frac{27}{28}\\cdot \\ldots  \\cdot\\frac{9n}{9n+1}.\n\\]\nChcemy obliczyć \\(\\mathbb{P}[\\bigcap_{n=1}^{\\infty}A_n]\\), więc prawdopodobieństwo,\nże kula \\(1\\) pozostanie w urnie. Z Twierdzenia 2.3 o ciągłości\nwynika\n\\[\n\\mathbb{P}\\bigg[\\bigcap_{n=1}^{\\infty}A_n\\bigg]\n= \\lim_{n\\\\infty} \\mathbb{P}[A_n]\n= \\prod_{n=1}^\\infty \\frac{9n}{9n+1}.\n\\]\nPozostaje więc policzenie powyższego iloczynu.\nW tym celu piszemy, dla ustalonej liczby \\(N\\),\n\\[\\begin{align*}\n\\bigg(\\prod_{n=1}^\\infty \\frac{9n}{9n+1}\\bigg)^{-1}\n& = \\prod_{n=1}^\\infty \\bigg( 1 + \\frac 1{9n} \\bigg)\n\\\\ & > \\prod_{n=1}^N \\bigg( 1 + \\frac 1{9n} \\bigg) \\\\ & > \\frac 19 + \\frac 1{18} +\\ldots + \\frac 1{9N}\n\\\\ & = \\frac 19 \\sum_{=1}^N \\frac 1i.\n\\end{align*}\\]\nPowyższe wyrażenie zbiega \\(\\infty\\), gdy \\(N\\\\infty\\).\nZatem szukane prawdopodobieństwo wynosi \\(0\\).\nPodobne obliczenia można wykonać dla kuli o dowolnym numerze.\npokazuje, że z prawdopodobieństwem \\(1\\) urna o godz. 12 będzie pusta.","code":""},{"path":"prawdopodobieństwo-warunkowe.html","id":"prawdopodobieństwo-warunkowe","chapter":"3 Prawdopodobieństwo warunkowe","heading":"3 Prawdopodobieństwo warunkowe","text":"Rozważmy pewien górnolotny przykład. Powiedzmy, że w chwili \\(0\\) zakupiliśmy opcję kupna na wykupienie\nakcji spółki w chwili \\(1\\) po określonej kwocie (taka opcja nazywana jest opcją Europejską). Jesteśmy zatem w sytuacji,\nktórej interesuje nas faktyczna wartość akcji tej spółki.Załóżmy, że w chwili \\(0.8\\) chcemy ten kontrakt sprzedać. Wówczas wyceny takiego kontraktu\nużywać będziemy historii ceny akcji na przedziale \\([0,0.8]\\). Innymi słowy będziemy musieli odpowiedzieć na pytanie,\njak zachowanie ceny na przedziale \\([0,0.8]\\) wpływa na wartość akcji w chwili \\(1\\)?\nWykres ceny akcji na przedziałach \\([0,0.8]\\) \\([0.8,1]\\) możemy traktować jak dwa zależne od siebie eksperymenty\nlosowe. Jak więc informacja o pierwszym wpływa na prawdopodobieństwa poszczególnych wyników w drugim?Pierwszym krokiem w kierunku takich zaawansowanych zastosowań rachunku prawdopodobieństwa jest\nzrozumienie pojęcia prawdopodobieństwa warunkowego.","code":""},{"path":"prawdopodobieństwo-warunkowe.html","id":"podstawowe-definicje","chapter":"3 Prawdopodobieństwo warunkowe","heading":"Podstawowe definicje","text":"Rozważmy rzut dwiema kośćmi sześciennymi. Wiemy już, że odpowiadająca temu doświadczeniu przestrzeń zdarzeń elementarnych \\[\n\\Omega = [6]^2=  \\{ (1,1), (1, 2),  \\ldots , (6,6) \\}.\n\\]\nJeżeli rozważymy zdarzenie \\(=\\) suma oczek na obu kościach wynosi \\(6\\), \\(\\) jest zbiorem danym przez\n\\[\n=  \\{ (1,5), (2, 4), (3,3), (4,2), (5,1) \\}.\n\\]\nZakładać będziemy, że kości są dobrze wyważone.\nopisu tego eksperymentu posłużymy się prawdopodobieństwem \\(\\mathbb{P}\\), które każdemu zdarzeniu\nelementarnemu przypisuje takie samo prawdopodobieństwo.\nWówczas \\(\\mathbb{P}[] = 5/36\\).\nZałóżmy teraz, że posiadamy dodatkową informację, że na pierwszej kości wypadło jedno oczko.\nWówczas powinniśmy zmienić rozważaną przestrzeń zdarzeń elementarnych, mianowicie\n\\[\nB=\\{(1,1), (1,2), (1,3), (1,4), (1,5), (1,6)\\}\n\\]\nMusimy zmienić również sposób przypisywania prawdopodobieństwa na \\(\\mathbb{P}_1\\) przypisujące każdemu\nzdarzeniu elementarnemu z \\(B\\) takie samo prawdopodobieństwo.\nWówczas zdarzenie \\(A_1=\\) suma oczek jest równa \\(6\\) \n\\[\nA_1 = \\cap B =\\{ (1,5)\\}\n\\]\nco daje \\(\\mathbb{P}_1[A_1] = 1/6\\). Zauważmy, że\n\\[\\begin{align*}\n\\mathbb{P}_1[A_1] & = \\mathbb{P}_1[\\cap B ]= \\frac{|\\cap B|}{ |B| } \\\\ & =\n\\frac{|\\cap B| \\cdot |\\Omega|^{-1}}{ |B| \\cdot |\\Omega|^{-1} } = \\frac{\\mathbb{P}[\\cap B]}{\\mathbb{P}[B]}.  \n\\end{align*}\\]\nOkazuje się, że prawdopodobieństwo \\(\\mathbb{P}_1\\) w nowej przestrzeni probabilistycznej można wyrazić w\nterminach pierwotnie rozważanego prawdopodobieństwa \\(\\mathbb{P}\\).\nTa konstrukcja jest spotykana tak często, że wyrażenie występujące po prawej stronie ostatniego\nwzoru ma swoją specjalną nazwę.Definicja 3.1  Niech \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) będzie przestrzenią probabilistyczną. Rozważmy zdarzenie \\(B\\)\ntakie, że \\(\\mathbb{P}[B]>0\\). Prawdopodobieństwem\nwarunkowym (zajścia) zdarzenia \\(\\) pod warunkiem (zajścia) zdarzenia \\(B\\) nazywamy liczbę\n\\[\n\\mathbb{P}[|B] = \\frac{\\mathbb{P}[\\cap B]}{\\mathbb{P}[B]}.\n\\]Przy ustalonym zbiorze \\(B\\), \\(\\mathbb{P}[\\cdot|B]\\) jest miarą probabilistyczną na \\((\\Omega,\\mathcal{F})\\).Prawdopodobieństwo warunkowe jest jednym z ważniejszych pojęć teorii prawdopodobieństwa.\nRzadko zdarza się, aby doświadczenie było wykonywane w idealnych warunkach zazwyczaj jest\nono obarczone zewnętrznymi czynnikami, pewną dodatkową informacją.\nJest wykorzystywane np. przez firmy ubezpieczeniowe (polisa samochodu zależy np. od płci wieku kierowcy;\npolisa na życie od wieku, przebytych chorób, ryzyka zawodowego), graczy giełdowych.\nDla przykładu, wysokość emerytury powinna zależeć (oprócz zgromadzonych środków) od przewidywanej długości życia emeryta,\nco z kolei zależy od płci (mężczyźni 73,8; kobiety 81,7 - dane wg GUS za 2018r.),\nale też aktualnego wieku (przeciętny 60 latek, niezależnie od płci, będzie żył jeszcze średnio 260,7 miesięcy,\n65 latek, 217,1 miesiąca).Przykład 3.1  Wybieramy losową rodzinę z dwojgiem dzieci.\nInteresuje nas prawdopodobieństwo, że jest dwóch chłopców, jeżeli wiemy, żestarsze dziecko jest chłopcem;jedno z nich ma na imię Franek.W obu przypadkach\n\\[\n\\Omega=\\{(c,c), (c,d), (d,c), (d,d)\\}.\n\\]\nW przypadku :\n\\[\n\\mathbb{P}[\\{(c,c)\\} | \\{(c,c),(d,c)\\}] = 1/2,\n\\]\nw przypadku b:\n\\[\n\\mathbb{P}[\\{(c,c)\\} | \\{(c,c),(d,c), (c,d)\\}] = 1/3.\n\\]","code":""},{"path":"prawdopodobieństwo-warunkowe.html","id":"wzór-na-prawdopodobieństwo-całkowite","chapter":"3 Prawdopodobieństwo warunkowe","heading":"Wzór na prawdopodobieństwo całkowite","text":"Liczba \\(\\mathbb{P}[|B]\\) mówi jakie jest prawdopodobieństwo zajścia zdarzenia \\(\\) jeżeli wiemy, że zaszło\nzdarzenie \\(B\\). Prawdopodobieństwa względem wyjściowej miary \\(\\mathbb{P}[\\cdot]\\)\nmożna reprezentować w terminach względem miary warunkowanej \\(\\mathbb{P}[\\cdot| B]\\). W wielu przypadkach\nułatwia rachunki. Mamy bowiem\n\\[\n    \\mathbb{P}[\\cap B] = \\mathbb{P}[B] \\mathbb{P}[|B]\n\\]\noraz, skoro zdarzenia \\(\\cap B\\) oraz \\(\\cap B^c\\) wykluczają się wzajemnie,\n\\[\\begin{align*}\n\\mathbb{P}[] & = \\mathbb{P}[\\cap B] + \\mathbb{P}[\\cap B^c] \\\\&=\n\\mathbb{P}[B]\\mathbb{P}[|B] + \\mathbb{P}[B^c]\\mathbb{P}[|B^c].\n\\end{align*}\\]\nOstatni wzór jest szczególnie pomocny kiedy eksperymenty podzielone są na etapy.\nZanim zbadamy konkretny przykład uogólnijmy powyższy rachunek na dowolną liczbę zdarzeń.Definicja 3.2  Niech \\(\\subseteq \\mathbb{N}\\) będzie zbiorem indeksów.\nMówimy, że rodzina zdarzeń \\(\\{B_k\\}_{k\\}\\) (dopuszczamy \\(||=\\infty\\)) jest rozbiciem zbioru \\(\\Omega\\), jeżeli\n\\[\n\\Omega = \\bigcup_{k \\} B_k\n\\]\noraz zbiory \\(B_k\\) są parami rozłączne.Twierdzenie 3.1  (Wzór na prawdopodobieństwo całkowite) Jeżeli \\(\\{B_k\\}_{k \\}\\) jest rozbiciem \\(\\Omega\\) (skończonym lub przeliczalnym) takim, że\n\\(\\mathbb{P}[B_k]>0\\) dla każdego \\(k\\\\), dla dowolnego zdarzenia \\(\\\\mathcal{F}\\)\n\\[\n  \\mathbb{P}[] = \\sum_{k \\} \\mathbb{P}[|B_k]\\mathbb{P}[B_k]\n\\]Proof. Korzystając z definicji rozbicia oraz prawdopodobieństwa warunkowego piszemy\n\\[\\begin{align*}\n\\mathbb{P}[] & = \\mathbb{P}\\left[ \\cap \\bigcup_{k\\} B_k\\right]\n= \\mathbb{P}\\left[  \\bigcup_{k\\} (\\cap B_k)\\right]\n\\\\ & = \\sum_{k\\} \\mathbb{P}[ \\cap B_k] = \\sum_{k\\} \\mathbb{P}[|B_k]\\mathbb{P}[B_k].\n\\end{align*}\\]Przykład 3.2  W loterii fantowej szansa wylosowania losu wygrywającego jest równa \\(p\\), przegrywającego \\(q\\), z prawdopodobieństwem\n\\(r\\) (\\(p+q+r=1\\)) wyciągamy los ‘graj dalej’. Los ‘graj dalej’ wrzucany\njest urny pozwala na kolejne losowanie. Jakie jest prawdopodobieństwo wygranej?\nOznaczmy przez \\(\\), \\(B\\), \\(C\\) zdarzenie polegające na wyciągnięciu losu odpowiednio wygrywającego,\nprzegrywającego, ‘graj dalej’, przez \\(W\\) zdarzenie wygrania w loterii. Wówczas\n\\[\\begin{align*}\n\\mathbb{P}[W] =&  \\mathbb{P}[W|]\\mathbb{P}[]+\\mathbb{P}[W|B]\\mathbb{P}[B]\\\\ &+\\mathbb{P}[W|C]\\mathbb{P}[C]\n\\\\ =& 1\\cdot p + 0\\cdot q + \\mathbb{P}[W]\\cdot r.\n\\end{align*}\\]\nZatem\n\\[\n\\mathbb{P}[W] = \\frac{p}{1-r} = \\frac p{p+q}.\n\\]","code":""},{"path":"prawdopodobieństwo-warunkowe.html","id":"wzór-bayesa","chapter":"3 Prawdopodobieństwo warunkowe","heading":"Wzór Bayesa","text":"Przykład 3.3  Rozważmy następujący test na obecność pewnej choroby.\nWiadomo, że \\(1\\) osoba na \\(1000\\) jest chora.\nPonadto wiemy, że u chorych test wykrywa chorobę z prawdopodobieństwem \\(99\\%\\),\nu osób zdrowych działa poprawnie (tzn. nie wykrywa choroby) z prawdopodobieństwem \\(95\\%\\).\nJakie jest prawdopodobieństwo, że u losowo wybranej osoby wynik będzie pozytywny?\nOznaczmy\\(C\\) - badana osoba jest chora;\\(Z\\) - badana osoba jest zdrowa;\\(T\\) - test był pozytywny.\nMamy\n\\[\\begin{align*}\n\\mathbb{P}[T] & = \\mathbb{P}[T|Z]\\mathbb{P}[Z] + \\mathbb{P}[T|C]\\mathbb{P}[C]\n\\\\ & = \\frac{5}{100}\\cdot \\frac{999}{1000}  +  \\frac{99}{100}\\cdot \\frac{1}{1000}\n= \\frac{5094}{100000} \\\\& =0.05094\n\\end{align*}\\]Zauważmy, że w powyższym przykładzie jest naturalne, o wiele istotniejsze pytanie.\nJeżeli test wyszedł pozytywny, jakie jest prawdopodobieństwo, że pacjent jest\nrzeczywiście chory? Pytamy więc o przyczynę pozytywnego wyniku.\nZ jakim prawdopodobieństwem wynik jest spowodowany przez chorobę?\nZ jakim prawdopodobieństwem wynik jest fałszywie pozytywny?\nOdpowiedzi na powyższe pytanie możemy udzielić stosując wzór Bayesa.Twierdzenie 3.2  (Wzór Bayesa) Przy założeniach jw. jeżeli \\(\\mathbb{P}[]>0\\), dla każdego \\(k\\\\),\n\\[\n\\mathbb{P}[B_k| ] = \\frac{\\mathbb{P}[|B_k]\\mathbb{P}[B_k]}{\\sum_{\\} \\mathbb{P}[|B_i]\\mathbb{P}[B_i]}.\n\\]Proof. Ze wzoru na prawdopodobieństwo całkowite\n\\[\n\\frac{\\mathbb{P}[|B_k]\\mathbb{P}[B_k]}{\\sum_{\\} \\mathbb{P}[|B_i]\\mathbb{P}[B_i]}\n= \\frac{\\mathbb{P}[\\cap B_k]}{\\mathbb{P}[]} =   \\mathbb{P}B_k| ].\n\\]Remark. Ze względu na strukturę wzorów w dwóch ostatnich twierdzeniach korzysta się z nich w różnych kontekstach.Wzór na prawdopodobieństwo całkowite pozwala na obliczanie prawdopodobieństw zdarzeń,\nktóre mogą zajść w wyniku innych zdarzeń, np. przy doświadczeniach wieloetapowych.Wzoru Bayesa używamy, gdy pytamy o przebieg doświadczenia znając już jego wynik.Przykład 3.4  Mamy \\(100\\) monet, spośród których jedna jest fałszywa ma orła po obu stronach.\nWybieramy losową monetę rzucamy nią \\(10\\) razy.\nW wyniku otrzymaliśmy \\(10\\) orłów. Jakie jest prawdopodobieństwo, że wylosowana moneta była fałszywa?\nOznaczmy zdarzenia\\(B_1\\) - wylosowaliśmy prawidłową monetę;\\(B_2\\) - wylosowaliśmy fałszywą monetę z dwoma orłami;\\(\\) - wyrzucono \\(10\\) orłów.Ze wzoru Bayesa\n\\[\\begin{align*}\n    \\mathbb{P}[B_2|] & =\n    \\frac{\\mathbb{P}[|B_2]\\mathbb{P}[B_2]}{\\mathbb{P}[|B_1]\\mathbb{P}[B_1] +  \\mathbb{P}[|B_2]\\mathbb{P}[B_2]}\n    \\\\& =\n    \\frac{1\\cdot \\frac{1}{100}}{\\frac 1{2^{10}}\\cdot \\frac{99}{100} + 1\\cdot \\frac 1{100}}\n    = \\frac{1024}{1123}\\\\ &\\approx 0,91.\n\\end{align*}\\]Przykład 3.5  U pacjenta przeprowadzono test na obecność pewnej choroby.\nWiadomo, że \\(1\\) osoba na \\(1000\\) jest chora.\nPonadto wiemy, że u chorych test wykrywa chorobę z prawdopodobieństwem \\(99\\%\\),\nu osób zdrowych działa poprawnie (tzn. nie wykrywa choroby) z prawdopodobieństwem \\(95\\%\\).\nZałóżmy, że u pacjenta test był pozytywny. Jakie jest prawdopodobieństwo, że jest chory?\nOznaczmy\n- \\(C\\) - badana osoba jest chora;\n- \\(Z\\) - badana osoba jest zdrowa;\n- \\(T\\) - test był pozytywny.\nZe wzoru Bayesa\n\\[\\begin{align*}\n\\mathbb{P}[C|T] & = \\frac{\\mathbb{P}[T|C]\\mathbb{P}[C]}{\\mathbb{P}[T|Z]\\mathbb{P}[Z] + \\mathbb{P}[T|C]\\mathbb{P}[C]}\n\\\\ & =\\frac{ \\frac{99}{100}\\cdot \\frac{1}{1000} }{ \\frac{5}{100}\\cdot \\frac{999}{1000}  +  \\frac{99}{100}\\cdot \\frac{1}{1000}  }\n= \\frac{99}{5094} \\\\& \\approx 0,019.\n\\end{align*}\\]\nPowyższy wynik jest zaskakujący.\nZobaczmy jak wygląda na przykładowych liczbach.\nJeżeli populacja składa się ze \\(100000\\) osób, wśród nich jest\nok. \\(100\\) chorych \\(99900\\) zdrowych. Aby lepiej zrozumieć\ndysproporcję można posłużyć się poniższym obrazkiem, gdzie\nstosunek pola małego kwadratu w lewym dolnym rogu całości obrazka dokładnie\n1:1000.Spośród chorych u \\(99\\) osób test wyjdzie pozytywny, spośród zdrowych u \\(4995\\).\nOgraniczenie przestrzeni probabilistycznej osób, u których test wyszedł pozytywny,\npozostawia nas w przestrzeni składającej się niemal wyłącznie z osób zdrowych.\nZauważmy, że powtórzenie testu niewiele poprawia jego skuteczność, dlatego też\nważna jest informacja o innych czynnikach związanych z chorobą (np. informacja genetyczna).","code":""},{"path":"niezależność-zdarzeń.html","id":"niezależność-zdarzeń","chapter":"4 Niezależność zdarzeń","heading":"4 Niezależność zdarzeń","text":"Wróćmy przykładu polegającego na rzucie dwoma kośćmi.\nWówczas przestrzenią probabilistyczną jest \\(\\Omega = [6]^2\\) z miarą jednostajną probabilistyczną \\(\\mathbb{P}\\) na \\(\\Omega\\).\nRozważmy dwa zdarzenia\n\\[\n  = \\{ \\mbox{na pierwszej kości wypadła liczba podzielna przez $3$}\\}\n\\]\noraz\n\\[\nB = \\{ \\mbox{na drugiej kości wypadła liczba mniejsza niż $6$}\\}.\n\\]\nWtedy \\(\\mathbb{P}[]= 1/2\\) oraz \\(\\mathbb{P}[B]= 5/6\\). Z kolei prawdopodobieństwo \\(\\) pod warunkiem \\(B\\) wynosi\n\\[\n\\mathbb{P}[\\: | \\: B] = 1/2 = \\mathbb{P}[].\n\\]\nInnymi słowy zajście zdarzenia \\(B\\) nie wpływa na prawdopodobieństwo zdarzenia \\(\\).\nPowyższa równość może być zapisana jako\n\\[\n\\mathbb{P}[\\cap B] = \\mathbb{P}[] \\mathbb{P}[B].\n\\]\nZauważmy, że powyższa równość nie wymusza założenia \\(\\mathbb{P}[B]>0\\).\nWłasność ta okazuje się być bardzo użyteczna często spotykana.Definicja 4.1  Niech \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) będzie przestrzenią probabilistyczną.\nZdarzenia \\(,B\\\\mathcal{F}\\) nazywamy niezależnymi, gdy\n\\[\n  \\mathbb{P}[\\cap B] = \\mathbb{P}[]\\mathbb{P}[B].\n  \\]Zauważmy, że jeżeli \\(\\mathbb{P}[]=0\\) lub \\(\\mathbb{P}[]=1\\), dla każdego \\(B\\\\mathcal{F}\\)\nzdarzenia \\(\\) \\(B\\) są niezależne.Przykład 4.1  Rzucamy kostką sześcienną. Niech \\(\\) będzie zdarzeniem, że wypadnie liczba parzysta,\n\\(B\\) zdarzeniem, że wypadnie wypadnie liczba podzielna przez \\(3\\).\nWówczas oba zdarzenia są niezależne.\nDefiniujemy przestrzeń probabilistyczną: \\(\\Omega = [6] = \\{ 1,2, \\ldots 6\\}\\),\n\\(\\mathcal{F}\\) składa się ze wszystkich podzbiorów \\(\\Omega\\), \\(\\mathbb{P}[ \\{ k \\}]=1/6\\).\nWówczas \\(= \\{2,4,6\\}\\), \\(B = \\{3,6\\}\\), \\(\\cap B = \\{6\\}\\).\nMamy\n\\[\\begin{equation*}\n\\mathbb{P}[\\cap B] = \\frac 16 = \\frac 12 \\cdot \\frac 13 = \\mathbb{P}[]\\cdot \\mathbb{P}[B].\n\\end{equation*}\\]Często z samego opisu zdarzeń nie wynika ich niezależność.Przykład 4.2  Wybieramy losową rodzinę posiadającą \\(n\\) dzieci.\nNiech \\(\\) będzie zdarzeniem, że w wybranej rodzinie jest co najwyżej jedna dziewczynka, \n\\(B\\) będzie zdarzeniem, że w wybranej rodzinie są dziewczynki chłopcy.\nCzy zdarzenia \\(\\) \\(B\\) są niezależne?\n\\(\\Omega\\) jest zbiór ciągów o długości \\(n\\) wyrazach \\(c\\) \\(d\\) (płeć dzieci uporządkowanych wg wieku).\nWtedy \\(|\\Omega| = 2^n\\). Przyjmujemy, że każdy ciąg jest jednakowo prawdopodobny.\nMamy\n\\(|| = n+1\\), \\(|B| = 2^n-2\\), \\(|\\cap B| = n\\) (dokładnie jedna dziewczynka). Zauważmy, że\n\\[\n\\mathbb{P}[]\\mathbb{P}[B] = \\frac{n+1}{2^n} \\cdot \\frac{2^n-2}{2^n}\n\\]\noraz\n\\[\n\\mathbb{P}[\\cap B] = \\frac{n}{2^n}.\n\\]\nZdarzenia \\(\\) \\(B\\) będą niezależne wtedy tylko wtedy, gdy\n\\[\n1+n = 2^{n-1}.\n\\]\nco ma miejsce jedynie dla \\(n=3\\).\nW tym przypadku powody niezależności są czysto algebraiczne.Definicja 4.2  Zdarzenia \\(A_1,\\ldots,A_n\\\\mathcal{F}\\) nazywamy niezależnymi, gdy\n\\[\n  \\mathbb{P}[A_{i_1} \\cap A_{i_2}\\cap \\ldots \\cap A_{i_k}] = \\mathbb{P}[A_{i_1}]\\cdot \\ldots \\cdot \\mathbb{P}[A_{i_k}]\n  \\]\ndla każdego ciągu \\(1\\le i_1 < i_2 <\\cdots < i_k \\le n\\), \\(k=2,3,\\ldots, n\\).Definicja 4.3  Zdarzenia \\(A_1,\\ldots,A_n\\\\mathcal{F}\\) nazywamy niezależnymi parami, gdy dla dowolnych \\(\\=j\\)\nzdarzenia \\(A_i\\) \\(A_j\\) są niezależneJeżeli zdarzenia \\(A_1,\\ldots, A_n\\) są niezależne, są niezależne parami. Odwrotna implikacja nie jest jednak prawdziwa.Przykład 4.3  Rzucamy 2 razy kostką. Oznaczmy\\(\\) - za pierwszym razem wypadła parzysta liczba oczek;\\(B\\) - za drugim razem wypadła parzysta liczba oczek;\\(C\\) - suma oczek jest parzysta;Wówczas zdarzania \\(,B,C\\) są niezależne parami, ale nie są niezależne. Rzeczywiście, mamy\n\\(\\cap B = \\cap C = B \\cap C\\) \\(\\cap B \\cap C = \\cap B\\). Stąd\\(\\mathbb{P}[] = 1/2\\)\\(\\mathbb{P}[B] = 1/2\\)\\(\\mathbb{P}[C] = 1/2\\)\\(\\mathbb{P}[\\cap B] = \\mathbb{P}[\\cap C]\\) \\(= \\mathbb{P}[C\\cap B]\\) \\(=\\mathbb{P}[\\cap B\\cap C]\\) \\(=1/4\\)Zbadanie niezależności \\(n\\) zdarzeń wymaga\nsprawdzenia \\({n\\choose 2}+ {n\\choose 3} +\\ldots + {n\\choose n} = 2^n-n-1\\) równań.\nChcemy badać ciągi niezależnych doświadczeń (może być ich nieskończenie wiele).","code":""},{"path":"niezależność-zdarzeń.html","id":"niezależne-sigma-ciała","chapter":"4 Niezależność zdarzeń","heading":"Niezależne \\(\\sigma\\)-ciała","text":"Definicja 4.4  Niech \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) będzie przestrzenią probabilistyczną\noraz niech \\(\\mathcal{F}_1\\), \\(\\mathcal{F}_2\\), …, \\(\\mathcal{F}_n\\)\nbędą \\(\\sigma\\)-ciałami zawartymi w \\(\\mathcal{F}\\).\nMówimy, że \\(\\sigma\\)-ciała te\nsą niezależne jeżeli dla dowolnych \\(A_1\\\\mathcal{F}_1\\), …, \\(A_n\\\\mathcal{F}_n\\)\nzachodzi warunek\n\\[\n\\mathbb{P}[A_1\\cap A_2\\cap \\ldots \\cap A_n] = \\mathbb{P}[A_1]\\cdot \\ldots \\cdot \\mathbb{P}[A_n].\n\\]Równoważnie (zadanie) \\(\\sigma\\)-ciała \\(\\mathcal{F}_1\\), \\(\\mathcal{F}_2\\), …, \\(\\mathcal{F}_n\\)\nsą niezależne gdy dowolne zdarzenia \\(A_1\\\\mathcal{F}_1\\), …, \\(A_n\\\\mathcal{F}_n\\) są niezależne.Definicja 4.5  Dowolna rodzina \\(\\sigma\\)-ciał \\(\\{\\mathcal{F}_i\\}_{\\}\\) jest niezależna,\njeżeli każdy skończony jej podzbiór jest niezależny.Przykład 4.4  Rzucamy \\(2\\) razy kostką. Pokażemy, że przy jednostajnym prawdopodobieństwie, rzuty te są niezależne.\nNiech \\(\\Omega = \\left\\{ (,j): ,j \\[6]\\right\\}\\), \\(\\mathcal{F}\\) składa się ze wszystkich podzbiorów\n\\(\\Omega\\), \\(\\mathbb{P}\\)\njest wspomnianą już jednostajną miarą probabilistyczną.\nRozważmy dwa \\(\\sigma\\)-ciała:\n\\[\\begin{align*}\n    \\mathcal{F}_1 & = \\left\\{ \\times [6]:\\ \\subset  [6]  \\right\\},\\\\\n    \\mathcal{F}_2 & = \\left\\{  [6]\\times B:\\ B\\subset  [6]  \\right\\}.\n\\end{align*}\\]\nWówczas zdarzenie \\(\\times [6] \\\\mathcal{F}_1\\) mówi, że wynik na pierwszej kości należy \\(\\) nie niesie\nżadnej informacji o wyniku drugiej kości.samo dotyczy zdarzeń postaci \\([6]\\times B \\\\mathcal{F}_2\\).Natomiast ich przekrój \\(\\times [6] \\cap [6]\\times B = \\times B\\)\noznacza zdarzenie, że wynik na pierwszej kości należy \\(\\) wynik na drugiej kości należy \\(B\\).Pokażemy, że \\(\\sigma\\)-ciała \\(\\mathcal{F}_1\\) \\(\\mathcal{F}_2\\) są niezależne.\nWeźmy dowolne zdarzenia \\(\\times [6] \\\\mathcal{F}_1\\) oraz \\([6]\\times B\\\\mathcal{F}_2\\).\nWówczas\n\\[\n\\mathbb{P}\\left[\\times [6] \\right] = \\frac{||\\cdot 6}{36} = \\frac{||}{6}.\n\\]\nPodobnie\n\\[\n\\mathbb{P}\\left[ [6] \\times B\\right] =\\frac{|B|}{6}.\n\\]\nDla przekroju\n\\[\n\\mathbb{P}\\left[ \\times [6]\\cap  [6] \\times B  \\right] = \\mathbb{P}[\\times B] = \\frac{|||B|}{36}.\n\\]Powyższy przykład pokazuje, że intuicyjne pojęcie niezależności dwóch rzutów kostką zgadza się z formalną definicją.\nŁatwo uogólnić go \\(n\\) rzutów.Lemma 4.1  Załóżmy, że zdarzenia \\(A_1,\\ldots, A_n\\) są niezależne.\nWtedy \\(\\sigma\\)-ciała generowane przez \\(A_i\\): \\(\\sigma(A_1),\\ldots, \\sigma(A_n)\\)\n(przypomnijmy \\(\\sigma() = \\{\\emptyset, , ^c, \\Omega\\}\\)) również są niezależne.Proof. Pozostawiamy jako zadanieWniosek 4.1  Jeżeli zdarzenia \\(A_1,\\ldots, A_n\\) są niezależne, \n\\[\n\\mathbb{P}\\left[ \\bigcup_{=1}^n A_i   \\right] =\n1 -\\mathbb{P}\\left[ \\bigcap_{=1}^n A_i^c   \\right] = 1 -\\prod_{=1}^n (1 - \\mathbb{P}[A_i]).\n\\]Rozważmy następujący problem.\nDany jest ciąg \\(n\\) doświadczeń, w którym wynik \\(\\)-tego doświadczenia jest\nopisany przestrzenią probabilistyczną \\((\\Omega_i,\\mathcal{F}_i, \\mathbb{P}_i)\\).\nJak zbudować przestrzeń probabilistyczną \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\)\nmodelującą przeprowadzenie tych \\(n\\) doświadczeń w sposób niezależny?\nZdefiniujmy\n\\[\n    \\Omega = \\Omega_1 \\times \\ldots \\times \\Omega_n\n\\]\noraz\n\\[\n\\mathcal{F}'_i = \\left\\{ \\Omega_1\\times \\ldots \\times \\Omega_{-1} \\times \\times \\Omega_{+1} \\times \\ldots \\times \\Omega_n :\n\\\\mathcal{F}_i \\right\\}.\n\\]\nWówczas \\(\\mathcal{F}'_i\\) jest kopią \\(\\mathcal{F}_i\\).\nWeźmy \\(\\mathcal{F} = \\sigma(\\mathcal{F}'_1,\\ldots,\\mathcal{F}'_n)\\) - \\(\\sigma\\)-ciało generowane przez \\(\\mathcal{F}'_i\\)\n(wówczas \\(\\mathcal{F} = \\mathcal{F}_1 \\otimes \\ldots \\otimes \\mathcal{F}_n\\) jest \\(\\sigma\\)-ciałem produktowym),\ntzn. elementami \\(\\mathcal{F}\\) są zbiory postaci \\(A_1\\times \\ldots \\times A_n\\), dla \\(A_i\\\\mathcal{F}_i\\).Chcemy, aby kolejne doświadczenia były niezależne, więc aby \\(\\sigma\\)-ciała \\(\\mathcal{F}'_1,\\ldots, \\mathcal{F}'_n\\)\nbyły niezależne. Poszukujemy więc prawdopodobieństwa \\(\\mathbb{P}\\) takiego, że dla dowolnych\n\\(A_i\\\\mathcal{F}_i\\) zachodzi\n\\[\\begin{multline*}\n\\mathbb{P}[A_1\\times\\ldots \\times A_n] = \\\\\n\\mathbb{P}\\left[(A_1\\times \\Omega_2 \\times \\ldots \\times \\Omega_n) \\cap \\ldots \\cap (\\Omega_1\\times \\ldots \\times A_n)\\right] \\\\\n=\\prod_{=1}^n \\mathbb{P}\\left[\\Omega_1 \\times \\ldots \\times A_i \\times \\ldots \\times \\Omega_n\\right].\n\\end{multline*}\\]\nSkoro chcemy, aby przestrzeń produktowa reprodukowała \\(\\)-ty eksperyment, musi również\nzachodzić\n\\[\n\\mathbb{P}[\\Omega_1 \\times \\ldots \\times A_i \\times \\ldots \\times \\Omega_n] = \\mathbb{P}_i[A_i].\n\\]\nUpraszczając zapis szukamy prawdopodobieństwa \\(\\mathbb{P}\\) na \\(\\Omega\\) takiego, że\n\\[\n\\mathbb{P}[A_1\\times\\ldots \\times A_n] =\\prod_{=1}^n \\mathbb{P}_i[A_i]\n\\]\ndla dowolnych zdarzeń \\(A_1, \\ldots, A_n\\).\nZ teorii miary wiemy, że istnieje dokładnie jedna taka miara probabilistyczna.\nJest miara produktowa:\n\\[\n\\mathbb{P} = \\mathbb{P}_1 \\otimes \\ldots \\otimes \\mathbb{P}_n.\n\\]Przykład 4.5  Losujemy niezależnie dwie liczby z przedziału \\([0,1]\\).\nW tym przypadku mamy \\(\\Omega_1=\\Omega_2=[0,1]\\),\n\\(\\mathcal{F}_1=\\mathcal{F}_2 = \\mathcal{B}([0,1])\\) oraz \\(\\mathbb{P}_1=\\mathbb{P}_2=\\lambda_1\\),\ngdzie \\(\\lambda_1\\) jest jednowymiarową miarą Lebesgue’.\nJak reprezentować wynik tego eksperymentu na jednej przestrzeni probabilistycznej?\nZ powyższej konstrukcji \\(\\Omega = \\Omega_1\\times \\Omega_2 = [0,1]^2\\),\n\\(\\mathcal{F} = \\mathcal{B}([0,1]) \\otimes \\mathcal{B}([0,1]) = \\mathcal{B}([0,1]^2)\\).\nOdpowiadające prawdopodobieństwo \n\\(\\mathbb{P}=\\mathbb{P}_1\\otimes \\mathbb{P}_2 = \\lambda_1\\otimes \\lambda_1\\).\nPrzypomnijmy, że jest jedyna miara na \\([0,1]^2\\) taka, że\n\\[\\begin{equation*}\n    \\lambda_1\\otimes \\lambda_1 (\\times B) = \\lambda_1 () \\lambda_1(B)\n\\end{equation*}\\]\ndla dowolnych \\(, B \\\\mathcal{B}([0,1])\\).\nJest dokładnie charakteryzacja dwuwymiarowej (płaskiej) miary Lebesgue’. Innymi słowy\n\\(\\mathbb{P} = \\lambda_1\\otimes \\lambda_1 = \\lambda_2\\).\nReasumując, wylosowanie niezależnie dwóch liczb z przedziału \\([0,1]\\) jest tożsame z wylosowaniem\npunktu z kwadratu jednostkowego \\([0,1]^2\\).Przykład 4.6  (Schemat Bernoulliego) Wykonano \\(n\\)-krotnie samo doświadczenie, w którym prawdopodobieństwo sukcesu wynosi \\(p\\).\nKolejne próby były niezależne.\nWówczas prawdopodobieństwo sukcesu w dokładnie \\(k\\) próbach wynosi\n\\({n\\choose k} p^k (1-p)^{n-k}\\).\nRzeczywiście, niech\\(\\Omega_i = \\{0,1\\}\\) (1 odpowiada sukcesowi w \\(\\)-tym doświadczeniu),\\(\\mathcal{F}_i = 2^{\\Omega_i}\\),\\(\\mathbb{P}_i[\\{1\\}] = p\\), równoważnie \\(\\mathbb{P}_i[\\{1\\}] = p^{\\omega_i}(1-p)^{1-\\omega_i}\\).Wówczas \\((\\Omega_i, \\mathcal{F}_i, \\mathbb{P}_i)\\)\nmodeluje wynik doświadczenia w \\(\\)-tym kroku. Definiując jak powyżej\\(\\Omega = \\Omega_1 \\times \\ldots \\times \\Omega_n = \\{0,1\\}^n\\),\\(\\mathcal{F} = \\mathcal{F}_1 \\otimes \\ldots \\otimes \\mathcal{F}_n\\),\\(\\mathbb{P}  = \\mathbb{P}_1 \\otimes \\ldots \\otimes \\mathbb{P}_n\\).Wtedy dla \\(\\omega = (\\omega_1,\\ldots, \\omega_n)\\\\Omega\\) mamy\n\\[\n\\mathbb{P}[\\{\\omega\\}] = p^{\\omega_1+\\ldots+\\omega_n}(1-p)^{n-(\\omega_1+\\ldots+\\omega_n)}.\n\\]\nNiech \\(A_k\\) oznacza zdarzenie osiągnięcia dokładnie \\(k\\) sukcesów, tj.\n\\[\nA_k = \\{\\omega\\\\Omega:\\; \\omega_1+\\ldots+\\omega_n = k\\},\n\\]\ninnymi słowy \\(A_k\\) zawiera \\(\\omega\\) zawierające dokładnie \\(k\\) jedynek.\nStąd \\(|A_k| = {n\\choose k}\\) oraz\n\\[\n\\mathbb{P}[A_k] = \\sum_{\\omega \\A_k} \\mathbb{P}[\\{\\omega\\}] =\n|A_k| p^k (1-p)^{n-k}= {n\\choose k} p^k (1-p)^{n-k}.\n\\]","code":""},{"path":"lemat-borela-cantellego.html","id":"lemat-borela-cantellego","chapter":"5 Lemat Borela-Cantellego","heading":"5 Lemat Borela-Cantellego","text":"","code":""},{"path":"lemat-borela-cantellego.html","id":"nieskończone-ciągi-eksperymentów","chapter":"5 Lemat Borela-Cantellego","heading":"Nieskończone ciągi eksperymentów","text":"W przyszłości chcemy analizować asymptotyczne zachowanie procesów losowych.\nW naturalny sposób potrzeba konstrukcji przestrzeni probabilistycznej, na której\nmożna wykonać nieskończenie wiele eksperymentów które mogą, ale nie muszą być niezależne.\nDla ustalenia uwagi skupimy się jednak na niezależnych eksperymentach.\nZakładać będziemy, że mamy ciąg przestrzeni probabilistycznych, w którym\n\\((\\Omega_n, \\mathcal{F}_n, \\mathbb{P}_n)\\) opisuje wynik \\(n\\)-tego eksperymentu. Zakładać będziemy, że\n\\[\\begin{equation}\n    \\Omega_n \\subseteq \\mathbb{R}, \\quad \\mathcal{F}_n \\subseteq \\mathcal{B}(\\mathbb{R}).\n    \\tag{5.1}\n\\end{equation}\\]\nChcemy udzielić odpowiedzi na pytanie jak skonstruować przestrzeń, na której\nwszystkie te eksperymenty wykonywane są niezależnie?Powiedzmy, że interesuje nas\nnieskończony ciąg rzutów monetą. Skoro ciąg \\(n\\) rzutów modelujemy przestrzenią skończonych\nciągów zer jedynek \\(\\{0,1\\}^n\\), nieskończony ciąg rzutów będziemy modelować\nprzestrzenią nieskończonych ciągów zer jedynek \\(\\{0,1\\}^\\mathbb{N}\\).\nNieskończony produkt kartezjański prowadzi pewnych trudności technicznych\nprzy sprawdzaniu warunku przeliczalnej addytywności, który nakładamy zawsze na rozważane\nprawdopodobieństwo \\(\\mathbb{P}\\). Trudności te są oczywiście przejścia.\njednak ograniczymy się gotowych rozwiązań.Przykład 5.1  Zamierzamy skonstruować przestrzeń probabilistyczną na której można\nzdefiniować ciąg niezależnych zdarzeń \\(\\{A_n\\}_{n=1}^{\\infty}\\)\ntakich, że \\(\\mathbb{P}[A_n]= 1/2\\).\nNiech \\((\\Omega, \\mathcal{F}, \\mathbb{P}) = ([0,1], \\mathcal{B}([0,1]), {\\rm Leb})\\).\nDla każdego \\(\\omega\\[0,1]\\) przyporządkowujemy jego rozwinięcie dwójkowe:\n\\[\n  \\omega =\\sum_{n=1}^\\infty \\frac{\\omega_n}{2^n} = 0,\\omega_1\\omega_2\\ldots\n\\]\nPowyższe przedstawienie nie jest jednoznaczne np.\n\\(1/2 = 0,10000\\ldots = 0,01111\\ldots\\), aby uzyskać jednoznaczność\nwybieramy rozwinięcie zawierające nieskończenie wiele \\(1\\).\nNiech\n\\[\nA_n = \\big\\{ \\omega \\[0,1]:\\; \\omega_n = 0  \\big\\},\n\\]\nczyli\n\\[\\begin{align*}\nA_1 &= [0,1/2),\\\\\nA_2 &= [0,1/4)\\cup [1/2,3/4),\\\\\nA_3 &= [0,1/8)\\cup [1/4, 3/8) \\cup [1/2,5/8)\\cup [3/4,7/8),\\\\\nA_4 & = \\cdots\n\\end{align*}\\]\nWtedy \\(\\mathbb{P}[A_n]=1/2\\).\nPonadto można pokazać, że zbiory \\(\\{A_n\\}\\) są niezależne (zadanie).Powyższy przykład pokazuje, że w przypadku rzutów monetą można wskazać bardzo konkretną przestrzeń,\nktórą możemy modelować nieskończony ciąg eksperymentów. Jeżeli bylibyśmy\nzainteresowani rzutami kostką, konstrukcję z powyższego przykładu można przeprowadzić\ndla rozwinięć w systemie szóstkowym.Co jeżeli nasz ciąg eksperymentów jest bardziej skomplikowany? Jeżeli w chwilach nieparzystych rzucamy\njedną kością, w chwilach parzystych rzucamy monetą z wyłączeniem chwil podzielnych przez 128,\nw których losujemy punkt z przedziału \\([0,42]\\)?Potrzebne nam jest ogólne narzędzie (maszynka) konstruowania przestrzeni probabilistycznych\ndla nieskończonych ciągów eksperymentów.Twierdzenie 5.1  (Kołmogorowa o istnieniu procesu) Załóżmy, że \\(\\{\\mathbb{P}_n'\\}_{n \\\\mathbb{N}}\\) jest ciągiem miar probabilistycznych takim, żedla każdego \\(n \\\\mathbb{N}\\), \\(\\mathbb{P}_n'\\) jest miarą na \\(\\mathbb{R}^n, \\mathcal{B}(\\mathbb{R}^n)\\).Spełniony jest warunek zgodności:\n\\[\n\\mathbb{P}_{n+1}'(A_1\\times A_2 \\times \\ldots \\times A_n\\times \\mathbb{R})\n=  \\mathbb{P}_{n}'(A_1\\times A_2 \\times \\ldots \\times A_n).\n\\]Wówczas istnieje jedyna miara probabilistyczna \\(\\mathbb{P}\\) na\n\\((\\mathbb{R}^\\mathbb{N}, \\mathcal{B}(\\mathbb{R}^\\mathbb{N}))\\) taka, że\n\\[\n\\mathbb{P}'(A_1\\times A_2 \\times \\ldots \\times A_n\\times \\mathbb{R}\\times \\ldots)\n=  \\mathbb{P}_{n}'(A_1\\times A_2 \\times \\ldots \\times A_n).\n\\]Proof. znalezienia tutaj, Theorem .3.1.\\(\\mathcal{B}(\\mathbb{R}^\\mathbb{N})\\) jest \\(\\sigma\\)-ciałem generowanym przez skończenie wymiarowe\nprostokąty\n\\[\\begin{equation*}\n    B_1\\times B_2 \\times \\cdots \\times B_n \\times \\mathbb{R} \\times \\mathbb{R} \\times \\cdots,\n\\end{equation*}\\]\ndla \\(n \\\\mathbb{N}\\), \\(B_1, \\ldots B_n \\\\mathcal{B}(\\mathbb{R})\\).Powyższe twierdzenie dostarcza ogólne narzędzie gwarantujące istnienie miary probabilistycznej.\nZauważmy też, że działa ono nie tylko dla niezależnych eksperymentów.\nJedynym warunkiem jest zgodność miar.\nW niektórych przypadkach można bezpośrednio skonstruować przestrzeń probabilistyczną.Wniosek 5.1  Istnieje przestrzeń probabilistyczna \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\)\nopisująca nieskończony ciąg niezależnych eksperymentów.Proof. Niech \\((\\Omega_n, \\mathcal{F}_n, \\mathbb{P}_n)\\) będzie ciągiem przestrzeni probabilistycznych\nspełniających (5.1).\nWówczas\n\\[\\begin{equation*}\n    \\mathbb{P}'_n = \\mathbb{P}_1\\otimes \\mathbb{P}_2\\otimes \\cdots \\otimes \\mathbb{P}_n\n\\end{equation*}\\]\nopisuje pierwsze \\(n\\) eksperymentów wykonanych niezależnie. Rodzina ta jest zgodna, bo\n\\[\\begin{multline*}\n\\mathbb{P}_{n+1}'[B_1\\times B_2\\times \\cdots B_{n} \\times \\mathbb{R}] \\\\\n= \\mathbb{P}_1[B_1]\\mathbb{P}_2[B_2] \\cdots \\mathbb{P}_n[B_n]\\mathbb{P}_{n+1}[\\mathbb{R}] \\\\\n\\mathbb{P}_{n}'[B_1\\times B_2\\times \\cdots B_{n}] \\\\\n\\end{multline*}\\]\nZ Twierdzenia 5.1 wynika, że istnieje \\(\\mathbb{P}\\) na \\(\\mathbb{R}^\\mathbb{N}\\) taka, że\n\\[\n\\mathbb{P}'(A_1\\times A_2 \\times \\ldots \\times A_n\\times \\mathbb{R}\\times \\ldots)\n=  \\mathbb{P}_{n}'(A_1\\times A_2 \\times \\ldots \\times A_n).\n\\]\nOkazuje się, że jest szukana przez nas miara probabilistyczna modelująca niezależne eksperymenty.\nNiech\n\\[\\begin{align*}\n    \\tilde{}_1 & = A_1\\times \\mathbb{R}\\times \\mathbb{R}\\times \\ldots \\\\\n    \\tilde{}_2 & = \\mathbb{R}\\times A_2 \\times \\mathbb{R} \\times \\ldots \\\\\n    \\tilde{}_3 & = \\mathbb{R} \\times \\mathbb{R} \\times A_3 \\times \\mathbb{R} \\ldots\n\\end{align*}\\]\nWówczas \\(\\tilde{}_j\\) jest kopią \\(A_j\\) w nieskończonym produkcie.\nWówczas dla każdego \\(j \\\\mathbb{N}\\),\n\\[\\begin{equation*}\n    \\mathbb{P}[\\tilde A_j] = \\mathbb{P}_j[A_j].\n\\end{equation*}\\]\nSkoro\n\\[\\begin{equation*}\n    A_1\\times A_2 \\times \\ldots \\times A_n \\times \\mathbb{R} \\ldots = \\tilde{}_1\\cap \\tilde{}_2\\cap \\ldots \\tilde{}_n,\n\\end{equation*}\\]\n\n\\[\\begin{equation*}\n    \\mathbb{P}[\\tilde{}_1\\cap \\tilde{}_2\\cap \\ldots \\tilde{}_n] = \\mathbb{P}[\\tilde A_1]\\mathbb{P}[\\tilde A_2] \\cdots \\mathbb{P}[\\tilde A_n].\n\\end{equation*}\\]\nCzyli miara \\(\\mathbb{P}\\) reprodukuje eksperymenty w sposób niezależny.Badając eksperymenty losowe polegające na nieskończonych powtórzeniach jednego eksperymentu, powiedzmy, że wykonujemy nieskończenie wiele rzutów monetą.Definicja 5.1  Dana jest przestrzeń probabilistyczna \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\)\noraz ciąg zdarzeń \\(\\{A_n\\}_{n\\\\mathbb{N}} \\subseteq \\mathcal{F}\\).\nGranicą górną ciągu zdarzeń\n\\(\\{A_n\\}_{n \\\\mathbb{N}}\\) nazywamy zdarzenie\n\\[\\begin{multline*}\n    \\limsup_{n\\\\mathbb{N}} A_n = \\bigcap_{m=1}^\\infty \\bigcup_{n=m}^\\infty A_n \\\\\n    = \\left( A_1\\cup A_2\\cup A_3\\cup\\ldots \\right)\\cap\n\\left( A_2\\cup A_3\\cup\\ldots \\right)\\cap\n\\left( A_3\\cup\\ldots \\right) \\cap \\ldots.\n\\end{multline*}\\]Przypomnijmy, że element należy przekroju zbiorów wtedy tylko wtedy, gdy należy\nkażdego z nich. Element należy sumy zbiorów wtedy tylko wtedy, gdy należy\njednego ze zbiorów. Mamy więc\n\\[\\begin{multline*}\n\\omega \\\\bigcap_{m=1}^\\infty\\bigcup_{n=m}^{\\infty }A_n \\iff\n\\forall m \\geq 1, \\: \\omega \\\\bigcup_{n=m}^{\\infty} A_m \\\\ \\iff\n\\forall m\\geq 1, \\: \\exists n\\geq m, \\: \\omega \\A_n.\n\\end{multline*}\\]\nReasumując, \\(\\omega\\\\limsup A_n\\) wtedy tylko wtedy, gdy \\(\\omega\\)\nnależy nieskończenie wielu zbiorów \\(A_i\\). Innymi słowy,\n\\(\\limsup A_n\\) zdarzenie polegające na tym, że\nzaszło nieskończenie wiele spośród zdarzeń \\(A_1, A_2, \\ldots\\).\nDla przykładu, rzucamy nieskończenie wiele razy kostką \\(A_n\\) oznacza zdarzenie,\nże w \\(n\\)-tym rzucie kostką wypadła \\(6\\), \\(\\limsup A_n\\) jest zdarzeniem,\nże wypadło nieskończenie wiele \\(6\\).Niezwykle przydatnym narzędziem będzie lemat wskazujący\nwarunki przy których zachodzi nieskończenie wiele zdarzeń.Lemma 5.1  (Borel-Cantelli) Załóżmy, że \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) jest przestrzenią probabilistyczną\noraz niech \\(\\{A_n\\}_{n \\\\mathbb{N}}\\subseteq \\mathcal{F}\\) będzie ciągiem zdarzeń.Jeżeli \\(\\sum_{n=1}^\\infty \\mathbb{P}[A_n] < \\infty\\), \\[\\mathbb{P}[\\limsup A_n] = 0,\\]\ntzn. z prawdopodobieństwem \\(1\\) zachodzi jedynie skończenie wiele spośród zdarzeń \\(A_n\\).Jeżeli \\(A_1,A_2,\\ldots\\) są niezależnymi zdarzeniami oraz\\(\\sum_{n=1}^\\infty \\mathbb{P}[A_n] = \\infty\\), \\[\\mathbb{P}[\\limsup A_n] =1,\\]\ntzn. z prawdopodobieństwem \\(1\\) zachodzi nieskończenie wiele zdarzeń \\(A_n\\).Proof. Korzystając z ciągłości miary, następnie z jej podaddytywności otrzymujemy\n\\[\\begin{equation*}\n\\mathbb{P}[\\limsup A_n] = \\mathbb{P}\\left[ \\bigcap_{m=1}^\\infty \\bigcup_{n=m}^\\infty A_n \\right]\n=\\lim_{m\\\\infty} \\mathbb{P}\\left[ \\bigcup_{n=m}^\\infty A_n \\right]\\le\n\\lim_{m\\\\infty} \\sum_{n=m}^\\infty \\mathbb{P}[A_n] = 0,\n\\end{equation*}\\]\ngdzie ostatnia równość wynika z sumowalności szeregu.\nAby wykazać punkt 2, wystarczy pokazać, że\n\\[\n  0 = \\mathbb{P}\\left[ (\\limsup A_n)^c\\right]=  \n  \\mathbb{P}\\left[\\left( \\bigcap_{m=1}^\\infty \\bigcup_{n=m}^\\infty A_n\\right)^c \\right]\n=   \\mathbb{P}\\left[ \\bigcup_{m=1}^\\infty \\bigcap_{n=m}^\\infty A_n^c \\right].\n\\]\nSkoro\n\\[\\begin{equation*}\n\\mathbb{P}\\left[ \\bigcup_{m=1}^\\infty \\bigcap_{n=m}^\\infty A_n^c \\right]\n\\leq \\sum_{m=1}^\\infty\\mathbb{P}\\left[ \\bigcap_{n=m}^\\infty A_n^c \\right],\n\\end{equation*}\\]\nwystarczy udowodnić, że dla każdego \\(m\\) mamy\n\\[\n\\mathbb{P}\\left[ \\bigcap_{n=m}^\\infty A_n^c \\right] = 0.\n\\]\nW tym celu piszemy (kolejno korzystamy z twierdzenia o ciągłości, niezależności zbiorów \\(A_n\\) oraz nierówności\n\\(1-x\\le e^{-x}\\))\n\\[\\begin{multline*}\n\\mathbb{P}\\left[ \\bigcap_{n=m}^\\infty A_n^c \\right]\n= \\lim_{k\\\\infty} \\mathbb{P}\\left[ \\bigcap_{n=m}^k A_n^c \\right]\n    = \\lim_{k\\\\infty} \\prod_{n=m}^k \\mathbb{P}\\left[ A_n^c \\right] \\\\\n    = \\lim_{k\\\\infty} \\prod_{n=m}^k \\left( 1 - \\mathbb{P}\\left[ A_n \\right]\\right)\n    \\le \\lim_{k\\\\infty} e^{-\\sum_{n=m}^k \\mathbb{P}[ A_n]} =  e^{-\\sum_{n=m}^\\infty \\mathbb{P}[ A_n]} = 0.\n  \\end{multline*}\\]Przykład 5.2  Uzasadnimy, że jeżeli będziemy rzucać odpowiednio długo kostką,\nz prawdopodobieństwem \\(1\\) wyrzucimy dowolną liczbę szóstek.\nNiech \\(\\Omega = \\{1,2,\\ldots,6\\}^\\mathbb{N}\\), \\(\\mathcal{F} = 2^\\Omega\\).\nPonadto niech \\(\\mathbb{P}\\) będzie odpowiednią miarą probabilistyczną.\nOznaczmy przez \\(A_n\\) zdarzenie, że w rzucie o numerze \\(n\\), wypadło \\(6\\).\nWówczas \\(\\mathbb{P}[A_n] = 1/6\\).\nZdarzania \\(A_i\\) są niezależne oraz\n\\[\n\\sum_{n=1}^\\infty \\mathbb{P}\\left[A_n\\right]  = \\sum_{n=1}^\\infty 1/6 = \\infty.\n\\]\nLemat Borela - Cantelliego pociąga\n\\[\n\\mathbb{P}\\left[\\limsup  A_n\\right] =1.\n\\]Przykład 5.3  Jeżeli będziemy rzucać odpowiednio długo kostką,\nz prawdopodobieństwem \\(1\\) wyrzucimy w kolejnych rzutach ciąg złożony z kolejnych \\(10\\) jedynek kolejnych \\(10\\) szóstek.\nNiech \\(\\Omega = \\{1,2,\\ldots,6\\}^\\mathbb{N}\\), \\(\\mathcal{F} = 2^\\Omega\\).\nPonadto niech \\(\\mathbb{P}\\) będzie odpowiednią miarą probabilistyczną.\nOznaczmy przez \\(A_n\\) zdarzenie, że w rzutach o numerach \\(n, n+1, \\ldots, n+19\\) wypadnie żądany ciąg.\nWówczas \\(\\mathbb{P}[A_n] = 1/6^{20}\\).Zdarzania \\(A_i\\) nie są niezależne, więc nie możemy dla nich użyć lematu Borela-Cantallego.\nZdefiniujmy jednak \\(\\widetilde A_n = A_{20n}\\).\nWówczas zbiory \\(\\widetilde A_n\\) są\nniezależne oraz\n\\[\n\\sum_{n=1}^\\infty \\mathbb{P}\\left[\\widetilde A_n\\right]  = \\sum_{n=1}^\\infty \\frac 1{6^{20}} = \\infty.\n\\]\nLemat Borela - Cantelliego pociąga\n\\[\n\\mathbb{P}\\left[\\limsup \\widetilde A_n\\right] =1,\n\\]\nwięc z prawdopodobieństwem \\(1\\) zdarzenia \\(\\widetilde A_n\\) (zatem również \\(A_n\\)) zachodzą nieskończenie wiele razy.\nW szczególności z prawdopodobieństwem \\(1\\) zajdzie przynajmniej jedno ze zdarzeń \\(A_n\\).Ostatni przykład można powtórzyć dla dowolnego ciągu wyników. W szczególności pociąga \nTwierdzenie o nieskończonej liczbie małp,\nktóre można sparafrazować w następujący sposób: jeżeli małpa będzie naciskać w sposób losowy klawisze maszyny pisania przez nieskończenie długi czas, z prawdopodobieństwem jeden napisze ona wszystkie dzieła Mickiewicza.Przykład 5.4  Rzucamy nieskończenie wiele razy niesymetryczną monetą (orzeł wypada na niej z prawdopodobieństwem \\(p\\=1/2\\)).\nNiech \\(A_n\\) oznacza zdarzenie, że w pierwszych \\(n\\) rzutach wypadło tyle samo orłów co reszek.\nPokażemy że z prawdopodobieństwem \\(1\\) zachodzi jedynie skończenie wiele zdarzeń \\(A_n\\).\nDla nieparzystej liczby rzutów mamy oczywiście \\(\\mathbb{P}[A_{2n+1}] = 0\\),\nnatomiast dla parzystej liczby\n\\[\n\\mathbb{P}[A_{2n}] = {2n \\choose n} p^n(1-p)^n \\sim \\frac{4^np^n(1-p)^n}{\\sqrt{n\\pi}},\n\\] gdzie ostatnia implikacja wynika ze wzoru Stirlinga\n\\[\\begin{equation}\nn! \\sim \\sqrt{2\\pi n} \\bigg(\\frac{n}{e}\\bigg)^n,\n\\end{equation}\\] zapis \\(a_n\\sim b_n\\) oznacza, że \\(\\lim_{n\\\\infty} \\frac{a_n}{b_n}=1\\).Funkcja \\(x\\4x(1-x)\\) na przedziale \\([0,1]\\) przyjmuje maksimum równe 1 dla \\(x=1/2\\), zatem dla \\(p\\= 1/2\\), \\(4p(1-p)\\),\nz kolei, na mocy kryterium Cauchy’ego oznacza zbieżność szeregu\n\\[\n\\sum_n \\mathbb{P}[A_n] <\\infty.\n\\] Z lematu Borella-Cantallego zdarzenia \\(A_n\\) z prawdopodobieństwem jeden zachodzą\njedynie skończenie wiele razy. (Zauważmy jednak, że liczba tych zdarzeń jest losowa).Jak zmienia się rysunek, gdy \\(p>1/2\\)? Jakie zachowanie obserwujemy dla małych wartości \\(p-1/2\\)?","code":""},{"path":"zmienne-losowe-i-ich-rozkłady.html","id":"zmienne-losowe-i-ich-rozkłady","chapter":"6 Zmienne losowe i ich rozkłady","heading":"6 Zmienne losowe i ich rozkłady","text":"Wróćmy na chwilę paradoksu Bertranda. Wyposażeni w aparat zdobyty tej pory jesteśmy w stanie\ndokładniej przeanalizować ten przykład. Zobaczymy też, w którym kierunku powinniśmy\nrozwijać naszą teorię.Jeżeli losujemy cięciwę pierwszym sposobem, tj. przez wylosowanie jej końców przestrzenią probabilistyczną,\nktóra reprezentuje jest \\(\\Omega_1 = [0,1]^2\\) z \\(\\sigma\\)-ciałem zbiorów borelowskich miarą Lebesgue’.Zdarzenie elementarne \\(\\omega = (\\theta, \\phi)\\) utożsamiamy z cięciwą o końcach\n\\((\\cos(2\\pi \\theta), \\sin(2\\pi \\theta))\\) oraz\n\\((\\cos(2\\pi \\phi), \\sin(2\\pi \\phi))\\). Wówczas długość takiej cięciwy \n\\[\\begin{equation*}\n    X_1(\\omega) = \\sqrt{(\\cos(2\\pi \\theta) -\\cos(2\\pi \\phi))^2 + (\\sin(2\\pi \\theta)-\\sin(2\\pi \\phi))^2}.\n\\end{equation*}\\]\nPostać \\(X_1(\\omega)\\) możemy oczywiście nieco uprościć. Wykorzystując niezmienniczość\nna obroty\n\\[\\begin{multline*}\n    X_1(\\omega) = X_1(\\theta,\\phi) = X_1(\\theta-\\phi,0) \\\\\n    = \\sqrt{(1-\\cos(2\\pi (\\theta -\\phi))^2 + (\\sin(2\\pi (\\theta - \\phi)))^2}\n    = 2 \\sin \\left( \\pi|\\theta-\\phi| \\right).\n\\end{multline*}\\]\nZauważmy, że interesująca nas wielkość jest funkcją rzeczywistą\nzdarzenia elementarnego, tj. \\(X_1 \\colon \\Omega \\\\mathbb{R}\\).Załóżmy teraz, że losujemy cięciwę poprzez wylosowanie jej środka.\nWówczas \\(\\Omega_2\\) jest kołem jednostkowym z \\(\\sigma\\)-ciałem zbiorów Borelowskich miarą Lebesgue’.Jeżeli \\(\\omega =(x,y) \\\\Omega_2\\) jest środkiem cięciwy, z twierdzenia Pitagorasa jej długość jest dana przez\n\\[\\begin{equation*}\n    X_2(\\omega) = 2 \\sqrt{1-x^2-y^2}.\n\\end{equation*}\\]\nWreszcie losując tylko odległość od środka koła wybieramy \\(\\Omega_3=[0,1]\\) z \\(\\sigma\\)-ciałem zbiorów Borelowskich\nmiarą Lebesgue’. Wówczas dla \\(\\omega \\\\Omega_3\\) długość wylosowanej cięciwy \n\\[\\begin{equation*}\n    X_3(\\omega) = 2\\sqrt{1-\\omega^2}.\n\\end{equation*}\\]\nwszystkich trzech przykładach otrzymaliśmy różne przestrzenie probabilistyczne.\nAby rzeczywiście stwierdzić, że powyższe trzy metody nie są równoważne musimy\nporównać funkcje \\(X_i\\colon \\Omega_i \\\\mathbb{R}\\) dla \\(=1,2,3\\) pod kątem probabilistycznym.\nPowyższe stanowi przykład sytuacji, w której bardziej od samych zdarzeń elementarnych\ninteresują nas wartości funkcji zdarzeń elementarnych. Pamiętajmy, że struktura przestrzeni \\(\\Omega\\) może być niemal dowolna.\nElementami przestrzeni zdarzeń elementarnych mogą być liczby,\nwyniki rzutów monetą, ciągi, zbiory (kart), wykresy (kursy akcji) itd.\nNas jednak zazwyczaj interesuje bardzo konkretna, często liczbowa, informacja.","code":""},{"path":"zmienne-losowe-i-ich-rozkłady.html","id":"funkcje-mierzalne","chapter":"6 Zmienne losowe i ich rozkłady","heading":"Funkcje mierzalne","text":"Pracując na ogólnej przestrzeni probabilistycznej \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\)\nbędziemy chcieli analizować funkcje \\(X \\colon \\Omega \\\\mathbb{R}\\) z punktu widzenia\nprawdopodobieństwa \\(\\mathbb{P}\\). Oznacza , że od funkcji \\(X\\) będziemy wymagali\nodpowiedniej regularności.Definicja 6.1  Niech \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\)\nbędzie przestrzenią probabilistyczną. Zmienna losowa jest dowolna mierzalna funkcja\n\\(X:(\\Omega, \\mathcal{F})\\(\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))\\).Przypomnijmy, że \\(X:(\\Omega, \\mathcal{F})\\(\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))\\) jest mierzalna gdy\ndla każdego \\(\\\\mathcal{B}(\\mathbb{R})\\),\n\\[\nX^{-1}()= \\{ \\omega \\\\Omega \\: : \\: X(\\omega) \\\\}\\\\mathcal{F}.\n\\]Przykład 6.1  Jeżeli rzucamy pięć razy kostką, \n\\(\\Omega = \\{(i_1,\\ldots, i_5), i_j \\\\{1,2,\\ldots, 6\\}\\}\\), \\(\\omega = (\\omega_1,\\ldots, \\omega_5)\\).\nJeśli chcemy obliczyć sumę wyników (nie interesują nas konkretne wyniki rzutów, ale suma oczek). Wówczas\nrozważamy \\(X(\\omega) = \\omega_1+\\ldots + \\omega_5\\). Wtedy \\(X \\colon \\Omega \\\\mathbb{R}\\) jest zmienną losową.Remark. Jeżeli \\(\\Omega\\) jest przeliczalny \\(\\mathcal{F} = 2^\\Omega\\), każde odwzorowanie\n\\(X:\\Omega\\mapsto \\mathbb{R}\\) jest zmienną losową (jeżeli \\(\\mathcal{F}\\= 2^\\Omega\\) nie musi być już prawdą).Remark. \\(X\\) jest zmienną losową jeżeli dla każdego \\(t\\\\mathbb{R}\\), \\(X^{-1}((-\\infty,t])\\\\mathcal{F}\\).Twierdzenie 6.1  Jeżeli \\(X_1,X_2,\\ldots\\) są zmiennymi losowymi, \\(X_1+X_2\\), \\(X_1-X_2\\), \\(X_1\\cdot X_2\\), \\(X_1/X_2\\) (\\(X_2\\=0\\)) są zmiennymi losowymi.Jeżeli \\(f:\\mathbb{R}^n\\mapsto \\mathbb{R}\\) jest mierzalne, \\(f(X_1,\\ldots, X_n)\\) jest zmienną losową.\\(\\inf_n X_n\\), \\(\\sup_n X_n\\), \\(\\limsup_n X_n\\), \\(\\liminf_n X_n\\) są zmiennymi losowymi.Proof. Pozostawiamy jako zadanie.","code":""},{"path":"zmienne-losowe-i-ich-rozkłady.html","id":"rozkłady-zmiennych-losowych","chapter":"6 Zmienne losowe i ich rozkłady","heading":"Rozkłady zmiennych losowych","text":"Chcąc porównać dwie zmienne losowe \\(X_1 \\colon \\Omega_1 \\\\mathbb{R}\\) oraz \\(X_2 \\colon \\Omega_2 \\\\mathbb{R}\\)\nzdefiniowane na dwóch różnych przestrzeniach probabilistycznych musimy znaleźć sposób\nna reprezentację ich na pewnej wspólnej przestrzeni. Tak się akurat składa, że obie\nfunkcje mają takie przeciwdziedziny. Jeżeli \\(X\\) jest zmienną losową, można jej użyć \nprzetransportowania miary \\(\\mathbb{P}\\) na \\((\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))\\).Definicja 6.2  Miarę \\(\\mu_X\\) na \\((\\mathbb{R},\\mathcal{B}(\\mathbb{R}))\\) zdefiniowaną wzorem\n\\[\n  \\mu_X(B) = \\mathbb{P}[X\\B] = \\mathbb{P}\\left[ \\{\\omega\\\\Omega:\\; X(\\omega)\\B\\} \\right] = \\mathbb{P}[X^{-1}(B)]\n  \\] dla każdego \\(B\\\\mathcal{B}(\\mathbb{R})\\), nazywamy rozkładem zmiennej losowej \\(X\\).W ten sposób zdefiniowaliśmy nową przestrzeń probabilistyczną \\((\\mathbb{R}, \\mathcal{B}(\\mathbb{R}),\\mu_X)\\).\nnie jest już abstrakcyjna przestrzeń \\(\\Omega\\), ale\nprzestrzeń o której sporo wiemy (dysponujemy narzędziami analitycznymi, na \\(\\mathbb{R}\\) zachodzi twierdzenie Kołmogorowa).Przykład 6.2  Wykonujemy \\(n\\) prób Bernoulliego z prawdopodobieństwem sukcesu \\(p\\) w pojedynczej próbie.\nNiech \\(S_n\\) będzie liczbą sukcesów. Wówczas\n\\[\\begin{equation*}\n    \\mathbb{P}[S_n=k]=p_k={n \\choose k}p^k(1-p)^{n-k}.\n\\end{equation*}\\]\nOznacza , że\n\\[\\begin{equation*}\n    \\mathbb{P}[S_n\\B] = \\sum_{k \\B} p_k = \\sum_{k=0}^n p_k\\delta_k(B).\n\\end{equation*}\\]\nInnymi słowy rozkład \\(\\mu_{S_n}\\) jest równy\n\\[\\begin{equation*}\n    \\mu_{S_n}(\\cdot) = \\sum_{k=0}^n p_k\\delta_k(\\cdot).\n\\end{equation*}\\]Powyższy przykład można bardzo łatwo uogólnić. Rzeczywiście, jeżeli zmienna losowa \\(X\\)\njest taka, że istnieje przeliczalny zbiór \\(\\{x_k\\}_{k \\\\mathbb{N}}\\) taki, że\n\\[\\begin{equation*}\n    \\sum_{k=1}^\\infty\\mathbb{P}[X-x_k] =1,\n\\end{equation*}\\]\njej rozkład zadany jest przez\n\\[\\begin{equation*}\n    \\mu_X(\\cdot)=\\sum_{k=1}^\\infty p_k\\delta_{x_k}(\\cdot),\n\\end{equation*}\\]\ngdzie\n\\[\\begin{equation*}\n    p_k = \\mathbb{P}[X = x_k], \\quad k \\\\mathbb{N}.\n\\end{equation*}\\]\nPowyższe stosuje się każdej zmiennej losowej określonej na zbiorze przeliczalnym.\nAby móc w podobny sposób analizować zmienne losowe określone na większych przestrzeniach\nmusimy wprowadzić dodatkowy aparat.Definicja 6.3  Dystrybuantą zmiennej losowej \\(X\\) nazywamy funkcję \\(F:\\; \\mathbb{R} \\[0,1]\\) zadaną wzorem\n\\[\n  F(t) = \\mathbb{P}[X\\le t] = \\mu_X\\left((-\\infty,t]\\right).\n  \\]Przykład 6.3  Rzucamy monetą. \\(\\Omega = \\{O,R\\}\\), \\(X(O)=1\\), \\(X(R) = -1\\). Wtedy\n\\[\n  F(t) = \\mathbb{P}[X\\le t] = \\left\\{\n  \\begin{array}{cc}\n    0 & \\mbox{ dla } t < -1 \\\\\n    1/2 & \\mbox{ dla } -1\\le t < 1 \\\\\n    1 &  \\mbox{ dla } t \\ge 1.\n  \\end{array}\n  \\right.\n  \\]Przykład 6.4  Losowa liczba z przedziału \\([0,1]\\): \\((\\Omega, \\mathcal{F}, \\mathbb{P})\n  = ([0,1], \\mathcal{B}([0,1]), \\lambda_1)\\), \\(X(\\omega) = \\omega\\).\n\\[\n  F(t) = \\mathbb{P}[X\\le t] = \\left\\{\n  \\begin{array}{cc}\n    0 & \\mbox{ dla } t < 0 \\\\\n    t & \\mbox{ dla } 0\\le t < 1 \\\\\n    1 &  \\mbox{ dla } t \\ge  1.\n  \\end{array}\n  \\right.\n  \\]Przykład 6.5  Rozważmy pierwszy sposób losowania w paradoksie Bertranda.\n\\[\n  F(t) = \\mathbb{P}[X_1\\le t] = \\left\\{\n  \\begin{array}{cc}\n    0 & \\mbox{ dla } t < 0 \\\\\n    \\frac{2}{\\pi} \\arcsin(t/2) & \\mbox{ dla } 0\\le t < 2 \\\\\n    1 &  \\mbox{ dla } t \\ge  2.\n  \\end{array}\n  \\right.\n\\]Twierdzenie 6.2  (Własności dystrybuanty) Niech \\(F\\) będzie dystrybuantą pewnej zmiennej losowej \\(X\\). Wówczas\\(F\\) jest niemalejąca.\\(\\lim_{t\\-\\infty} F(t) = 0\\), \\(\\lim_{t\\ \\infty} F(t) = 1\\).\\(F\\) jest prawostronnie ciągła.dla dowolnego \\(t\\\\mathbb{R}\\) istnieje lewostronna granica\n\\[F(t-) = \\lim_{s\\t^-}F(s) = \\mathbb{P}[X<t].\\]\\(F\\) jest nieciągła w punkcie \\(t_0\\) wtedy tylko wtedy, gdy \\(\\mathbb{P}[X = t_0] > 0\\). Wówczas\n\\(\\mathbb{P}[X=t_0] = F(t_0) - F(t_0-)\\). Punkt \\(t_0\\) nazywamy wówczas atomem rozkładu.Proof. Punkt 1. Jeżeli \\(t_1<t_2\\), zachodzi inkluzja \\((-\\infty,t_1] \\subset (-\\infty, t_2]\\). Wówczas:\n\\[\nF(t_1) = \\mu((-\\infty,t_1]) \\leq \\mu((-\\infty, t_2]) = F(t_2).\n\\]Punkt 2. Niech \\(\\{t_n\\}\\) będzie dowolnym ciągiem rosnącym \\(+\\infty\\). Wówczas rodzina zbiorów \\((-\\infty,t_n]\\) jest rosnąca, ponadto:\n\\[\n\\mathbb{R} = \\bigcup_n (-\\infty,t_n].\n\\]\nZ twierdzenia o ciągłości otrzymujemy:\n\\[\n\\lim_{n\\\\infty} F(t_n) = \\lim_{n\\\\infty} \\mu((-\\infty,t_n]) = \\mu(\\mathbb{R}) = 1.\n\\]\nAnalogicznie dowodzimy drugiej części.Punkt 3. Ustalmy \\(t \\\\mathbb{R}\\) niech \\(\\{t_n\\}\\) będzie ciągiem malejącym \\(t\\). Wówczas ciąg przedziałów \\((t,t_n]\\) jest malejący spełnia:\n\\[\n\\bigcap_n (t,t_n] = \\emptyset.\n\\]\nZ twierdzenia o ciągłości otrzymujemy:\n\\[\\begin{multline*}\n\\lim_{n\\\\infty} \\left( F(t_n) - F(t) \\right)\n= \\lim_{n\\\\infty} \\left( \\mu((-\\infty,t_n]) - \\mu((-\\infty, t]) \\right)\\\\\n= \\lim_{n\\\\infty} \\mu((t,t_n]) = \\mu(\\emptyset) = 0.\n\\end{multline*}\\]Punkt 4. Dowód przebiega analogicznie jak w punkcie 3.Punkt 5. Jest konsekwencją punktu 4.Twierdzenie 6.3  Jeżeli \\(F\\) jest funkcją na \\(\\mathbb{R}\\) spełniającą warunki 1,2 3 z poprzedniego twierdzenia:\\(F\\) jest niemalejąca;\\(\\lim_{t\\-\\infty} F(t) = 0\\), \\(\\lim_{t\\ \\infty} F(t)\\) = 1;\\(F\\) jest prawostronnie ciągła;\\(F\\) jest dystrybuantą pewnego rozkładu.Proof. Naszym celem jest skonstruowanie przestrzeni probabilistycznej\n\\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) oraz zmiennej losowej \\(X\\) na niej określonej\ntakiej, że\n\\(F\\) jest dystrybuantą \\(X\\), tzn.\n\\[\\begin{equation}\n    F(t) = \\mathbb{P}[X\\le t]\n\\tag{6.1}\n\\end{equation}\\]\nZdefiniujmy \\((\\Omega, \\mathcal{F}, \\mathbb{P}) = \\big( (0,1), \\mathcal{B}((0,1)), {\\rm Leb} \\big)\\).Załóżmy najpierw, że funkcja \\(F\\) jest odwracalna zdefiniujmy\n\\[\nX(\\omega) = F^{-1}(\\omega).\n\\]\nMusimy najpierw sprawdzić, że \\(X\\) jest mierzalna.\nZauważmy, że dla dowolnego \\(t\\\\mathbb{R}\\),\n\\[\nX^{-1}((-\\infty, t]) = \\{\\omega:\\; X(\\omega) \\le t \\} = \\{\\omega:\\; \\omega \\le F(t)\\} = [0,F(t)] \\\\mathcal{B}(0,1).\n\\]\nDla sprawdzenia (6.1) piszemy\n\\[\n\\mathbb{P}[X(\\omega) \\le t] = \\mathbb{R}[ \\omega \\le F(t)] = F(t).\n\\]\nRozważmy teraz ogólny przypadek.\nZdefiniujmy zmienną losową \\(X\\)\njako uogólnioną funkcję odwrotną \\(F\\):\n\\[\\begin{equation}\nX(\\omega) = F^{-1}(\\omega):= \\sup\\{y\\\\mathbb{R}:\\; F(y)< \\omega\\}.\n\\tag{6.2}  \n\\end{equation}\\]\nPokażemy, że \\(F\\) jest dystrybuantą \\(X\\), tzn. zachodzi (6.1).\nW tym celu wystarczy pokazać\n\\[\\begin{equation}\n\\{ \\omega:\\; X(\\omega) \\le t \\} =\\{\\omega:\\; \\omega \\le F(t)\\}\n\\tag{6.3}\n\\end{equation}\\]\ndla każdego \\(t\\).Oznaczmy przez \\(L\\) (\\(P\\)) zbiór po lewej (prawej) stronie formuły (6.3).\nPokażemy najpierw, że \\(L\\supset P\\). Istotnie, niech\n\\(\\omega\\P\\), tzn. \\(\\omega \\le F(t)\\).\nWtedy \\[t\\notin \\{ y\\\\mathbb{R}:\\; F(y)< \\omega\\}.\\]\nSkoro \\(F\\) jest monotoniczna, \n\\[\nt \\geq \\sup\\{ y \\\\mathbb{R} \\: : \\: F(y)<\\omega\\} = F^{-1}(\\omega)=X(\\omega)\n\\]\nzatem \\(X(\\omega) \\le t\\).dowodu odwrotnej implikacji, niech \\(\\omega\\notin P\\), czyli \\(\\omega >F(t)\\).\nKorzystając z prawostronnej ciągłości dystrybuanty \\(F\\),\nistnieje \\(\\varepsilon > 0\\) tż. \\(F(t+\\varepsilon) < \\omega\\).\nZatem \\[t+\\varepsilon \\\\{ y\\\\mathbb{R}:\\; F(y) < \\omega \\}.\\]\nStąd\n\\[\nt<t+\\varepsilon \\leq \\sup\\{ y \\\\mathbb{R} \\: : \\: F(y)<\\omega\\} = F^{-1}(\\omega) = X(\\omega).\n\\]\nSkoro\n\\(X(\\omega)>t\\), czyli \\(\\omega \\notin L\\).\nZ równości (6.3) wynika, że \\(X\\) jest zmienną losową.\nAby pokazać, że \\(X\\) ma zadaną dystrybuantę\n\\[\n\\mathbb{P}[X\\le t] = \\mathbb{P}\\left[ \\{ \\omega:\\; X(\\omega) \\le t \\}\\right]\n=\\mathbb{P}\\left[\\{\\omega:\\; \\omega \\le F(t)\\}\\right] = F(t),\n\\]\ngdzie ostatnia równość wynika z definicji miary probabilistycznej \\(\\mathbb{P}\\),\nktóra jest miarą Lebesgue’na \\([0,1]\\).Twierdzenie 6.4  (Twierdzenie o jednoznaczności) Dystrybuanta zmiennej losowej \\(X\\) wyznacza jednoznacznie jej rozkład.Powyższy wynik jest konsekwencją lematu o \\(\\pi-\\lambda\\) układach zwanym też twierdzeniem Dynkina.Definicja 6.4  Niepustą rodzinę \\(\\mathcal{K}\\) podzbiorów \\(\\Omega\\) nazywamy \\(\\pi\\)-układem,\njeżeli jest zamknięta na operację przekroju, tzn. \\(\\cap B\\\\mathcal{K}\\) dla wszystkich \\(,B\\\\mathcal{K}\\).Definicja 6.5  Niepustą rodzinę \\(\\mathcal{L}\\) podzbiorów \\(\\Omega\\) nazywamy \\(\\lambda\\)-układem,\njeżeli\\(\\Omega \\\\mathcal{L}\\).jeżeli \\(,B\\\\mathcal{L}\\) \\(\\subset B\\), \\(B\\setminus \\\\mathcal{L}\\).jeżeli \\(A_1,A_2,...\\) jest wstępującym ciągiem elementów \\(\\mathcal{L}\\),\n\\(\\bigcup_{n=1}^\\infty A_n \\\\mathcal{L}\\).Lemma 6.1  (Dynkin) Jeżeli \\(\\mathcal{L}\\) jest \\(\\lambda\\)-układem zawierającym \\(\\pi\\)-układ \\(\\mathcal{K}\\),\n\\(\\mathcal{L}\\) zawiera także \\(\\sigma(\\mathcal{K})\\),\n\\(\\sigma\\)-ciało generowane przez \\(\\mathcal{K}\\).Proof. Krok 1. Pokażemy najpierw, że jeżeli \\(\\mathcal{L}\\) jest jednocześnie \\(\\pi\\)-układem oraz \\(\\lambda\\)-układem, jest \\(\\sigma\\)-ciałem. Istotnie\n\\(\\mathcal{L}\\) jest zamknięte na operację sumy: jeżeli \\(,B\\\\mathcal{L}\\), \n\\[\n\\cup B = \\cup \\big(  B\\setminus \\cap B \\big) =\n\\big( ^c \\setminus (B\\setminus \\cap B)   \\big)^c \\\\mathcal{L}.\n\\] Przez indukcję pokazuje się, że \\(\\mathcal{L}\\) jest zamknięte na skończone sumy,\ntzn. jeżeli \\(A_1,\\ldots, A_n\\\\mathcal{L}\\), \n\\(A_1\\cup A_2\\cup\\ldots\\cup A_n\\\\mathcal{L}\\).\nZałóżmy teraz, że \\(\\{A_n\\}_{n\\\\mathbb{N}}\\) jest przeliczalnym ciągiem elementów \\(\\mathcal{L}\\),\nwówczas \\(\\bigcup_{k=1}^n A_k\\) jest ciągiem wstępującym elementów \\(\\mathcal{L}\\), stąd\n\\[\n\\bigcup_{n=1}^\\infty A_n = \\bigcup_{n=1}^\\infty \\bigcup_{k=1}^n A_k \\\\mathcal{L},\n\\] zatem \\(\\mathcal{L}\\) jest \\(\\sigma\\)-ciałem.Krok 2. Niech \\(\\mathcal{L}_0\\) będzie przekrojem wszystkich \\(\\lambda\\)-układów\nzawierających \\(\\mathcal{K}\\). Wystarczy pokazać, że \\(\\mathcal{L}_0\\) jest \\(\\pi\\)-układem,\nbo wówczas z kroku 1 jest \\(\\sigma\\)-ciałem \n\\[\n\\mathcal{K} \\subset \\sigma(\\mathcal{K}) \\subset \\mathcal{L}_0 \\subset \\mathcal{L}.\n\\]\nUstalmy \\(\\\\mathcal{K}\\) rozważmy rodzinę\n\\[\\mathcal{K}_1^= \\{    B\\subset \\Omega:\\; \\cap B\\\\mathcal{L}_0   \\}.\\]\nWówczas \\(\\mathcal{K}\\subset  \\mathcal{K}_1^\\) (bo \\(\\mathcal{K}\\) jest \\(\\pi\\)-układem),\nale ponadto \\(\\mathcal{K}_1^\\) jest \\(\\lambda\\)-układem:\\(\\Omega \\\\mathcal{K}_1^\\);jeżeli \\(B_1,B_2\\\\mathcal{K}_1^\\) oraz \\(B_1\\subset B_2\\), \n\\[\\cap (B_2\\setminus B_1) = (\\cap B_2) \\setminus (\\cap B_1) \\\\mathcal{L}_0;\\]jeżeli \\(B_n\\) jest wstępującą rodziną elementów \\(\\mathcal{K}_1^\\), \n\\[\n\\cap \\bigg(\\bigcup B_n \\bigg) = \\bigcup (\\cap B_n) \\\\mathcal{L}_0\n\\] zatem \\(\\bigcup B_n \\\\mathcal{K}_1^\\).Pokazaliśmy, że \\(\\mathcal{K}_1^\\) jest \\(\\lambda\\)-układem zawierającym \\(\\mathcal{K}\\)\nStąd wynika, że \\(\\mathcal{L}_0 \\subseteq \\mathcal{K}_1^.\\).\nCzyli jeżeli \\(\\\\mathcal{K}, B\\\\mathcal{L}_0\\), \\(\\cap B \\\\mathcal{L}_0\\).Następnie ustalmy dowolny zbiór\n\\(B\\\\mathcal{L}_0\\) zdefiniujmy \\[\\mathcal{K}_2^B = \\{ : \\cap B\\\\mathcal{L}_0  \\}.\\]\nsamo rozumowanie co powyżej uzasadnia, że \\(\\mathcal{K}_2^B\\) jest \\(\\lambda\\)-układem oraz\nzawiera rodzinę \\(\\mathcal{K}\\), więc \\(\\mathcal{L}_0 \\subseteq \\mathcal{K}_2^B\\).\nPodsumowując pokazaliśmy, że jeżeli\n\\(,B\\\\mathcal{L}_0\\), \\(\\cap B\\\\mathcal{L}_0\\), więc \\(\\mathcal{L}_0\\) jest \\(\\pi\\)-układem,\nco kończy dowód.Proof. Dowód twierdzenia o jednoznaczności (Twierdzenie 6.4)\nChcemy pokazać, że jeżeli \\(X\\) \\(Y\\) są dwoma zmiennymi losowymi o tej samej dystrybuancie \\(F\\), muszą mieć te rozkłady:\n\\[\\begin{equation}\n\\mu_X(B) = \\mu_Y(B),\n\\tag{6.4}\n\\end{equation}\\]\ndla wszystkich \\(B\\\\mathcal{B}(\\mathbb{R})\\). (Zauważmy, że \\(X\\) \\(Y\\) mogą być\nokreślone na różnych przestrzeniach probabilistycznych. Pokazujemy równość\nrozkładów, nie równość zmiennych losowych.)Z definicji dystrybuanty wynika, że (6.4) zachodzi dla wszystkich zbiorów\npostaci \\((-\\infty,t]\\). Oznaczmy te zbiory przez \\(\\mathcal{K}\\). Tworzą one \\(\\pi\\)-układ. Niech\n\\[\n\\mathcal{L} = \\big\\{ \\\\mathcal{B}(\\mathbb{R}):\\; \\mu_X() = \\mu_Y()  \\big\\}.\n\\] Rodzina \\(\\mathcal{L}\\) jest \\(\\lambda\\)-układem. Zatem z lematu o \\(\\pi-\\lambda\\) układach\n\\[\n\\mathcal{L}\\supset \\sigma(\\mathcal{K}) = \\mathcal{B}((\\mathbb{R}).\n\\] pokazuje, że (6.4) zachodzi dla wszystkich zbiorów borelowskich,\nwięc obie miary \\(\\mu_X\\) \\(\\mu_Y\\) są równe.","code":""},{"path":"wartość-oczekiwana-definicja-i-własności.html","id":"wartość-oczekiwana-definicja-i-własności","chapter":"7 Wartość oczekiwana: definicja i własności","heading":"7 Wartość oczekiwana: definicja i własności","text":"Definicja 7.1  Niech \\(X\\) będzie zmienną losową określoną na przestrzeni probabilistycznej\n\\((\\Omega, \\mathcal{F}, \\mathbb{P})\\). Mówimy, że \\(X\\) ma wartość oczekiwaną jeżeli\n\\[\n  \\int_\\Omega |X| \\mathrm{d} \\mathbb{P} = \\int_\\Omega |X(\\omega)|\\mathbb{P}(\\mathrm{d}\\omega) <\\infty.\n\\]\nWówczas wartością oczekiwaną zmiennej losowej \\(X\\) nazywamy liczbę\n\\[\n    \\mathbb{E}[ X ] =  \\int_\\Omega X \\mathrm{d}\\mathbb{P} =\n    \\int_{\\Omega}X(\\omega) \\mathbb{P}(\\mathrm{d}\\omega).\n\\]Przykład 7.1  Rzucamy kostką. Niech \\(X\\) oznacza liczbę wyrzuconych oczek.\nWówczas, dla \\(\\Omega = [6]=\\{1,2, \\ldots, 6\\}\\),\n\\[\\begin{equation*}\n    \\mathbb{E}[X] = \\sum_{k=1}^6 k \\mathbb{P}[\\{ k\\}] = 3,5.\n\\end{equation*}\\]Jak widzimy po powyższym przykładzie nazwy “wartość oczekiwana” nie należy interpretować\ndosłownie. W rzucie kostką nie będziemy oczekiwać wartości \\(3,5\\).Zauważmy, że jeżeli \\(\\Omega = \\{\\omega_j\\}_j\\) jest przestrzenią dyskretną\n\\(\\mathbb{P}[\\{\\omega_i\\}]=p_i\\), \n\\[\n\\mathbb{E} [X] =\\sum_{=1}^n X(\\omega_i)p_i.\n\\]\nWartość oczekiwaną należy rozumieć jako średnią ważoną \\(X\\).Istnieje jeszcze jedno spojrzenie na wartość oczekiwaną. Załóżmy, że rzucamy kostką \\(n\\) razy.\nNiech \\(X_j\\) oznacza wynik \\(j\\)tego rzutu. Wówczas średnia empiryczna (uzyskanych wyników) \n\\[\\begin{equation*}\n    \\frac 1n \\sum_{j=1}^n X_j.\n\\end{equation*}\\]\nMożemy ją napisać równoważnie jako\n\\[\\begin{multline*}\n    \\frac 1n \\sum_{j=1}^n X_j=\n    1 \\frac{\\# \\{ j \\leq n \\: : \\: X_j=1\\}}{n}+\n    2 \\frac{\\# \\{ j \\leq n \\: : \\: X_j=2\\}}{n}+ \\ldots\\\\    \n    +6 \\frac{\\# \\{ j \\leq n \\: : \\: X_j=6\\}}{n}.\n\\end{multline*}\\]\nJeżeli \\(n\\) jest bardzo duże, spodziewamy się, że\n\\[\\begin{equation*}\n\\frac{\\# \\{ j \\leq n \\: : \\: X_j=k\\}}{n} \\\\mathbb{P}[\\{k\\}]=\\frac 16.\n\\end{equation*}\\]\nDokładne uzasadnienie powyższego faktu zobaczymy w dalszej części wykładu.\nChcąc zaagitować za naturalnością przyjętej przez nas definicji wartości oczekiwanej\nprzyjmijmy go chwilowo za prawdziwy. Wówczas średnia empiryczna zbiega \n\\[\\begin{equation*}\n    \\frac 1n \\sum_{j=1}^n X_j \\\\sum_{k=1}^6 k \\mathbb{P}[\\{ k\\}]\n\\end{equation*}\\]\nczyli wartości oczekiwanej wyniku jednego rzutu. Powyższy heurystyczny argument nie\njest zależny od wyważenia czy też liczby ścian na kostce.Przykład 7.2  Dwaj gracze \\(\\) \\(B\\) grają w następującą grę.\nRzucają kostką. Niech \\(k\\) będzie wynikiem rzutu.\nJeżeli \\(k\\) jest nieparzyste, \\(\\) wygrywa \\(k\\) złotych, jeżeli \\(k\\) jest parzyste,\n\\(B\\) wygrywa \\(k\\) złotych. Czy gra jest uczciwa?Wartość oczekiwana jest najważniejszym parametrem decydującym o opłacalności np. gier\nhazardowych. MIT Students Won 8 Millions Massachusetts Lottery.Twierdzenie 7.1  Załóżmy, że \\(X\\) \\(Y\\) są zmiennymi losowymi takimi, że \\(\\mathbb{E} X\\) \\(\\mathbb{E} Y\\) istnieją.\nWtedyJeżeli \\(X\\ge 0\\), \\(\\mathbb{E} [X] \\ge 0\\).\\(|\\mathbb{E} [X]| \\le \\mathbb{E} [|X|]\\).Dla dowolnych \\(,b\\\\mathbb{R}\\)\n\\[\n\\mathbb{E}[aX+] = \\mathbb{E} [X] + b\\mathbb{E}[Y].\n\\]Proof. Wszystkie własności są konsekwencją definicji wartości oczekiwanej jako całki z funkcji\nmierzalnej \\(X\\).\nTrzeci punkt wynika z liniowości całki\n\\[\\begin{multline*}\n    \\mathbb{E}[AX+] = \\int_\\Omega aX+\\mathrm{d}\\mathbb{P} =\\\\\n    \\int_\\Omega X\\mathrm{d}\\mathbb{P} +\n    b\\int_\\Omega Y \\mathrm{d}\\mathbb{P}\n    =\\mathbb{E}[X]+b\\mathbb{E}[Y].\n\\end{multline*}\\]Jeżeli \\(X \\geq 0\\), \n\\[\\begin{equation*}\n    \\mathbb{E}[X]=\\int_\\Omega X \\mathrm{d}\\mathbb{P} \\geq 0.\n\\end{equation*}\\]\nPodobnie jeżeli \\(X \\geq Y\\), \n\\[\\begin{equation*}\n    \\int_\\Omega X \\mathrm{d}\\mathbb{P}\n\\geq \\int_\\Omega Y \\mathrm{d}\\mathbb{P}\n\\end{equation*}\\]\npoprzez chociażby zastosowanie poprzedniego pierwszego punktu zmiennej losowej \\(X'=X-Y\\).\nSkoro \\(-|X|\\leq X \\leq |X|\\), \n\\[\\begin{equation*}\n    -\\int_\\Omega |X| \\mathrm{d}\\mathbb{P}\n\\leq \\int_\\Omega X \\mathrm{d}\\mathbb{P}\n\\leq \\int_\\Omega |X| \\mathrm{d}\\mathbb{P}\n\\end{equation*}\\]\nczyli\n\\[\\begin{equation*}\n-\\mathbb{E}[|X|]\\leq \\mathbb{E}[X] \\leq \\mathbb{E}[|X|]\n\\end{equation*}\\]\nco pociąga drugi punkt.Przykład 7.3  Rzucamy \\(100\\) razy kostką. Niech \\(X\\) oznacza sumę wyrzuconych oczek. Jaka jest wartość oczekiwana \\(X\\)?\nBezpośrednie użycie definicji jest kłopotliwe, ponieważ przestrzeń zdarzeń elementarnych jest\nbardzo duża. Rozważymy inne podejście.\nNiech \\(X_i\\) oznacza liczbę oczek uzyskanych w \\(\\)-tym rzucie. Wtedy\n\\[\nX = \\sum_{=1}^{100} X_i.\n\\]\nPonadto \\(\\mathbb{E} X_i = 3,5\\).\nZatem z liniowości wartości oczekiwanej\n\\[\n\\mathbb{E} [X] = \\sum_{=1}^{100} \\mathbb{E} [X_i] = 350.\n\\]Przykład 7.4  Niech \\(\\sigma\\) będzie losową permutacją zbioru \\(\\{1,\\ldots,n\\}\\)\n(losujemy jednostajnie element grupy permutacji \\(S_n\\)).\nNiech \\(X\\) oznacza liczbę punktów stałych. Obliczymy \\(\\mathbb{E} X\\).\nTen przykład ma wiele alternatywnych sformułowań np. wkładamy losowo \\(n\\)\nlistów zaadresowanych kopert, ilu adresatów otrzyma właściwy list.\nOznaczmy\n\\[X_i = \\left\\{\\begin{array}{cc}\n                 1 & \\mbox{ jeżeli $$-ty punkt jest stały} \\\\\n                 0 & \\mbox{ jeżeli $$-ty punkt nie jest stały}\n               \\end{array}\n\\right.\n\\]\nJeżeli oznaczymy przez \\(A_i\\) zdarzenie, że \\(\\)-ty punkt jest stały, \\(X_i = \\mathbf{1}_{A_i}\\).\nWtedy\n\\[\n\\mathbb{E} [X_i] = \\int_\\Omega \\mathbf{1}_{A_i}(\\omega) \\mathbb{P}(\\mathrm{d}\\omega) = \\mathbb{P}[A_i] = 1/n\n\\]\noraz \\(X = \\sum_{=1}^n X_i\\), stąd\n\\[\\mathbb{E} [X]  = \\sum_{=1}^n \\mathbb{E} [X_i] = n\\cdot \\frac 1n = 1.\\]Twierdzenie 7.2  Załóżmy, że \\(\\{X_n\\}_{n \\\\mathbb{N}}\\) jest ciągiem zmiennych losowych takich,\nże \\(\\mathbb{E} [X_n]\\) istnieją. Wtedy(Lemat Fatou) Jeżeli \\(X\\ge 0\\), \n\\[\n\\mathbb{E} \\left[\\liminf_{n\\\\infty} X_n\\right] \\le \\liminf_{n\\\\infty} \\mathbb{E} \\left[X_n\\right]\n\\](tw. o zbieżności monotonicznej)\nJeżeli \\(X_n\\ge 0\\) oraz \\(\\{X_n\\}_{n \\\\mathbb{N}}\\) jest ciągiem monotonicznym, \n\\[\n            \\mathbb{E} \\left[\\lim_{n\\\\infty} X_n \\right]= \\lim_{n\\\\infty} \\mathbb{E} \\left[X_n\\right]\n        \\](tw. Lebesgue’o zbieżności zmajoryzowanej) Jeżeli \\(|X_n|\\le Y\\) dla pewnej\nzmiennej losowej \\(Y\\) takiej, że\n\\(\\mathbb{E} |Y|<\\infty\\) oraz istnieje granica \\(\\lim_{n\\\\infty} X_n(\\omega) = X(\\omega)\\)\ndla \\(\\mathbb{P}\\)p.k.\\(\\omega\\), \n\\[\n\\mathbb{E} [X] =  \\mathbb{E}\\left[ \\lim_{n\\\\infty} X_n\\right] = \\lim_{n\\\\infty} \\mathbb{E} \\left[X_n\\right].\n\\]","code":""},{"path":"wartość-oczekiwana-zastosowania.html","id":"wartość-oczekiwana-zastosowania","chapter":"8 Wartość oczekiwana: zastosowania","heading":"8 Wartość oczekiwana: zastosowania","text":"Twierdzenie 8.1  Niech \\(f:\\mathbb{R}\\\\mathbb{R}\\) będzie funkcją borelowską, \\(X\\) zmienną\nlosową o rozkładzie \\(\\mu_X\\). Wówczas\n\\[\n  \\mathbb{E}\\left[ f(X)\\right]  = \\int_{\\mathbb{R}} f(x)\\mu_X(\\mathrm{d}x),\n\\]\no ile jedna z tych całek istnieje.Przed dowodem zobaczmy proste zastosowanie tego faktu.Przykład 8.1  Ile średnio należy wykonać rzutów kostką, aby otrzymać pierwszą szóstkę?\nOgólniej wykonujemy doświadczenie o prawdopodobieństwie sukcesu \\(p\\).\nIle średnio razy należy je powtórzyć, aby otrzymać sukces.Niech \\(X\\) będzie liczbą prób, które należy wykonać, aby otrzymać pierwszy sukces. Wówczas\n\\[\n\\mathbb{P}[X=k] = p_k= (1-p)^{k-1}p.\n\\]\nZatem rozkład \\(X\\) zadaje się przez\n\\[\\begin{equation*}\n    \\mu_X(\\cdot) = \\sum_{k=1}^\\infty p_k\\delta_k(\\cdot).\n\\end{equation*}\\]\nWzór z Twierdzenia 8.1 zapisuje się jako\n\\[\\begin{equation*}\n    \\mathbb{E}[f(X)] = \\int_\\mathbb{R} f(x) \\mu_X(\\mathrm{d}x)\n    = \\sum_{k=1}^\\infty f(k) p_k.\n\\end{equation*}\\]\nBiorąc \\(f(x)=x\\) otrzymujemy interesująca nas wartość oczekiwaną \\(X\\),\n\\[\n\\mathbb{E}\\left[ X\\right]  =\n\\sum_{k=1}^{\\infty} kp(1-p)^{k-1} =\np \\sum_{k=1}^{\\infty} k (1-p)^{k-1} = \\frac 1p.\n\\]\nOstatnia równość wynika z różniczkowania szeregu\n\\[\\begin{equation*}\n    \\sum_{k=0}^\\infty q^k = \\frac {1}{1-q}\n\\end{equation*}\\]\nabsolutnie zbieżnego dla \\(q\\(0,1)\\). Po zróżniczkowaniu stronami otrzymujemy\n\\[\\begin{equation*}\n    \\sum_{k=1}^\\infty kq^{k-1} = \\frac{1}{(1-q)^2}.\n\\end{equation*}\\]Proof (Twierdzenia 8.1). Dowód używa standardowych metod z teorii miary.\nPrzybliża się funkcję \\(f\\) funkcjami prostymi.Krok 1.\nJeżeli \\(f = {\\bf 1}_A\\) dla \\(\\\\mathcal{B}(\\mathbb{R})\\), \\(f(X)\\) jest zmienną\nlosową przyjmującą 2 wartości: \\(0\\) \\(1\\).\nMożemy więc napisać\n\\[\\begin{multline*}\n  \\mathbb{E} f(X) = 1\\cdot \\mathbb{P}[X\\] + 0 \\cdot \\mathbb{P}[X\\notin ] = \\mu_X()\\\\ =\n  \\int_A  \\mu_X(\\mathrm{d}x) = \\int_{\\mathbb{R}}  {\\bf 1}_A(x) \\mu_X(\\mathrm{d}x)\n  = \\int_{\\mathbb{R}}  f(x) \\mu(\\mathrm{d}x).\n\\end{multline*}\\]Krok 2. Jeżeli \\(f = \\sum_{=1}^n a_i {\\bf 1}_{A_i}\\) jest funkcją prostą, teza\nwynika z kroku 1 oraz liniowości wartości oczekiwanej.Krok 3. Niech \\(f\\ge 0\\) niech \\(f_n\\) będzie rosnącym ciągiem funkcji\nprostych zbiegającym punktowo \\(f\\). Wtedy teza wynika z twierdzenia o zbieżności monotonicznej.Krok 4. Dla dowolnej funkcji \\(f\\) zapisujemy ją w postaci \\(f = f^+ - f^-\\) korzystamy z kroku 3.Przykład 8.2  Kupujemy \\(k\\) losów w loterii, w której \\(M\\) losów jest przegrywających,\n\\(N\\) wygrywających (zakładamy \\(k\\le M,N\\)).\nNiech \\(X\\) będzie liczbą losów wygrywających wśród\ntych, które kupiliśmy. Znajdziemy \\(\\mathbb{E} [X]\\).1 sposób Zdefiniujmy\n\\[X_i = \\left\\{\\begin{array}{cc}\n                 1 & \\mbox{ jeżeli $$-ty los wygrywa} \\\\\n                 0 & \\mbox{ jeżeli $$-ty los przegrywa}\n               \\end{array}\n\\right.\n\\]\nWtedy \\(X = \\sum_{=1}^k X_i\\) oraz \\(\\mathbb{E} X_i = \\frac{N}{N+M}\\)\n\\[ \\mathbb{E} X = \\sum_{=1}^k \\mathbb{E} X_i  =  \\frac{kN}{N+M}.\\]\n2 sposób Szkolna metoda:\n\\[\n\\mathbb{E} X = \\sum_{j=0}^k j \\mathbb{P}[X=j] = \\sum_{j=0}^k j\\cdot \\frac{{N\\choose j}{M\\choose k-j}}{{N+M \\choose k}}\n\\]\nObie metody muszą prowadzić tego samego wyniku, stąd\n\\[\n\\sum_{j=0}^k j\\cdot \\frac{{N\\choose j}{M\\choose k-j}}{{N+M \\choose k}} = \\frac{kN}{N+M}.\n\\]Przykład 8.3  Tasujemy talię 52 kart metodą Top random, tzn. kartę, która znajduje się na górze\nwkładamy w losowe miejsce w talii (są 52 możliwości), następnie powtarzamy czynność.\nIle należy wykonać tasowań, aby można było uznać talię za potasowaną.\nJaka jest średnia liczba tasowań?Kluczowa jest karta, która początkowo znajduje się na samym dole talii. Oznaczmy ją przez \\(\\).\nZauważmy, że w trakcie tasowania będzie się ona przemieszczać powoli góry talii.\nPrawdopodobieństwo, że w pierwszym ruchu karta z góry talii znajdzie się pod \\(\\) wynosi \\(1/52\\).\nJest ono małe, ale z prawdopodobieństwem 1 (Lemat Borela-Cantellego) nastąpi moment,\ngdy pod \\(\\) znajdzie się jakaś karta, oznaczmy ją przez \\(K_1\\).Po wykonaniu losowej liczby tasowań kolejna karta \\(K_2\\) zostanie wstawiona poniżej \\(\\).\nWzajemne ułożenie kart \\(K_1\\) \\(K_2\\) jest losowe (tzn. z prawdopodobieństwem \\(1/2\\) karta \\(K_1\\)\njest nad \\(K_2\\) na odwrót). Kontynuując rozumowanie w pewnej chwili pod \\(\\) będzie\nznajdować się \\(k\\) kart: \\(K_1, \\ldots K_k\\). Każde ułożenie tychże kart jest tak samo prawdopodobne\n(indukcja!). Zatem karty pod \\(\\) są dobrze potasowane.Karta \\(\\) dotrze w końcu na sam szczyt talii właśnie ona zostanie wstawiona.\nW tej chwili wszystkie karty będą dobrze potasowane (każde ich wzajemne ułożenie jest\njednakowo prawdopodobne).Obliczmy ile czasu potrzebuje karta \\(\\), aby dotrzeć na górę talii.\nNiech \\(X_i\\) oznacza czas jaki karta \\(\\) spędza na \\(\\)-tej pozycji od dołu. Prawdopodobieństwo,\nże karta wstawiona zostanie poniżej \\(\\) wynosi \\(1/52\\). Czas oczekiwania na sukces \\(X_1\\),\nzgodnie z poprzednim przykładem jest zmienną\nlosową o rozkładzie Geom(\\(1/52\\)), zatem \\(\\mathbb{E} X_1 = 52\\).\nPodobnie dla każdego \\(\\): \\(\\mathbb{E} X_i = 52/\\). Podsumowując\n\\[\\mathbb{E} X = \\mathbb{E} \\sum_{=1}^{52 } X_i = \\sum_{=1}^{52} \\mathbb{E} X_i = \\sum_{=1}^{52} \\frac{52}\n= 52 \\sum_{=1}^{52} \\frac 1i \\sim 52 (\\log 52 + \\gamma) \\sim 235, \\]\ngdzie \\(\\gamma\\) jest stałą Eulera–Mascheroniego znaną z wykładu z analizy (\\(\\gamma \\sim 0,577\\)).","code":""},{"path":"przegląd-ważniejszych-rozkładów.html","id":"przegląd-ważniejszych-rozkładów","chapter":"9 Przegląd ważniejszych rozkładów","heading":"9 Przegląd ważniejszych rozkładów","text":"Celem tego rozdziału jest systematyzacja wiedzy o najczęściej\nspotykanych wykorzystywanych rozkładach.","code":""},{"path":"przegląd-ważniejszych-rozkładów.html","id":"rozkłady-dyskretne","chapter":"9 Przegląd ważniejszych rozkładów","heading":"Rozkłady dyskretne","text":"Zaczniemy od rozkładów skupionych na przeliczalnych zbiorach.Definicja 9.1  Zmienna losowa \\(X\\) o rozkładzie \\(\\mu_X\\) ma rozkład dyskretny\njeżeli istnieje przeliczalny (skończony) zbiór \\(S\\) taki, że \\(\\mu_X(S)=1\\).Jeżeli \\(X\\) ma rozkład dyskretny \n\\[\\begin{equation*}\n    S = \\left\\{ x\\\\mathbb{R}:\\; \\mathbb{P}[X = x]>0 \\right\\}\n\\end{equation*}\\]\njest zbiorem atomów. Zbiór \\(S\\) można ustawić w ciąg \\(S = \\{s_k\\}_{k \\\\mathbb{N}}\\). Mamy wówczas\n\\[\\begin{equation*}\n    \\sum_{k=1}^\\infty \\mathbb{P}[X=s_k]=1.\n\\end{equation*}\\]Twierdzenie 8.1 dla zmiennych o rozkładzie dyskretnym zapisuje się w postaci sumy.Wniosek 9.1  Jeżeli zmienna losowa \\(X\\) jest dyskretna ze zbiorem atomów \\(S=\\{s_k\\}_{k}\\),\n\\(\\varphi \\colon \\mathbb{R} \\\\mathbb{R}\\) jest funkcją borelowską,\n\\(\\mathbb{E} \\left[\\phi(X)\\right]\\) istnieje wtedy tylko wtedy, gdy\n\\(\\sum_i |\\phi(x_i)|\\mathbb{P}[X=x_i]<\\infty\\). Wówczas\n\\[\n  \\mathbb{E} \\left[\\phi(X)\\right] = \\sum_i \\phi(s_i)\\mathbb{P}[X=s_i].\n\\]Poniżej dokonamy przeglądu najczęściej spotykanych rozkładów dyskretnych.","code":""},{"path":"przegląd-ważniejszych-rozkładów.html","id":"rozkład-skupiony-w-punkcie","chapter":"9 Przegląd ważniejszych rozkładów","heading":"Rozkład skupiony w punkcie","text":"Rozkład skupiony w punkcie \\(\\\\mathbb{R}\\) odpowiada zmiennej losowej, która z prawdopodobieństwem jeden\njest równa \\(\\). Czyli\n\\[\\begin{equation*}\n    \\mathbb{P}[X=]=1.\n\\end{equation*}\\]\nWówczas odpowiadający rozkład jest skupiony w punkcie \\(\\), czyli \\(\\mu_X=\\delta_a\\). W tym przypadku zbiór atomów jest\njednopunktowy \\(S = \\{\\}\\).","code":""},{"path":"przegląd-ważniejszych-rozkładów.html","id":"rozkład-dwumianowy-bernoulliego","chapter":"9 Przegląd ważniejszych rozkładów","heading":"Rozkład dwumianowy (Bernoulliego)","text":"","code":""},{"path":"przegląd-ważniejszych-rozkładów.html","id":"rozkład-geometryczny","chapter":"9 Przegląd ważniejszych rozkładów","heading":"Rozkład Geometryczny","text":"Niech \\(p \\(0,1)\\). Powiemy, że zmienna losowa \\(X\\) ma rozkład geometryczny\nz parametrem \\(p\\), jeżeli\n\\[\\begin{equation*}\n    \\mathbb{P}[X=j] = (1-p)^{j-1}p, \\qquad j \\geq 1.\n\\end{equation*}\\]\nPiszemy wtedy \\(X \\sim Geo(p)\\).\nWówczas \\(X\\) modeluje liczbę prób jaką należy wykonać aby uzyskać pierwszy sukces w ciągu prób,\nw którym w pojedynczej próbie sukces zachodzi z prawdopodobieństwem \\(p\\).Należy mieć na względzie, że \\(\\mathbb{P}[X=j]>0\\) dla każdego \\(j\\). Zera pojawiające się\nna powyższym histogramie wynikają wyłącznie z zaokrąglenia.","code":""},{"path":"przegląd-ważniejszych-rozkładów.html","id":"rozkład-poissona","chapter":"9 Przegląd ważniejszych rozkładów","heading":"Rozkład Poissona","text":"Pokażemy w zadaniu, że jeżeli wykonujemy \\(n\\) prób Bernoulliego z prawdopodobieństwem sukcesu\n\\(p=\\lambda/n\\) przy \\(n \\\\infty\\) liczba sukcesów będzie miała w przybliżeniu rozkład\nPoissona z parametrem \\(\\lambda\\). Oznacza , że rozkład Poissona używany jest opisu\nliczby rzadkich zdarzeń takich jak liczba wypadków samochodowych czy\nliczba chorych na rzadką chorobę.","code":""},{"path":"przegląd-ważniejszych-rozkładów.html","id":"rozkłady-absolutnie-ciągłe","chapter":"9 Przegląd ważniejszych rozkładów","heading":"Rozkłady absolutnie ciągłe","text":"Definicja 9.2  Zmienna losowa \\(X\\) o rozkładzie \\(\\mu_X\\) ma rozkład absolutnie ciągły\n(względem miary Lebesgue’)\njeżeli istnieje funkcja borelowska \\(f_X:\\mathbb{R}\\[0,\\infty)\\) taka,\nże dla dowolnego \\(B\\\\mathcal{B}(\\mathbb{R})\\)\n\\[\n\\mathbb{P}[X\\B] = \\mu_X(B) = \\int_B f_X(s)\\mathrm{d}s.\n\\]\nFunkcję \\(f_X\\) nazywamy gęstością rozkładu \\(X\\).Gęstość jest jednoznacznie wyznaczona z dokładnością zbiorów miary Lebesgue’zero.\nZauważmy, że\n\\[\\begin{equation*}\n    \\int_\\mathbb{R} f_X(s) \\mathrm{d}s=\\mu_X(\\mathbb{R}) =1.\n\\end{equation*}\\]\nKażda funkcja mierzalna nieujemna \\(f\\) taka, że \\(\\int f(x)\\mathrm{d}x = 1\\) jest gęstością\npewnej zmiennej losowej.\nJeżeli przez \\(F_X\\) oznaczymy dystrybuantę zmiennej \\(X\\), \n\\[\nF_X(t) = \\int_{(-\\infty,t]} f_X(s) \\mathrm{d}s.\n\\]\nJeżeli \\(f_X\\) jest funkcją ciągłą, powyższa całka jest całką Riemanna wówczas \\(F'_X = f_X\\).Wniosek 9.2  Jeżeli zmienna losowa \\(X\\) ma rozkład absolutnie ciągły z gęstością \\(f_X\\), \ndla każdej borelowskiej \\(\\varphi \\colon \\mathbb{R} \\\\mathbb{R}\\) mamy\n\\[\\begin{equation*}\n    \\mathbb{E}[\\varphi(X)] = \\int_\\mathbb{R} \\varphi(s)f_X(s) \\mathrm{d}s.\n\\end{equation*}\\]","code":""},{"path":"przegląd-ważniejszych-rozkładów.html","id":"rozkład-jednostajny","chapter":"9 Przegląd ważniejszych rozkładów","heading":"Rozkład jednostajny","text":"Niech \\(D\\\\mathcal{B}(\\mathbb{R})\\) będzie dowolnym zbiorem o niezerowej mierze Lebesgue’.\nPowiemy, że zmienna losowa \\(X\\) ma rozkład jednostajny na \\(D\\) jeżeli\n\\[\n  f_X(s) = \\frac{{\\bf 1}_D(s)}{\\lambda_1(D)}.\n\\]\nWtedy dla każdego \\(B\\\\mathcal{B}(\\mathbb{R})\\)\n\\[\n  \\mathbb{P}[X\\B] = \\int_B f_X(s)\\mathrm{d}s = \\frac{\\lambda_1(B\\cap D)}{\\lambda_1(D)}.\n\\]\nDla przykładu jeżeli \\(D=[,b]\\), \nodpowiadająca gęstość zadaje się wzorem\n\\[\\begin{equation*}\n    f_X(s) = \\frac{1}{b-} \\mathbf{1}_{[,b]}(s).\n\\end{equation*}\\]","code":""},{"path":"przegląd-ważniejszych-rozkładów.html","id":"rozkład-wykładniczy","chapter":"9 Przegląd ważniejszych rozkładów","heading":"Rozkład wykładniczy","text":"Niech \\(\\lambda>0\\). Powiemy, że zmienna losowa \\(X\\) ma\nrozkład wykładniczy z parametrem \\(\\lambda\\) jeżeli\njej rozkład jest absolutnie ciągły z gęstością\n\\[\n  f_X(x) = \\lambda e^{-\\lambda x} {\\bf 1}_{[0,\\infty)}(x).\n\\]\nPiszemy wtedy \\(X \\sim \\mathrm{Exp}(\\lambda)\\).\nWówczas dystrybuanta zadaje się wzorem\n\\[\\begin{equation*}\n\\quad F(x) = \\left\\{\n  \\begin{array}{cc}\n    0, & x<0  \\\\\n    1-e^{-\\lambda x}, & x\\ge 0.\n  \\end{array}\\right.\n\\end{equation*}\\]Rozkład wykładniczy określa moment zajścia zdarzenia losowego","code":""},{"path":"przegląd-ważniejszych-rozkładów.html","id":"rozkład-normalny","chapter":"9 Przegląd ważniejszych rozkładów","heading":"Rozkład normalny","text":"Powiemy, że zmienna losowa \\(X\\) ma standardowy rozkład normalny, jeżeli\njest rozkład jest absolutnie ciągły z gęstością\n\\[\\begin{equation*}\n  f(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2}.\n\\end{equation*}\\]\nPiszemy wtedy \\(X \\sim \\mathcal{N}(0,1)\\). Funkcja gęstości nie jest elementarna,\nwięc dystrybuanta \\(F_X\\) nie ma zwartej postaci.\n\\[\\begin{equation*}\n    F(x) = \\frac 1{\\sqrt {2\\pi}} \\int_{-\\infty}^x e^{-t^2/2}\\mathrm{d}t.\n\\end{equation*}\\]\nNiech \\(\\mu \\\\mathbb{R}\\) \\(\\sigma^2>0\\). Powiemy, że zmienna losowa \\(X\\) ma rozkład normalny\nz parametrami \\(\\mu\\) oraz \\(\\sigma^2\\), jeżeli ma gęstość\n\\[\\begin{equation*}\n  f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-(x-\\mu)^2/(2\\sigma^2)}.\n\\end{equation*}\\]\nPiszemy wtedy \\(\\mathcal{N}(\\mu, \\sigma^2)\\).","code":""},{"path":"przegląd-ważniejszych-rozkładów.html","id":"pozostałe-rozkłady","chapter":"9 Przegląd ważniejszych rozkładów","heading":"Pozostałe rozkłady","text":"Rozkłady dyskretne ciągłe nie wyczerpują wszystkich możliwości. Wiemy, że każdą miarę \\(\\mu\\) można jednoznacznie zapisać w postaci\n\\[\n\\mu =  \\mu_{{\\rm abs}} + \\mu_{{\\rm sing}},\n\\] gdzie \\(\\mu_{{\\rm abs}} \\ll{\\rm Leb}\\) (tzn. \\(\\mu_{{\\rm abs}}\\) jest absolutnie ciągła względem \\({\\rm Leb}\\),\n\\(\\mu_{{\\rm sing}} \\perp {\\rm Leb}\\) są wzajemnie singularne (rozkład Lebesgue’).","code":""},{"path":"wektory-losowe.html","id":"wektory-losowe","chapter":"10 Wektory losowe","heading":"10 Wektory losowe","text":"Definicja 10.1  Niech \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) będzie przestrzenią probabilistyczną.\nNiech \\(d \\\\mathbb{N}\\). \\(d\\)-wymiarowym\nWektorem losowym\nnazywamy dowolną funkcję mierzalną \\(\\vec{X}: (\\Omega, \\mathcal{F}) \\(\\mathbb{R}^d, \\mathcal{B}(\\mathbb{R}^d)\\).Jeżeli \\(d=1\\), powyższa definicja opisuje zmienną losową.\nJeżeli \\(\\vec{X}\\colon \\Omega \\\\mathbb{R}^d\\), \ndla \\(\\omega \\\\Omega\\),\n\\[\\begin{equation*}\n    \\vec{X}(\\omega) = (X_1(\\omega), X_2(\\omega), \\ldots , X_d(\\omega)).\n\\end{equation*}\\]\nWówczas \\(\\vec{X}\\) jest wektorem losowym wtedy tylko wtedy, gdy \\(X_k\\) jest zmienną losową dla\nkażdego \\(k\\leq d\\). Istotnie, wystarczy zauważyć, że dla dowolnych\n\\(A_1,A_2, \\ldots ,A_d \\\\mathcal{B}(\\mathbb{R})\\),\n\\[\\begin{equation*}\n    \\vec{X}^{-1}[A_1\\times A_2\\times \\cdots \\times A_d] =\n    X_1^{-1}(A_1)\\cap X_2^{-1}(A_2)\\cap \\ldots \\cap X_d^{-1}[A_d] \\\\mathcal{F}\n\\end{equation*}\\]\njako przekrój skończonej liczby zdarzeń.Przykład 10.1  Wylosowano 13 kart z 52.\nNiech \\(X_1\\) oznacza liczbę pików, \\(X_2\\) liczbę kierów.\nWówczas \\(\\vec{X}=(X_1,X_2)\\) jest 2-wymiarowym wektorem losowym.Przykład 10.2  Losujemy punkt z kwadratu \\([0,1]^2\\). Dla \\(\\omega \\[0,1]^2\\) niech \\(X_1\\) będzie odległością\nwylosowanego punktu od lewej krawędzi kwadratu. Wtedy \\(X_1(\\omega) = \\omega_1\\) dla \\(\\omega=(\\omega_1, \\omega_2)\\).\nNiech \\(X_2\\) będzie odległością od punktu \\((0,0)\\). Wówczas \\(X_2(\\omega) = \\sqrt{\\omega_1^2+\\omega_2^2}\\).\nWówczas \\(\\vec{X}=(X_1, X_2)\\) jest wektorem losowym.Podstawowe własności wektorów losowych są analogiczne zmiennych losowych.jeżeli \\(\\vec{X}, \\vec{Y}\\) są \\(d\\)-wymiarowymi wektorami losowymi, \\(\\vec{X}+\\vec{Y}\\),\n\\(\\vec{X}-\\vec{Y}\\) również;jeżeli \\(\\phi:\\mathbb{R}^d\\\\mathbb{R}^k\\) jest mierzalne, \\(\\phi(\\vec{X})\\) jest \\(k\\)-wymiarowym wektorem losowym.Definicja 10.2  Rozkładem \\(d\\)-wymiarowego wektora losowego \\(\\vec{X}=(X_1, \\ldots , X_d)\\)\nnazywamy miarę probabilistyczną\n\\(\\mu_{\\vec{X}}\\) na \\(\\mathbb{R}^d\\)\nzadaną wzorem\n\\[\n    \\mu_{\\vec{X}}(B) =\n    \\mathbb{P}\\left[\\vec{X}\\B\\right] =\n    \\mathbb{P}[(X_1, \\ldots, X_d) \\B] = \\mathbb{P}\\left[\\vec{X}^{-1}(B)\\right].\n  \\]\nRozkład \\(\\mu_X\\) nazywamy rozkładem łącznym zmiennych losowych \\(X_1\\), \\(X_2, \\ldots , X_d\\).Niech \\(\\mu\\) będzie dowolną miarą probabilistyczną na \\(\\mathbb{R}^d\\).\nWówczas \\((\\Omega, \\mathcal{F}, \\mathbb{P})\n  = (\\mathbb{R}^d, \\mathcal{B}(\\mathbb{R}^d),\\mu_X)\\) jest przestrzenią probabilistyczną.\nJeżeli zdefiniujemy \\(\\vec{X} \\colon \\Omega \\\\mathbb{R}^d\\) poprzez \\(\\vec{X}(\\omega) = \\omega\\),\nrozkład wektora \\(\\vec{X}\\) \n\\[\\begin{equation*}\n    \\mu_{\\vec{X}}() = \\mathbb{P}\\left[ \\vec{X}^{-1}() \\right] = \\mathbb{P}[] = \\mu().\n  \\end{equation*}\\]\nOznacza , że dowolna miara probabilistyczna \\(\\mu\\)\nna \\(\\mathbb{R}^d\\) jest rozkładem pewnego wektora losowego \\(\\vec{X}\\).Definicja 10.3  \\(d\\)-wymiarowy wektor losowy \\(X\\) ma rozkład dyskretny, jeżeli istnieje przeliczalny zbiór \\(S\\subseteq \\mathbb{R}^d\\)\ntaki, że \\(\\mu_X(S)=1\\).W takim przypadku zbiór atomów\n\\[\n    S = \\left\\{s \\\\mathbb{R}^d \\: : \\: \\mathbb{P}[X=s] >0 \\right\\}\n\\]\nmożna ustawić w ciąg \\(S = \\{s_j\\}_{j \\\\mathbb{N}}\\). Wówczas\n\\[\\begin{equation*}\n    \\sum_{j=1}^\\infty\\mathbb{P}[S=s_j]=1.\n\\end{equation*}\\]\nWektor losowy \\(X=(X_1, \\ldots, X_d)\\) ma rozkład dyskretny wtedy tylko wtedy, dla każdego \\(j\\leq d\\)\nzmienna losowa \\(X_j\\) ma rozkład dyskretny.Przykład 10.3  W urnie są 2 kule czerwone, 5 białych 3 zielone. Wybieramy losowo 3 kule (jednocześnie). Niech \\(X_1\\) oznacza liczbę kul białych, \\(X_2\\) liczbę kul czerwonych. Wówczas \\((X_1,X_2)\\) jest 2-wymiarową zmienną losową ma rozkład dyskretny:\n\\[\n  \\begin{array}{c|c|c|c|c}\n    X_1\\setminus X_2 & 0 & 1 & 2 & \\mathbb{P}[X_1 = x] \\\\[2mm] \\hline\n    0 & \\frac{1}{120} & \\frac{6}{120} & \\frac{3}{120} & \\frac{10}{120} \\\\[2mm] \\hline\n    1 & \\frac{15}{120} & \\frac{30}{120} & \\frac{5}{120} & \\frac{50}{120} \\\\[2mm] \\hline\n    2 & \\frac{30}{120} & \\frac{20}{120} & 0 & \\frac{50}{120} \\\\[2mm] \\hline\n    3 & \\frac{10}{120} & 0 & 0 & \\frac{10}{120} \\\\[2mm] \\hline\n    \\mathbb{P}[X_2 = y] & \\frac{56}{120} & \\frac{56}{120} & \\frac{8}{120} & 1\n  \\end{array}\n  \\]\nMając dany powyższy rozkład możemy obliczyć:\n\\[\\begin{align*}\n    \\mathbb{P}[X_1\\le X_2] & =\\frac{45}{120} = \\frac 38 \\\\\n    \\mathbb{P}[X_1=1|X_1 \\le X_2] & =\n    \\frac{\\mathbb{P}[X_1=1 \\mbox{ } X_1\\le X_2]}{\\mathbb{P}[X_1 \\le X_2]} = \\frac{35/120}{45/120} = \\frac{7}{9}.\n  \\end{align*}\\]Aby badać rozkłady wektorów losowych, które nie są dyskretne musimy wprowadzić dodatkowe pojęcie.Definicja 10.4  Dystrybuanta \\(d\\)-wymiarowego wektora losowego \\(X=(X_1,\\ldots,X_d)\\) jest funkcja\n\\(F:\\mathbb{R}^d\\mapsto [0,1]\\) zadana wzorem\n\\[\\begin{multline*}\n  F(t_1,t_2,\\ldots, t_d) = \\mu\\big(\n  (-\\infty,t_1]\\times (-\\infty,t_2]\\times \\ldots \\times (-\\infty,t_d]\n  \\big)\\\\\n  = \\mathbb{P}\\big[ X_1\\le t_1, X_2\\le t_2,\\ldots, X_d\\le t_d \\big].\n  \\end{multline*}\\]Twierdzenie 10.1  Niech \\(F\\) będzie dystrybuantą \\(d\\)-wymiarowej zmiennej losowej \\(\\vec{X}\\). Wówczasjeżeli \\(x_i\\-\\infty\\) dla pewnego \\(\\), \\(F(x_1,\\ldots, x_d)\\0\\);jeżeli \\(x_i\\+\\infty\\) dla każdego \\(\\), \\(F(x_1,\\ldots, x_d)\\1\\);dystrybuanta zmiennej losowej \\(X\\) jednoznacznie wyznacza jej rozkład.Proof. Pierwsze dwa punkty wynikają z ciągłości miary. Trzeci jest zastosowaniem Lematu o \\(\\pi\\) \\(\\lambda\\) układach.\nUstalmy \\(\\leq d\\). Zauważmy, że\n\\[\\begin{equation*}\n    \\bigcap_{n\\\\mathbb{N}}\n    \\mathbb{R}^{-1} \\times(-\\infty, -n] \\times \\mathbb{R}^{d-} =\\emptyset.\n\\end{equation*}\\]\nZ ciągłości miary\n\\[\\begin{equation*}\n    \\lim_{n \\\\infty} \\mu_{\\vec{X}} \\left[\\mathbb{R}^{-1}\\times (-\\infty, -n] \\mathbb{R}^{d-} \\right] =0.\n\\end{equation*}\\]\nNiech teraz \\(\\{x_i(m)\\}_{m\\\\mathbb{N}}\\) dla \\(\\leq d\\) będzie dowolną kolekcją ciągów.\nJeżeli \\(x_i(m)\\-\\infty\\),\ndla dostatecznie dużych \\(m\\), \\(x_i(m)\\leq -n\\). Wówczas\n\\[\\begin{equation*}\n    F_{\\vec{X}}(x_i, \\ldots, x_d) \\leq\n    \\mu_{\\vec{X}} \\left[\\mathbb{R}^{-1}\\times (-\\infty, -n] \\times\\mathbb{R}^{d-} \\right]\n\\end{equation*}\\]\ngdzie prawa strona, poprzez dobór \\(n\\) może być uczyniona dowolnie małą.\nAby uzasadnić punkt drugi stosujemy podobny argument w oparciu o\n\\[\\begin{equation*}\n    \\bigcap_{n \\\\mathbb{N}} (-\\infty,n]^d =\\mathbb{R}^d.\n\\end{equation*}\\]\nAby uzasadnić ostatni punkt zauważmy, że\n\\[\\begin{equation*}\n\\mathcal{K} = \\{ (-\\infty, t_1] \\times \\cdots (-\\infty, t_d] \\: : \\: t_1, \\ldots t_d \\\\mathbb{R} \\}\n\\end{equation*}\\]\njest \\(\\pi\\)-układem. Jeżeli dwa wektory losowe \\(\\vec{X}\\) oraz \\(\\vec{Y}\\) mają równe dystrybuanty,\n\n\\[\\begin{equation*}\n    \\mathcal{K} \\subseteq \\mathcal{L} = \\{B \\\\mathcal{B}(\\mathbb{R}^d)\n    \\: \\: \\mu_{\\vec{X}}() = \\mu_{\\vec{Y}}() \\}.\n\\end{equation*}\\]\nSprawdzaliśmy już, że zbiory \\(\\mathcal{L}\\) postaci jak wyżej są \\(\\lambda\\)-układami.\nZ lematu o \\(\\pi\\)-\\(\\lambda\\) układach\n\\(\\mu_{\\vec{X}}() = \\mu_{\\vec{Y}}()\\) dla każdego borelowskiego \\(\\).Okazuje się, że podobnie jak w przypadku jednowymiarowym rozkład wektora losowego\nmożna wyrazić w terminach jego dystrybuanty.\nPrzykładowo, dla \\(d=2\\) mamy\n\\[\\begin{equation*}\n\\mu_X((,b]\\times(c,d]) = F_X(b,d) -F(b,c)-F(,d)+F(,c).\n\\end{equation*}\\]\nW szczególności oznacza , że dla dwuwymiarowej dystrybuanty prawa strona jest nieujemna\ndla każdych\n\\(< b\\) oraz \\(c < d\\).Przykład 10.4  Z powyższego łatwo liczymy, że\n\\[\\begin{equation*}\n    F_{\\vec{X}}(t_1, t_2) = \\left\\{\\begin{array}{cc}\n    4t_1t_2 & \\sqrt{2}(t_1+t_2)\\leq 1 \\\\\n    1-(1-\\sqrt{2}t_1)_+^2 -(1-\\sqrt{2}t_2)_+^2 & \\sqrt{2}(t_1+t_2)>1\n\\end{array} \\right.\n\\end{equation*}\\]\nWykres naszej dwuwymiarowej dystrybuanty przedstawia się następującoDefinicja 10.5  \\(d\\)-wymiarowy wektor \\(\\vec{X}\\) ma rozkład absolutnie ciągły, jeżeli istnieje funkcja borelowska\n\\(f_\\vec{X}:\\mathbb{R}^d \\mapsto [0,\\infty)\\)\ntaka, że\n\\[\n    \\mathbb{P}\\left[\\vec{X}\\B\\right] = \\mu(B) = \\int_B f_\\vec{X}(s)\\mathrm{d}s\n    \\qquad B\\\\mathcal{B}(\\mathbb{R}^d).\n  \\]Wówczas\n\\[\n  F_\\vec{X}(t_1,\\ldots,t_d) = \\int_{-\\infty}^{t_1}\\ldots \\int_{-\\infty}^{t_d} f_\\vec{X}(x_1,\\ldots, x_d)dx_1\\ldots dx_d.\n  \\]\nPonadto, jeżeli \\(F\\C^d(\\mathbb{R}^d)\\), \n\\[\n  f(x_1,\\ldots,x_d) = \\frac{\\partial^d F}{\\partial x_1\\ldots \\partial x_d}(x_1,\\ldots,x_d).\n  \\]Zauważmy, że jeżeli \\(\\vec{X}\\) ma rozkład absolutnie ciągły, \nkażdy \\(X_k\\) dla \\(k \\leq d\\) ma rozkład absolutnie ciągły. Implikacja przeciwna nie jest\njednak prawdziwa.Przykład 10.5  Niech \\(\\Omega = [0,1]\\) z \\(\\sigma\\)-ciałem zbiorów borelowskich jednowymiarową miara Lebesgue’.\nRozważmy \\(X_1(\\omega)=\\omega\\) oraz \\(X_2(\\omega) = 1-\\omega\\).\nWówczas \\(\\vec{X} = (X_1, X_2)\\) ma rozkład skupiony na nieprzeliczalnym zbiorze\no płaskiej mierze Lebesgue’zero\n\\[\\begin{equation*}\n\\{(x,1-x) \\: : \\: x \\[0,1]\\} \\subseteq \\mathbb{R}^2.\n\\end{equation*}\\]Przykład 10.6  Niech \\(S\\) będzie dowolnym mierzalnym ograniczonym podzbiorem \\(\\mathbb{R}^2\\),\nwtedy jego losowy punkt \\(\\vec{X}=(X_1,X_2)\\) (wybrany jednostajnie\nwzględem miary Lebesgue’) jest 2-wymiarową zmienną losową. Mamy wówczas\n\\[\n  f_{\\vec{X}}(x_1,x_2) = \\frac{1}{|S|}\\; {\\bf 1}_S\n  \\]","code":""},{"path":"wektory-losowe.html","id":"wielowymiarowy-rozkład-normalny","chapter":"10 Wektory losowe","heading":"Wielowymiarowy rozkład normalny","text":"Przykład 10.7  Powiemy, że \\(d\\)-wymiarowy wektor \\(\\vec{X}\\) ma \\(d\\)-wymiarowy standardowy rozkład normalny, jeżeli\njego rozkład jest absolutnie ciągły z gęstością\n\\[\\begin{equation*}\n    f_{\\vec{X}}(s_1, \\ldots, s_d) =\n(2\\pi)^{-d/2} e^{-\\|s\\|^2/2}.   \n(2\\pi)^{-d/2} e^{-(s_1^2+s_2^2+\\ldots +s_d^2)/2},\n\\end{equation*}\\]\ngdzie \\(s = (s_1, s_2, \\ldots, s_d)\\).Sprawdźmy, że podana wyżej funkcja rzeczywiście jest gęstością na \\(\\mathbb{R}^d\\).\nBędziemy stosować oznaczenie \\(s = (s_1, \\ldots, s_d)\\). Mamy\n\\[\\begin{multline*}\n    \\int_{\\mathbb{R}^d} f_\\vec{X}(s) \\mathrm{d}s =\n    \\int_\\mathbb{R} \\ldots \\int_\\mathbb{R} (2\\pi)^{-d/2} e^{-s_1^2/2} \\cdots\n    e^{-s_d/2} \\mathrm{d}s_1 \\ldots \\mathrm{d}s_d \\\\\n    = \\left( \\sqrt{2\\pi} \\int_\\mathbb{R} e^{-x^2/2} \\mathrm{d}x \\right)^d=1.\n\\end{multline*}\\]\nJeżeli wygenerujemy \\(100\\) punktów ma płaszczyźnie zgodnie ze standardowym rozkładem normalnym,\nbędą się one układały symetrycznie wokół zera.Niech \\(\\vec{m}\\) będzie wektorem \\(d\\)-wymiarowym \\(\\) odwracalną macierzą wymiaru \\(d\\times d\\).\nRozważmy zmienną losową\n\\[\\begin{equation*}\n    \\vec{Y} = \\vec{X} +m.\n\\end{equation*}\\]\nJaki rozkład ma wektor \\(\\vec{Y}\\)?\nZauważmy, że dla \\(B \\\\mathcal{B}(\\mathbb{R}^d)\\),\n\\[\\begin{equation*}\n    \\mu_{\\vec{Y}} (B)=\\mathbb{P}\\left[\\vec{Y}\\B\\right] =\n    \\mathbb{P}\\left[ \\vec{X} \\^{-1}B - ^{-1}\\vec{m} \\right],\n\\end{equation*}\\]\ngdzie\n\\[\\begin{equation*}\n    ^{-1}B-^{-1}\\vec{m} = \\left\\{ ^{-1}\\vec{b} -^{-1}\\vec{m} \\: : \\: \\vec{b} \\B \\right\\}.    \n\\end{equation*}\\]\nCałkując przez podstawienie \\(s = ^{-1}y -^{-1}\\vec{m}\\),\n\\[\\begin{equation*}\n    \\mu_{\\vec{Y}}(B) = \\int_{^{-1}B-^{-1}\\vec{m}} f_\\vec{X}(s) \\mathrm{d}s =\n    \\int_B |\\mathrm{det}(^{-1})| f_\\vec{X}\\left(^{-1}(y-\\vec{m})\\right) \\mathrm{d}y.\n\\end{equation*}\\]\nSkoro gęstość jest wyznaczona jednoznacznie, wektor losowy \\(\\vec{Y}\\) ma rozkład o gęstości\n\\[\\begin{multline*}\nf_\\vec{Y}(y) =\n|\\mathrm{det}(^{-1})| f_\\vec{X}\\left(^{-1}(y-\\vec{m})\\right)=\\\\\n\\frac{|\\mathrm{det}(^{-1})|}{(2\\pi)^{d/2}}\\exp \\left\\{ -\\left\\| ^{-1}(y-\\vec{m}) \\right\\|^2/2  \n\\right\\}\n\\end{multline*}\\]\nZauważmy, że\n\\[\\begin{multline*}\n    \\left\\| ^{-1}(y-\\vec{m}) \\right\\|^2=\n    \\langle ^{-1}(y-\\vec{m}), ^{-1}(y-\\vec{m})  \\rangle=\\\\\n    \\langle (^{-1})^TA^{-1}(y-\\vec{m}), y-\\vec{m}  \\rangle.\n\\end{multline*}\\]\nOznaczmy \\(\\Sigma = AA^T\\). Wówczas macierz \\(\\Sigma\\) jest symetryczna nieujemnie określona.\nGęstość wektora losowego \\(\\vec{Y}\\) zapisuje się jako\n\\[\\begin{equation*}\n    f_\\vec{Y}(y) =\n    \\mathrm{det}(\\Sigma)^{-1/2}(2\\pi)^{-d/2}\n    \\exp \\left\\{ -\\langle \\Sigma^{-1}(y-\\vec{m}), y-\\vec{m} \\rangle /2  \\right\\}.\n\\end{equation*}\\]\nJeżeli \\(\\Sigma\\) jest symetryczną macierzą dodatnio określoną, \\(\\vec{m} \\\\mathbb{R}^d\\) wektor\nlosowy \\(\\vec{Y}\\) ma rozkład o gęstości jak wyżej, będziemy mówić, że\n\\(\\vec{Y}\\) ma wielowymiarowy rozkład normalny o parametrach \\(\\vec{m}\\) \\(\\Sigma\\). Piszemy wówczas\n\\(\\vec{Y} \\sim \\mathcal{N}(\\vec{m}, \\Sigma)\\). Niebawem dowiemy się, jak dobór macierzy \\(\\Sigma\\) wpływa\nna kształt gęstości \\(f_{\\vec{Y}}\\) oraz na własności probabilistyczne wektora \\(\\vec{Y}\\).\nZauważmy jedynie teraz, że \\(\\vec{Y}\\) powstał z \\(\\vec{X}\\) poprzez przekształcenie liniowe\nprzesunięcie. samo będzie tyczyło się związku między \\(f_\\vec{X}\\) oraz \\(f_\\vec{Y}\\).Przykład 10.8  Rozważmy poprzedni przykład dla macierzy \\(\\) zadanej przez\n\\[\\begin{equation*}\n    = \\left( \\begin{array}{cc} 1 & 1 \\\\ 0 & 1 \\end{array}\\right).\n\\end{equation*}\\]\nWówczas\n\\[\\begin{equation*}\n    \\Sigma = \\left( \\begin{array}{cc} 2 & 1 \\\\ 1 & 1 \\end{array}\\right), \\qquad\n    \\Sigma^{-1} = \\left( \\begin{array}{cc} 1 & -1 \\\\ -1 & 2 \\end{array}\\right).\n\\end{equation*}\\]\nGęstość wektora \\(\\vec{Y}\\) o dwuwymiarowym rozkładzie normalnym\nz parametrami \\(\\vec{m} =(0,0)\\) \\(\\Sigma\\) wynosi\n\\[\\begin{equation*}\n    f_{\\vec{Y}}(x,y) = \\frac{1}{2 \\pi \\sqrt{3}} \\exp \\left\\{ -(x^2-2xy+2y^2)/2 \\right\\}.\n\\end{equation*}\\]","code":""},{"path":"wektory-losowe.html","id":"rozkłady-brzegowe","chapter":"10 Wektory losowe","heading":"Rozkłady brzegowe","text":"Definicja 10.6  Niech \\(X=(X_1,\\ldots, X_d)\\) będzie \\(d\\)-wymiarową zmienną losową.\nWówczas dla każdego \\(k\\le d\\), rozkład \\(\\mu_{X_k}\\) nazywamy rozkładem brzegowym \\(\\mu_X\\).Rozkłady brzegowe nie wyznaczają jednoznacznie rozkładu łącznego (poza pewnymi szczególnymi sytuacjami). znaczy, może się\nzdarzyć, że jeżeli dane są cztery zmienne losowe \\(X_1,X_2,Y_1, Y_2\\), takie, że \\(X_1\\) \\(Y_1\\) mają takie rozkłady oraz \\(X_2\\) \\(Y_2\\) mają takie rozkłady, ale \\((X_1,X_2)\\) \\((Y_1,Y_2)\\) mają różne rozkłady.Przykład 10.9  Losujemy punkt z odcinka \\([0,1]\\). Dla wylosowanej liczby \\(\\omega \\\\Omega = [0,1]\\) niech\n\\(X_1(\\omega) = \\omega\\) \\(X_2(\\omega)=1-\\omega\\). Wówczas wektor losowy \\(\\vec{X} = (X_1, X_2)\\)\nprzyjmuje wartości w zbiorze\n\\[\\begin{equation*}\n    L = \\{(x,1-x) \\: : \\: x\\[0,1]\\}.\n\\end{equation*}\\]\nZauważmy, że \\(\\lambda_2(L)=0\\). Rozkład wektora \\(\\vec{X}\\) nie jest zatem ani dyskretny ani ciągły.\nJakie są rozkłady brzegowe? Dla \\(X_1\\) mamy\n\\[\\begin{equation*}\n    \\mu_{X_1}(B)=\\mathbb{P}[X_1\\B] = \\lambda_1(B),\n\\end{equation*}\\]\ndla \\(B \\\\mathcal{B}([0,1])\\).\nJest zatem rozkład jednostajny na \\([0,1]\\). Dla \\(X_2\\) mamy\n\\[\\begin{equation*}\n\\mu_{X_2}(B)=\\mathbb{P}[X_1\\B] = \\lambda_1( \\{ \\omega \\[0,1] \\: : \\: 1-\\omega \\B \\} ).\n\\end{equation*}\\]\nSkoro symetria względem \\(x=1/2\\) jest izometrią\n\\[\\begin{equation*}\n\\lambda_1( \\{ \\omega \\[0,1] \\: : \\: 1-\\omega \\B \\} ) = \\lambda_1(B).\n\\end{equation*}\\]\nJest również rozkład jednostajny na \\([0,1]\\).Przykład 10.10  Losujemy punkt z kwadratu jednostkowego \\([0,1]^2\\). Dla \\(\\omega = (\\omega_1, \\omega_2) \\\\Omega = [0,1]^2\\)\nniech \\(X_1(\\omega) = \\omega_1\\), \\(X_2(\\omega) = \\omega_2\\). Wówczas rozkład wektora losowego\n\\(\\vec{X} = (X_1, X_2)\\) płaska miara Lebesgue’\\(\\lambda_2\\) ograniczona \\([0,1]^2\\).\nZauważmy, że z wagi na symetrię rozkłady brzegowe są takie .\nDla \\(B \\\\mathcal{B}([0,1])\\) mamy\n\\[\\begin{equation*}\n    \\mu_{X_2}(B) = \\mu_{X_1}(B) = \\mathbb{P}[X_1\\B] = \\lambda_2(B \\times [0,1])\n    = \\lambda_1(B).\n\\end{equation*}\\]\nJest kolejny raz rozkład jednostajny na \\([0,1]\\).Zauważmy, że w dwóch ostatnich przykładach różne rozkłady łączne miały takie rozkłady brzegowe.\nWynika z tego, że rozkłady brzegowe nie determinują rozkładu łącznego.","code":""},{"path":"rozkłady-warunkowe.html","id":"rozkłady-warunkowe","chapter":"11 Rozkłady warunkowe","heading":"11 Rozkłady warunkowe","text":"Zobaczymy teraz jak koncepcja prawdopodobieństwa warunkowego współgra z koncepcjami\nwartości oczekiwanej rozkładu zmiennej losowej.\nNiech \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) będzie przestrzenią probabilistyczną.\nRozważać będziemy zdarzenie \\(\\) o dodatnim prawdopodobieństwie.\nPrzypomnijmy, że definiujemy prawdopodobieństwo warunkowe pod warunkiem zajścia zdarzenia \\(\\) poprzez\n\\[\\begin{equation*}\n    \\mathbb{P}[B | ] = \\frac{\\mathbb{P}[\\cap B]}{\\mathbb{P}[]}, \\qquad B \\\\mathcal{F}.    \n\\end{equation*}\\]Definicja 11.1  Niech \\(\\vec{X}\\) będzie \\(d\\)-wymiarowym wektorem losowym.\nRozkładem warunkowym \\(\\vec{X}\\) pod warunkiem zajścia zdarzenia \\(\\)\nnazywamy miarę probabilistyczną \\(\\mu_{\\vec{X}|}\\) na \\(\\mathbb{R}^d\\)\nzadaną przez\n\\[\\begin{equation*}\n    \\mu_{\\vec{X}|}(B) = \\mathbb{P} \\left[\\left. \\vec{X} \\B \\right|  \\right],\n    \\qquad B \\\\mathcal{B}\\left(\\mathbb{R}^d\\right).\n\\end{equation*}\\]Zauważmy, że powyższa konstrukcja nie różni się znacząco od rozkładu \\(\\mu_{\\vec{X}}\\)\nwektora losowego \\(\\vec{X}\\). Można wręcz powiedzieć, że jest taka sama. Istotnie,\nzauważmy, że funkcja zbioru \\(\\mathbb{P}[\\cdot |]\\) jest miarą probabilistyczną na\n\\((\\Omega, \\mathcal{F})\\). Mamy w takim przypadku dwie miary probabilistyczne na \\(\\Omega\\).\nSą nimi wyjściowe prawdopodobieństwo \\(\\mathbb{P}[\\cdot]\\) oraz prawdopodobieństwo warunkowe\n\\(\\mathbb{P}[\\cdot|]\\). Wektor losowy \\(\\vec{X} \\colon \\Omega \\\\mathbb{R}^d\\) ma więc dwa\nopisy probabilistyczne (rozkłady) w zależności od tego która z przestrzeni probabilistycznych\n\\[\\begin{equation*}\n    (\\Omega, \\mathcal{F}, \\mathbb{P}), \\qquad (\\Omega, \\mathcal{F}, \\mathbb{P}[\\cdot | ])\n\\end{equation*}\\]\njest aktualnie rozważana. Oznacza , że wszystkie związki między\n\\(\\mathbb{P}\\) \\(\\mu_{\\vec{X}}\\) zachodzą też między \\(\\mathbb{P}[\\cdot|]\\) \\(\\mu_{\\vec{X}|}\\).\nJednym z najczęściej spotykanych przykładów jest związek między wartością oczekiwaną\ncałką względem rozkładu.Definicja 11.2  Niech \\(X\\) będzie zmienną losową posiadająca wartość oczekiwaną.\nWarunkową wartością oczekiwaną pod warunkiem zajścia zdarzenia \\(\\)\nnazywamy\n\\[\\begin{equation*}\n    \\mathbb{E}[X | ] = \\int_\\Omega X(\\omega) \\mathbb{P}[\\mathrm{d}\\omega |].\n\\end{equation*}\\]Według powyższej definicji \\(\\mathbb{E}[X|]\\) jest po prostu całką z funkcji \\(X\\) względem\nmiary \\(\\mathbb{P}[\\cdot |]\\). Oznacza , że dla dowolnej\nfunkcji borelowskiej \\(\\varphi \\colon \\mathbb{R} \\\\mathbb{R}\\) zachodzi\n\\[\\begin{equation*}\n    \\mathbb{E} [\\varphi(X) | ] = \\int_\\mathbb{R} \\varphi(s) \\mu_{X|}(\\mathrm{d}s).\n\\end{equation*}\\]\nPowyższa dyskusja pokazuje, że pod katem teoretycznym rozkłady warunkowe mają\ntaką samą naturę jak zwykłe rozkłady poznane wcześniej.\nW praktyce jednak operacja warunkowania istotnie zmienia własności probabilistyczne zmiennych losowych.Przykład 11.1  Rzucamy nieskończenie wiele razy symetryczną monetą.\nNiech \\(X\\) będzie liczbą rzutów potrzebnych otrzymania pierwszego orła.\nInteresuje nas rozkład \\(X\\) po warunkiem zdarzenia \\(\\), że w pierwszych \\(n\\) rzutach pojawił\nsię (co najmniej jeden) orzeł.\nZauważmy najpierw, że \\(= \\{ X \\leq n\\}\\). Chcąc wyznaczyć rozkład \\(X\\) wystarczy\nwyznaczyć prawdopodobieństwa zdarzeń \\(\\{X=k\\}\\) względem prawdopodobieństwa warunkowego.\nZauważmy, że\n\\[\\begin{equation*}\n    \\mathbb{P}[X \\leq n] = 1-\\mathbb{P}[X > n] = 1-2^{-n}.\n\\end{equation*}\\]\nDla \\(k \\leq n\\) mamy\n\\[\\begin{equation*}\n    \\mathbb{P}[X =k | ] = \\frac{2^{-k}}{1-2^{-n}}\n\\end{equation*}\\]\noraz \\(\\mathbb{P}[X =k | ] = 0\\) dla \\(k>n\\).\nDla porównania \\(\\mathbb{P}[X=k] = 2^{-k}\\). Warunkowanie zmienia tylko\nprawdopodobieństwo o stałą na pierwszych \\(n\\) liczbach naturalnych.Przykład 11.2  Ponownie rzucamy symetryczną monetą. Niech \\(X\\) będzie liczbą rzutów potrzebną otrzymania\npierwszego orła. Tym razem niech \\(\\) będzie zdarzeniem, że w pierwszych \\(n\\) rzutach pojawił się\ndokładnie jeden orzeł.\nWówczas\n\\[\\begin{equation*}\n    \\mathbb{P}[] = n 2^{-n}.\n\\end{equation*}\\]\nDla \\(k\\leq n\\),\n\\[\\begin{equation*}\n    \\mathbb{P}[X = k|] = \\frac{2^{-n}}{n2^{-n}} = \\frac 1n.\n\\end{equation*}\\]\nTym razem warunkowanie całkowicie zmieniło charakter rozkładu. Rzeczywiście, pod warunkiem \\(\\)\nzmienna \\(X\\) ma rozkład jednostajny na \\([n] = \\{1, 2, \\ldots , n\\}\\).","code":""},{"path":"niezależne-zmienne-losowe.html","id":"niezależne-zmienne-losowe","chapter":"12 Niezależne zmienne losowe","heading":"12 Niezależne zmienne losowe","text":"Niech \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) będzie przestrzenią probabilistyczną \nniech \\(X\\) będzie zmienną losową.Definicja 12.1  \\(\\sigma\\)-ciało generowane przez \\(X\\) \\(\\sigma\\)-ciało podzbiorów \\(\\Omega\\) zadane przez\n\\[\\begin{equation*}\n    \\sigma(X) = \\{ X^{-1}(B) \\: : \\: B \\\\mathcal{B}(\\mathbb{R})\\}.\n\\end{equation*}\\]Z własności przeciwobrazu łatwo pokazujemy, że \\(\\sigma(X)\\) jest rzeczywiście\n\\(\\sigma\\)-ciałem. Definicja zmiennej losowej wymaga, aby \\(X^{-1}(B) \\\\mathcal{F}\\) dla\n\\(B \\\\mathcal{B}(\\mathbb{R})\\). Oznacza , że \\(\\sigma(X) \\subseteq \\mathcal{F}\\).\nZauważmy też, że \\(\\sigma(X)\\) jest najmniejsze \\(\\sigma\\)-ciało takie, że\n\\[\\begin{equation*}\n    X \\colon (\\Omega, \\sigma(X)) \\(\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))\n\\end{equation*}\\]\njest mierzalne. \\(\\sigma(X)\\) zawiera informacje o eksperymencie losowym, które możemy\nwywnioskować wyłącznie na podstawie wartości \\(X\\).Przykład 12.1  Losujemy liczbę z odcinka \\([0,1)\\). Niech \\(X(\\omega)\\) będzie pierwszą cyfrą\npo przecinku \\(\\omega \\[0,1)\\) w zapisie dziesiętnym. Jak wygląda \\(\\sigma(X)\\)?\nZauważmy, że \\(X(\\omega) = \\lfloor 10\\omega\\rfloor\\) oraz\n\\[\\begin{equation*}\n    \\{ X = k\\} = \\left[ k/10, (k+1)/10 \\right), \\qquad k=0,1, \\ldots 9.\n\\end{equation*}\\]\nStąd\n\\[\\begin{equation*}\n    \\sigma(X) = \\sigma \\left( [k/10, (k+1)/10) \\: :\\: k=0,1, \\ldots 9 \\right).\n\\end{equation*}\\]Zauważmy, że każde zdarzenie \\(\\\\sigma(X)\\) ma następującą własność. Jeżeli dla \\(\\omega_1, \\omega_2 \\\\Omega\\), \\(X(\\omega_1) = X(\\omega_2)\\), \n\\[\\begin{equation*}\n    \\omega_1 \\\\iff \\omega_2 \\.\n\\end{equation*}\\]Definicja 12.2  Niech \\(\\{X_i\\}_{\\}\\) będzie rodziną zmiennych losowych określonych na \\(\\Omega\\).\nZmienne te są niezależne, jeżeli \\(\\sigma(X_i)\\) (\\(\\sigma\\)-ciała generowane przez \\(X_i\\))\nsą niezależne. Innymi słowy \\(\\{X_i\\}_{\\}\\) są niezależne,\ngdy dla dowolnych parami różnych \\(i_1,i_2,\\ldots, i_n\\\\) oraz dowolnych\n\\(B_1,\\ldots, B_n\\\\mathcal{B}(\\mathbb{R})\\) zachodzi\n\\[\n\\mathbb{P}\\left[ X_{i_1}\\B_1,\\ldots, X_{i_n}\\B_n \\right] = \\mathbb{P}[X_{i_1}\\B_1]\\cdots \\mathbb{P}[X_{i_n}\\B_n].\n\\]Przykład 12.2  Losujemy jednostajnie punkt z prostokąta \\([0,3]\\times[0,1]\\).\nWówczas \\(\\mathbb{P}[\\cdot] = \\lambda_2(\\cdot)/3\\).\nNiech \\(X_1(\\omega) = \\omega_1\\) \\(X_2(\\omega) = \\omega_2\\) dla\n\\(\\omega=(\\omega_1, \\omega_2) \\\\Omega = [0,3]\\times [0,1]\\).\nZbiory z \\(\\sigma(X_1)\\) są postaci\n\\[\\begin{equation*}\n    \\{X_1 \\B\\} = \\{\\omega = (\\omega_1, \\omega_2) \\: : \\: \\omega_1 \\B\\} = B \\times [0,1]\n\\end{equation*}\\]\ndla \\(B \\\\mathcal{B}([0,3])\\). Podobnie\n\\[\\begin{equation*}\n    \\{X_2 \\\\} = [0,1] \\times \n\\end{equation*}\\]\ndla \\(\\\\mathcal{B}([0,1])\\).\nCzyli\n\\[\\begin{multline*}\n\\mathbb{P}[X_1 \\B, \\: X_2 \\] = \\mathbb{P}[B \\times ] = \\lambda_2(B \\times )/3\n\\\\= \\frac{\\lambda_1(B)}{3} \\cdot \\lambda_1() = \\mathbb{P}[X_1 \\B] \\mathbb{P}[X_2\\].\n\\end{multline*}\\]\nZmienne losowe \\(X_1\\) \\(X_2\\) są zatem niezależne.Przykład 12.3  Rozważmy schemat Bernoulliego na przestrzeni \\(\\Omega = \\{0,1\\}^n\\) z \\(\\mathcal{F} = 2^\\Omega\\)\noraz miarą produktową jako probabilistyczną. Zdefiniujmy\n\\[\nX_i(\\omega) = X_i(\\omega_1,\\ldots,\\omega_n) = \\left\\{\n\\begin{array}{cc}\n     1, & \\mbox{jeżeli w $$-tej próbie był sukces} \\\\\n     0, &  \\mbox{jeżeli w $$-tej próbie była porażka}\n   \\end{array}\n   \\right.\n\\] Wówczas \\(X_1,\\ldots, X_n\\) są niezależnymi zmiennymi losowymi.\nWynika bezpośrednio z definicji miary produktowej.Przykład 12.4  Zauważmy, że jeżeli \\(b=d=\\sqrt{2}/2\\) oraz \\(,c > \\sqrt{2}/4\\), \n\\[\\begin{equation*}\n    \\{ X_1 \\[c,d]\\} \\cap \\{X_2 \\[,b]\\} = \\emptyset.\n\\end{equation*}\\]\nWobec czego\n\\[\\begin{equation*}\n    \\mathbb{P} [X_1 \\[c,d], X_2 \\[,b]] = 0.\n\\end{equation*}\\]\nZ kolei\n\\[\\begin{equation*}\n    \\mathbb{P}[X_1 \\[c,d]] \\mathbb{P}[X_2 \\[,b]]>0.\n\\end{equation*}\\]\nWobec tego zmienne \\(X_1\\) \\(X_2\\) nie są niezależne.Twierdzenie 12.1  Niech \\(X_1,\\ldots,X_n\\) będą zmiennymi losowymi niech\n\\(X = (X_1,\\ldots, X_n)\\). Następujące warunki są równoważne:\\(X_1,\\ldots,X_n\\) są niezależne;dla dowolnych \\(B_1,\\ldots,B_n\\\\mathcal{B}(\\mathbb{R})\\), zdarzenia \\(\\{X_1\\B_1\\}, \\ldots, \\{X_n\\B_n\\}\\) są niezależne;\\(\\mu_X = \\mu_{X_1}\\otimes\\ldots \\otimes \\mu_{X_n}\\);\\(F_{X}(t_1,\\ldots, t_n) = F_{X_{1}}(t_1)\\ldots F_{X_n}(t_n)\\).dla dowolnych ograniczonych funkcji borelowskich \\(f_1, \\ldots, f_n\\),\n\\[\\begin{equation*}\n   \\mathbb{E}[f_1(X_1) f_2(X_2) \\cdots f_n(X_n)] = \\mathbb{E}[f_1(X_1)] \\cdot \\mathbb{E}[f_2(X_2)]\n   \\cdots \\mathbb{E}[f_n(X_n)].\n\\end{equation*}\\]Proof. Równoważność 1 2 wynika z definicji.\nPokażemy, że 2 implikuje 4. Weźmy \\(B_i \\(-\\infty,t_i]\\). Wówczas\n\\[\\begin{multline*}\n     F_X(t_1,\\ldots, t_n) = \\mathbb{P}[X_1\\le t_1,\\ldots, X_n\\le t_n] \\\\ =\n     \\mathbb{P}[X_1\\le t_1]\\ldots \\mathbb{P}[X_n\\le t_n] = F_{X_1}(t_1)\\ldots F_{X_n}(t_n).\n\\end{multline*}\\]Załóżmy teraz warunek 4 pokażemy, że implikuje 3. Niech \\(X'\\) będzie \\(n\\)-wymiarową zmienną losową\no rozkładzie \\(\\mu_{X_1}\\otimes\\ldots \\otimes \\mu_{X_n}\\). Pokażemy, że \\(X\\) \\(X'\\) mają taką samą dystrybuantę. Wtedy\n\\[\\begin{multline*}\nF_{X'}(t_1,\\ldots, t_n)\n= \\mu_{X_1}\\otimes\\ldots \\otimes \\mu_{X_n}\\big( (-\\infty; t_1] \\times \\ldots \\times (-\\infty, t_n] \\big) \\\\\n=\\mu_{X_1}((-\\infty,t_1]) \\cdot     \\ldots \\cdot \\mu_{X_n}((-\\infty,t_n])\n= F_{X_1}(t_1)\\ldots F_{X_n}(t_n) = F_X(t_1,\\ldots, t_n).\n\\end{multline*}\\]\nZ twierdzenia o jednoznaczności \\(\\mu_{X'}=\\mu_X\\).Sprawdzamy teraz, że warunek 3 implikuje 2.\nDla dowolnych podzbiorów borelowskich \\(B_1,\\ldots, B_n\\) mamy\n\\[\\begin{multline*}\n\\mathbb{P}\\big[ X_1\\B_1,\\ldots, X_n\\B_n \\big] = \\mu_X(B_1\\times \\cdots \\times B_n) \\\\\n= \\mu_{X_1}(B_1)\\cdot\\ldots\\cdot \\mu_{X_n}(B_n) =\n\\mathbb{P}[X_1\\B_1]  \\cdot \\ldots \\cdot \\mathbb{P}[X_n\\B_n].\n\\end{multline*}\\]\nPokażemy wreszcie, że 3. pociąga też 5. Mamy\n\\[\\begin{multline*}\n      \\mathbb{E}[f_1(X_1) f_2(X_2) \\cdots f_n(X_n)] =\n      \\int_{\\mathbb{R}^n} \\prod_{j=1}^nf_j(x_j) \\mu_{\\vec{X}}(\\mathrm{d}x_1\\ldots x_n) =\\\\\n      \\prod_{j=1}^n \\int_\\mathbb{R} f_j(x_j) \\mu_{X_j}(\\mathrm{d}x_j)=\n       \\mathbb{E}[f_1(X_1)] \\cdot \\mathbb{E}[f_2(X_2)]\n      \\cdots \\mathbb{E}[f_n(X_n)].\n\\end{multline*}\\]\nNa koniec uzasadnimy, że 5. pociąga 2. Jeżeli rozważymy \\(f_j(y) = \\mathbf{1}_{B_j}(y)\\), \n\\(f_j(X_j(\\omega)) = \\mathbf{1}_{X_j \\B_j}(\\omega)\\) oraz\n\\[\\begin{equation*}\n\\mathbb{E}[f_j(X_j)] = \\mathbb{P}[X_j\\B_j]\n\\end{equation*}\\]\noraz\n\\[\\begin{equation*}\n\\mathbb{E}[f_1(X_1) f_2(X_2) \\cdots f_n(X_n)] =\n  \\mathbb{P}[X_1\\B_1, X_2\\B_2 \\ldots X_n \\B_n]\n\\end{equation*}\\]Wniosek 12.1  Zmienne losowe \\(X_1,\\ldots, X_n\\) mające rozkłady dyskretne są niezależne wtedy tylko wtedy gdy dla dowolnych\n\\(s_1\\S_{X_1},\\ldots, s_n\\S_{X_n}\\) zachodzi\n\\[\\begin{equation}\\label{eq:kwt1}\n\\mathbb{P}[X_1=s_1,\\ldots, X_n = s_n] = \\mathbb{P}[X_1=s_1]\\cdot\\ldots\\cdot \\mathbb{P}[X_n=s_n]\n\\tag{12.1}\n\\end{equation}\\]Proof. Jeżeli zmienne losowe \\(X_1,\\ldots, X_n\\) są niezależne, warunek (12.1)\njest oczywiście spełniony.\nImplikację odwrotną pokażemy jedynie dla \\(n=2\\) (dla uproszczenia dowodu).\nOznaczmy przez \\(S_{X_1}\\) oraz \\(S_{X_2}\\)\nnośniki rozkładów zmiennych losowych \\(X_1\\) oraz \\(X_2\\) (czyli zbiorem ich wartości).\nKorzystając z warunku\n(12.1) otrzymujemy dla dowolnych zbiorów borelowskich \\(B_1,B_2\\):\n\\[\\begin{multline*}\n  \\mathbb{P}[X_1\\B_1, X_2\\B_2] = \\mathbb{P}[X_1 \\B_1\\cap S_{X_{1}}, X_2 \\B_2 \\cap S_{X_2}]\\\\\n= \\sum_{\\substack{ x_1 \\B_1 \\cap S_{X_1}\\\\  x_2 \\B_2 \\cap S_{X_2}}}\n\\mathbb{P}[X_1 = x_1, X_2 = x_2]\n= \\sum_{x_1 \\B_1 \\cap S_{X_1}}\\sum_{x_2 \\B_2 \\cap S_{X_2}} \\mathbb{P}[X_1 = x_1]\\mathbb{P}[X_2 = x_2]\\\\\n= \\mathbb{P}[X_1 \\B_1]\\mathbb{P}[X_2\\B_2]\n\\end{multline*}\\]Wniosek 12.2  Jeżeli zmienne losowe \\(X\\) \\(Y\\) są niezależne mają wartości oczekiwane, \n\\[\\begin{equation*}\n\\mathbb{E}[XY] = \\mathbb{E}[X]\\mathbb{E}[Y].\n\\end{equation*}\\]Proof. Niech \\(\\vec{X}=(X,Y)\\).\nRozważmy rozkład łączny \\(\\mu_{\\vec{X}}\\) zmiennych \\(X\\) \\(Y\\).\nWówczas \\(\\mu_{\\vec{X}} = \\mu_{X}\\otimes \\mu_Y\\). Mamy zatem\n\\[\\begin{multline*}\n    \\mathbb{E}[XY] = \\int_{\\mathbb{R}^2} xy \\: \\mu_{\\vec{X}}(\\mathrm{d}xy) =\n    \\int_{\\mathbb{R}}\\int_\\mathbb{R} xy \\: \\mu_{X} (\\mathrm{d}x) \\mu_{Y}(\\mathrm{d}y ) = \\\\\n    \\int_{\\mathbb{R}} x \\: \\mu_{X} (\\mathrm{d}x)\n    \\int_{\\mathbb{R}} y \\: \\mu_{Y}(\\mathrm{d}y) = \\mathbb{E}[XY].\n\\end{multline*}\\]Wniosek 12.3  Zmienne losowe \\(X_1,\\ldots, X_n\\) o gęstościach \\(f_1,\\ldots, f_n\\) są niezależne wtedy tylko wtedy,\ngdy wektor\n\\(\\vec{X} = (X_1,\\ldots, X_n)\\) ma gęstość\n\\[\nf(x_1,x_2,\\ldots,x_n) = f_1(x_1)f_2(x_2)\\ldots f_n(x_n).\n\\]Jeżeli zmienne losowe \\(X_1,X_2\\) są niezależne mają absolutnie ciągły rozkład,\nmożna skutecznie liczyć rozkłady ich sum:Twierdzenie 12.2  Załóżmy, że \\(X_1\\) \\(X_2\\) są niezależnymi zmiennymi losowymi o rozkładach\nabsolutnie ciągłych z gęstościami \\(f_1\\) \\(f_2\\). Wówczas\nzmienna losowa \\(Z=X_1+X_2\\) ma rozkład absolutnie ciągły z gęstością\n\\[\n  f_Z(x) = f_1\\ast f_2(x) = \\int_\\mathbb{R} f_1(x-y)f_2(y) \\mathrm{d}y.\n\\]\nFunkcję \\(f_Z\\) nazywamy splotem funkcji \\(f_1\\) \\(f_2\\).Proof. Niech \\(\\vec{X}=(X_1, X_2)\\).\nDla dowolnego \\(t \\\\mathbb{R}\\):\n\\[\\begin{align*}\n    F_Z(t)=\\mathbb{P}[X_1+X_2 \\leq t] & = \\mu_{\\vec{X}}\\left( \\{(x_1,x_2):\\; x_1+x_2 \\leq t\\} \\right)\\\\\n    &= \\int\\int_{\\{(x_1,x_2):\\; x_1+x_2\\leq t \\}} \\mu_{\\vec{X}}(\\mathrm{d}x_1x_2)\\\\\n    &= \\int\\int_{\\{(x_1,x_2):\\; x_1+x_2\\leq t\\}} f_1(x_1) f_2(x_2) \\mathrm{d}x_1 \\mathrm{d}x_2\\\\\n    &= \\int\\int_{\\{(z,y):\\; z\\leq t\\}} f_1(z-y) f_2(y) \\mathrm{d}z \\mathrm{d}y\\\\\n    &= \\int_{(-\\infty,t]}\\int_{\\mathbb{R}} f_1(z-y) f_2(y) \\mathrm{d}y \\mathrm{d}z\\\\\n    &= \\int_{(-\\infty,t]} f_Z(z) \\mathrm{d}z.\n  \\end{align*}\\]\nTeza wynika z twierdzenia o jednoznaczności.Podstawowych intuicji związanych z operacją splotu można nabrać z filmu\nGranta Sandersona.Przykład 12.5  Niech \\(X_1\\) \\(X_2\\) będą niezależnymi zmiennymi losowymi o rozkładzie jednostajnym na \\([0,1]\\),\nczyli \\(U([0,1])\\). Szukamy gęstości \\(X_1+X_2\\).\nZnamy gęstości \\(X_1\\) oraz \\(X_2\\)\n\\[\n  f_1(x_1) = {\\bf 1}_{[0,1]}(x_1), \\qquad\n  f_2(x_2) = {\\bf 1}_{[0,1]}(x_2).\n\\]\nZatem \\(X_1+X_2\\) ma gęstość\n\\[\\begin{align*}\n    f_1\\ast f_2(x) &= \\int_\\mathbb{R}  {\\bf 1}_{[0,1]}(x - y) {\\bf 1}_{[0,1]}(y)\\mathrm{d}y\\\\\n    &=\\int_\\mathbb{R} {\\bf 1}_{[x-1,x]}(y) {\\bf 1}_{[0,1]}(y)\\mathrm{d}y = \\big| [x-1,x] \\cap [0,1] \\big|\n  \\end{align*}\\]\nStąd\n\\[\nf_1\\ast f_2(x) = \\left\\{\n\\begin{array}{cc}\n   0 & \\mbox{dla } x<0 \\\\\n   x & \\mbox{dla } x\\[0,1]\\\\\n   2-x & \\mbox{dla } x\\[1,2]\\\\\n   0 & \\mbox{dla } x>2\n\\end{array}\n\\right.\n\\]Przykład 12.6  Przypomnijmy, że zmienna losowa\n\\(X\\) ma rozkład normalny z parametrami \\(m\\\\mathbb{R}\\) \\(\\sigma>0\\),\n\\(\\mathcal{N}(m,\\sigma^2)\\) jeżeli jej gęstość\njest zadana wzorem\n\\[\n  f(x) = \\frac{1}{\\sqrt{2\\pi} \\sigma} e^{-\\frac{(x-m)^2}{2\\sigma^2}}.\n  \\]Załóżmy, że \\(X_1, X_2\\) są niezależnymi zmiennymi losowymi o rozkładzie \\(N(m_1,\\sigma_1^2)\\) oraz \\(N(m_2,\\sigma_2^2)\\), czyli z gęstościami \\[\n  f_i(x) = \\frac{1}{\\sqrt{2\\pi} \\sigma_i} e^{-\\frac{(x-m_i)^2}{2\\sigma_i^2}} \\qquad =1,2. \\]\nPokazuje się wówczas, że\n\\[   f_1\\ast f_2(x) = \\frac{1}{2\\pi \\sigma_1\\sigma_2} \\int_\\mathbb{R} e^{-\\frac{(x-y-m_1)^2}{2\\sigma_1^2} -\\frac{(y-m_2)^2}{2\\sigma_2^2}  }dy\n     = \\frac{1}{\\sqrt{2\\pi (\\sigma_1^2+\\sigma^2_2)}} e^{-\\frac{(x-m_1-m_2)^2}{2(\\sigma_1^2+\\sigma^2_2)}}\n\\]\nZatem \\(X_1+X_2\\) ma rozkład \\(\\mathcal{N}(m_1+m_2, \\sigma_1^2+\\sigma_2^2)\\).Jeszcze więcej o splotach od Granta Sandersona.","code":""},{"path":"wariancja.html","id":"wariancja","chapter":"13 Wariancja","heading":"13 Wariancja","text":"Dla zmiennej losowej \\(X\\) jej wartość oczekiwana \\(\\mathbb{E}[X]\\) jest średnią ważoną.\nW niektórych przypadkach, kiedy zmienna \\(X\\) jest skomplikowana,\naby wydobyć jakiekolwiek ilościowe informacje o \\(X\\),\nzmuszeni jesteśmy przybliżać \\(X\\) przez jej średnią \\(\\mathbb{E}[X]\\).\nChcielibyśmy wtedy wiedzieć jaki jest bład takiego przybliżenia.\ntego będzie nam służyła wariancja.\nWariancję będziemy definiować dla zmiennych losowych, które są dostatecznie regularne.Definicja 13.1  Powiemy, że zmienna losowa \\(X\\) jest całkowalna z kwadratem, jeżeli\n\\(\\mathbb{E}\\left[X^2 \\right]<\\infty\\).Przypomnijmy, że funkcja \\(\\varphi \\colon \\mathbb{R} \\\\mathbb{R}\\) jest wypukła,\njeżeli dla każdych \\(x, y \\\\mathbb{R}\\) oraz \\(\\alpha \\(0,1)\\)\n\\[\\begin{equation}\n    \\varphi(\\alpha x +(1-\\alpha)y) \\leq \\alpha \\varphi(x) +(1-\\alpha)\\varphi(y).\n    \\tag{13.1}\n\\end{equation}\\]\nGeometrycznie powyższy warunek oznacza, że odcinek łączący dwa punkty na wykresie \\((x, \\varphi(x))\\)\noraz \\((y, \\varphi(y))\\) leży w całości ponad wykresem.Lemma 13.1  (Nierówność Jensena) Niech \\(X\\) będzie zmienną losową taką, że \\(\\varphi(X)\\) ma wartość oczekiwaną dla\npewnej wypukłej funkcji \\(\\varphi \\colon \\mathbb{R} \\\\mathbb{R}\\). Wówczas\n\\[\\begin{equation*}\n    \\varphi\\left( \\mathbb{E} [X] \\right) \\leq \\mathbb{E}\\left[ \\varphi(X) \\right].\n\\end{equation*}\\]Proof. Pozostawiamy jako zadanie.Zauważmy, że jeżeli \\(\\mathbb{P}[X=x] =\\alpha\\) \\(\\mathbb{P}[X=y]=1-\\alpha\\), \nnierówność Jensena sprowadza się (13.1).\nRzeczywiście, mamy\n\\[\\begin{align*}\n    \\mathbb{E}[X] & = x \\mathbb{P}[X=x] +y\\mathbb{P}[X=y] \\\\ &= x\\alpha+y(1-\\alpha).\n\\end{align*}\\]\nStąd\n\\[\\begin{equation*}\n    \\varphi\\left(\\mathbb{E}[X] \\right) = \\varphi(\\alpha x +(1-\\alpha)y)\n\\end{equation*}\\]\njest nie większe niż\n\\[\\begin{align*}\n    \\mathbb{E}\\left[\\varphi(X)\\right] &= \\varphi(x) \\mathbb{P}[X=x] +\\varphi(y)\\mathbb{P}[X=y]\n    \\\\&= \\varphi(x)\\alpha+\\varphi(y)(1-\\alpha).\n\\end{align*}\\]Przypomnijmy, że jeżeli funkcja \\(\\varphi\\) jest dwukrotnie różniczkowalna,\njest ona wypukła wtedy tylko wtedy, gdy \\(\\varphi''(x) \\geq 0\\)\ndla wszystkich \\(x \\\\mathbb{R}\\).Zauważmy, że stosując nierówność Jensana funkcji wypukłej \\(\\varphi(x)=x^2\\) otrzymujemy\n\\[\\begin{equation*}\n    \\mathbb{E}[|X|]^2 \\leq \\mathbb{E}\\left[X^2 \\right].\n\\end{equation*}\\]\nOznacza , że każda zmienna całkowalna z kwadratem posiada wartość oczekiwaną.Definicja 13.2  Niech \\(X\\) będzie zmienną losową całkowalną z kwadratem. Liczbę\n\\[\\mathbb{V}ar  [X] =\\mathbb{E}[X-\\mathbb{E} [X]]^2\n  \\] nazywamy wariancją zmiennej losowej \\(X\\).Pierwiastek z wariancji nazywamy odchyleniem standardowym\n\\[\n  \\sigma_X =  \\sqrt{\\mathbb{V}ar[ X]}.\n  \\]Wartość oczekiwana odpowiada średniej wartości,\nwariancja opisuje odchylenie od wartości oczekiwanej.\nDla przykładu instytucje finansowe\nopisując inwestycję podają dwa kluczowe parametry:\nstopę zwrotu (wartość oczekiwaną zysku) oraz ryzyko (odchylenie standardowe).\nCelem inwestycji jest taki dobór instrumentów,\naby przy określonej stopie zwrotu zminimalizować ryzyko.Przykład 13.1  Przypuśćmy, że właśnie otrzymaliśmy propozycję nie odrzucenia;\nktoś podarował nam dwa losy na pewną loterię.\nOrganizatorzy loterii sprzedają \\(100\\) losów na cotygodniowe losowanie.\nKażdy z losów jest wybierany w jednorodnym procesie losowym, znaczy, że każdy los może być\nwybrany z takim samym prawdopodobieństwem — szczęśliwy\nwłaściciel wybranego losu wygrywa sto milionów dolarów.\nPozostałe \\(99\\) losów nic nie wygrywa.Możemy teraz wykorzystać nasz prezent na dwa sposoby: albo kupujemy dwa losy na samo losowanie,\nalbo kupimy po jednym losie na dwa różne losowanie.\nKtóra strategia jest lepsza? Spróbujmy przeanalizować\nprzy użyciu zmiennych losowych \\(X_1\\) \\(X_2\\)\nodpowiadających wysokości wygranej dla pierwszego dla drugiego losu.\nWartość oczekiwana \\(X_1\\), w milionach, wynosi\n\\[\n\\mathbb{E}[X_1] = \\frac{99}{100} \\cdot 0 + \\frac{1}{100} \\cdot 100 = 1\n\\]\njest taka sama dla \\(X_2\\). Wartości oczekiwane są addytywne,\ntak więc średnia całkowita wygrana (w milionach) wynosi\n\\[\n\\mathbb{E}[X_1 + X_2] = \\mathbb{E}[X_1] + \\mathbb{E}[X_2] = 2,\n\\]\nniezależnie od tego, jaką strategię wybierzemy.\nMimo obydwie strategie wyglądają różnie.\nNie patrzmy jednak na wartości oczekiwane \nprzeanalizujmy dokładnie rozkład zmiennej losowej \\(X_1 + X_2\\):Gdy kupimy dwa losy na tej samej loterii,\nwówczas mamy \\(98\\%\\) szansy przegranej \\(2\\%\\) szansy wygrania \\(100\\) milionów dolarów.\nJeśli kupimy je na różne losowania, mamy \\(98,01\\%\\) szansy przegranej,\nczyli odrobinę więcej niż poprzednio; mamy \\(0,01\\%\\) szansy wygrania \\(200\\) milionów dolarów,\nco jest również troszkę więcej niż poprzednio, nasze szanse na wygranie \\(100\\) milionów dolarów wynoszą teraz \\(1,98\\%\\).\nTak więc rozkład \\(X_1 + X_2\\) w drugim przypadku jest bardziej rozproszony:\nwartość oczekiwana, \\(100\\) milionów dolarów, jest mniej prawdopodobna,\nale wartości ekstremalne są odrobinę bardziej prawdopodobne.Wariancja ma służyć właśnie analizy pojęcia rozproszenia zmiennej losowej.\nMierzymy rozproszenie jako kwadrat odchylenia zmiennej losowej\nod jej wartości oczekiwanej. W przypadku 1 wariancja wynosi\n\\[\\begin{align*}\n& 0{,}98(0M - 2M)^2 + 0{,}02(100M - 2M)^2 \\\\&= 196M^2,\n\\end{align*}\\]\nw przypadku \\(2\\),\n\\[\\begin{align*}\n& 0{,}9801(0M - 2M)^2 + 0{,}0198(100M - 2M)^2 \\\\ & + 0{,}0001(200M - 2M)^2 \\\\&= 198M^2.\n\\end{align*}\\]\nTak jak oczekiwaliśmy,\ndruga wariancja jest odrobinę większa,\nponieważ rozkład losowy w przypadku \\(2\\) jest odrobinę bardziej rozproszony.Przykład 13.2  Rozważmy jeszcze jeden przykład o podobnej naturze.\nStudent w trakcie roku ma zaliczenia dwa kolokwia. Procentowy wynik\nkażdego kolokwium jest jednostajnie rozłożony na odcinku \\([0,1]\\).\nWyniki obu sprawdzianów są od siebie niezależne.\nJeżeli przez \\(U_1\\) \\(U_2\\) oznaczymy wyniki w odpowiednio pierwszym \ndrugim kolokwium, końcowa ocena studenta\njest wyliczona na podstawie wyniku\n\\[\n    X=U_1+U_2,\n\\]\nŚredni wynik z obu sprawdzianów \\(\\mathbb{E}[X]=1\\).\nPewien student nie mógł przystąpić pierwszego kolokwium,\nwobec czego prowadzący postanowił przeskalować wynik z pierwszego kolokwium.\nCzy jest rozwiązanie korzystne dla studenta? Wówczas ocena jest wyliczana na podstawie\nwyniku\n\\[\n    Y=2U_2\n\\]\nZe średnim wynikiem \\(\\mathbb{E}[Y]=1\\).\nAby dokładniej przeanalizować obie możliwości zauważmy, że\n\\[\\begin{equation*}\n\\mathbb{V}ar[Y] = \\int_0^1 (2x-1)^2 \\mathrm{d}x=4/3.\n\\end{equation*}\\]\nAby policzyć wariancję zmiennej \\(X\\) przypomnijmy, że ma ona gęstość zadaną przez\n\\[\\begin{equation*}\nf_X(x) = x \\mathbf{1}_{[0,1]}(x) + (2-x)\\mathbf{1}_{(1,2]}(x).\n\\end{equation*}\\]\nStąd\n\\[\\begin{align*}\n\\mathbb{V}ar[X] =& \\int_0^1 (x-1)^2 x \\mathrm{d}x \\\\ &+\\int_1^2(1-x)^2(2-x) \\mathrm{d}x =2/3.\n\\end{align*}\\]\nRozwiązanie zaproponowane przez prowadzącego ma istotnie większą wariancję. Sugeruje , że\nw przypadku przeskalowania wyniku drugiego kolokwium ostateczny wynik jest bardziej rozproszony.\nWydać też na symulacjach.Powodem jest , że rozkład \\(Y\\) dopuszcza wartości ekstremalne z większym prawdopodobieństwem.\nRzeczywiście, zauważmy, że \\(Y\\) ma rozkład o gęstości \\(f_Y(x) = \\mathbf{1}_{[0,2]}(x)/2\\).Twierdzenie 13.1  Niech \\(X\\) \\(Y\\) będą zmiennymi losowymi takimi, że \\(\\mathbb{E} [X^2]\\),\\(\\mathbb{E}[Y^2] <\\infty\\).\nWówczas\\(\\mathbb{V}ar[ X] <\\infty\\)\\(\\mathbb{V}ar[ X] = \\mathbb{E} [X^2] - (\\mathbb{E} [X])^2\\)\\(\\mathbb{V}ar[ X ]\\ge 0\\)\\(\\mathbb{V}ar[aX] = ^2\\mathbb{V}ar[X]\\)\\(\\mathbb{V}ar[X+] = \\mathbb{V}ar[X]\\)\\(\\mathbb{V}ar[ X] = 0\\) wtedy tylko wtedy, gdy \\(X\\) jest stałe z prawdopodobieństwem jedenJeżeli \\(X\\) \\(Y\\) są niezależne, \\(\\mathbb{V}ar[X+Y] = \\mathbb{V}ar[X] + \\mathbb{V}ar[Y]\\)Proof. Punkty 1 pokazaliśmy już wyżej. Punkt 2. wynika z rachunku\n\\[\\begin{align*}\n    \\mathbb{V}ar[X] & = \\mathbb{E}\\left[X^2 -2X\\mathbb{E}[X] + \\mathbb{E}[X]^2\\right]\\\\\n    & = \\mathbb{E}\\left[X^2 \\right] - 2 \\mathbb{E}\\left[X\\mathbb{E}[X] \\right] + \\mathbb{E}[X]^2\\\\\n    & = \\mathbb{E}\\left[X^2 \\right] - \\mathbb{E}[X]^2.\n\\end{align*}\\]\nDowód punktów 3-6 pozostawiamy jako ćwiczenie.\nPunkt 7 wynika z punktu 2:\n\\[\\begin{align*}\n\\mathbb{V}ar(X+Y)   = &   \\mathbb{E}\\left[ (X+Y)^2 \\right] - \\mathbb{E}[X + Y]^2\\\\\n     = & \\mathbb{E}[X^2] + 2 \\mathbb{E}[XY] + \\mathbb{E} [Y^2]  \\\\ &- (\\mathbb{E}[X]^2 +\n    2\\mathbb{E}[X] \\cdot \\mathbb{E} [Y] +\\mathbb{E}[ Y]^2)\\\\\n   = & \\mathbb{E}[ X^2]  - (\\mathbb{E}[X]^2  + \\mathbb{E} [Y^2] \\\\\n   &- (\\mathbb{E}[Y])^2  = \\mathbb{V}ar[ X] + \\mathbb{V}ar[ Y].\n\\end{align*}\\]\n□Jeżeli \\(X\\) ma rozkład dyskretny zadany przez\n\\(\\mathbb{P}[X=x_i] = p_i\\), \\(m = \\mathbb{E} [X]\\), \n\\[\\begin{align*}\n\\mathbb{V}ar[ X] & = \\sum_i p_i (x_i - m)^2  \\\\ &= \\sum_i  x_i^2p_i - m^2\n\\end{align*}\\]\nJeżeli natomiast \\(X\\) ma rozkład absolutnie ciągły z gęstością \\(g\\) \\(m = \\mathbb{E}[X]\\), \n\\[\\begin{align*}\n\\mathbb{V}ar [X]  & = \\int_\\mathbb{R} (x-m)^2 g(x)\\mathrm{d}x \\\\\n& = \\int_\\mathbb{R} x^2 g(x)\\mathrm{d}x - m^2.\n\\end{align*}\\]Przykład 13.3  Załóżmy, że zmienna losowa \\(X\\) ma rozkład geometryczny z parametrem \\(p>0\\)\n(\\(X\\sim{\\rm Geom}(p)\\)), tzn. \\(\\mathbb{P}[X=k] = p(1-p)^{k-1}\\), dla \\(k\\\\mathbb{N}\\).\nPrzypomnijmy, że \\(X\\) oznacza moment pierwszego sukcesu w nieskończonym schemacie Bernoulliego.\nIle wynosi \\(\\mathbb{V}ar[X]\\)?\nDla dowolnych \\(p, q \\(0,1)\\) mamy\n\\[\\begin{align*}\n    \\sum_{k=0}^\\infty q^kp & = \\frac{p}{1-q}\\\\\n    \\sum_{k=0}^\\infty k q^{k-1}p & = \\frac{p}{(1-q)^2}\\\\\n    \\sum_{k=0}^\\infty k(k-1) q^{k-2}p & = \\frac{2p}{(1-q)^3}\\\\\n    \\sum_{k=0}^\\infty k(k-1) q^{k-1}p & = \\frac{2pq}{(1-q)^3}.\n\\end{align*}\\]\nJeżeli podstawimy \\(q=1-p\\), druga czwarta równość dają\n\\[\\begin{align*}\n    \\mathbb{E}[X]& = \\sum_{k=0}^\\infty k q^{k-1}p  = \\frac{p}{(1-q)^2}=\\frac 1p\\\\\n    \\mathbb{E}[X(X-1)] & = \\sum_{k=0}^\\infty k(k-1) q^{k-1}p  = \\frac{2pq}{(1-q)^3} \\\\ &=\n    \\frac{2(1-p)}{p^2}\n\\end{align*}\\]\nWówczas\n\\[\\begin{align*}\n   \\mathbb{V}ar[ X] &  = \\mathbb{E}\\left[X^2\\right] - \\mathbb{E}[X]^2 \\\\ &=\n   \\mathbb{E}[X(X-1)] +\\mathbb{E}[X] -\\mathbb{E}[X]^2\n   \\\\ & = \\frac{1-p}{p^2}\n\\end{align*}\\]Przykład 13.4  Jeżeli \\(X\\sim \\mathrm{Bin}(n,p)\\)\n(\\(X\\) ma rozkład dwumianowy z parametrami \\(n,p\\)), \\(X\\) możemy przestawić\nw postaci \\(X = X_1+\\ldots + X_n\\), gdzie\n\\[X_i = \\left\\{\\begin{array}{cc}\n                 1 & \\mbox{ w $$-tym doświadczeniu jest sukces  } \\\\\n                 0 & \\mbox{ w $$-tym doświadczeniu jest porażka }\n               \\end{array}\n\\right.\n\\]\nZmienne \\(X_i\\) są niezależne oraz\n\\[\\begin{align*}\n\\mathbb{E} X_i & = p,\\\\\n\\mathbb{V}ar[ X_i] & = \\mathbb{E} \\left[X_i^2\\right] - (\\mathbb{E} X_i)^2\n\\\\ &= \\mathbb{E} [X_i] - p^2 = p(1-p).\n\\end{align*}\\]\nZatem z powyższego twierdzenia\n\\[\\begin{align*}\n  \\mathbb{E} [X] & = \\sum_{=1}^n \\mathbb{E} [X_i] = np\\\\\n  \\mathbb{V}ar[ X] & = \\sum_{=1}^n \\mathbb{V}ar [X_i] = np(1-p)\n\\end{align*}\\]Przykład 13.5  Jeżeli \\(X\\sim \\mathcal{N}(m,\\sigma^2)\\) (zmienna losowa \\(X\\) ma rozkład normalny z\nparametrami \\(m, \\sigma^2\\)), \n\\[\\begin{align*}\n  \\mathbb{E} X & =\n  \\frac{1}{\\sqrt{2\\pi}\\sigma} \\int_\\mathbb{R} x e^{-\\frac{(x-m)^2}{2\\sigma^2}} \\mathrm{d}x\n  \\\\ & \\overset{ y=(x-m)/\\sigma}{ =} \\frac{1}{\\sqrt{2\\pi}} \\int_\\mathbb{R} (\\sigma y + m )\n  e^{-y^2/2} \\mathrm{d}y = m\n\\end{align*}\\]\noraz\n\\[\\begin{align*}\n  \\mathbb{V}ar[ X] =&  \\mathbb{E}(X-m)^2  \n  \\\\ =&  \\frac{1}{\\sqrt{2\\pi}\\sigma} \\int_\\mathbb{R} (x-m)^2 e^{-\\frac{(x-m)^2}{2\\sigma^2}} \\mathrm{d}x\\\\\n  &\\overset{ y=(x-m)/\\sigma}{ =} \\frac{1}{\\sqrt{2\\pi}} \\int_\\mathbb{R} \\sigma^2 y^2   e^{-y^2/2} \\mathrm{d}y \\\\\n  =&\\frac{\\sigma^2}{\\sqrt{2\\pi}}\\big( -y e^{-\\frac{y^2}2} \\big)\\Big|_{-\\infty}^{+\\infty}\n   \\\\ &+ \\frac{\\sigma^2}{\\sqrt{2\\pi}} \\int_\\mathbb{R} e^{-y^2/2}\\mathrm{d}y = \\sigma^2.\n\\end{align*}\\]","code":""},{"path":"kowariancja.html","id":"kowariancja","chapter":"14 Kowariancja","heading":"14 Kowariancja","text":"Wariancja stanowi ilościowy opis rozproszenia rozkładu zmiennej.\nOpiszemy teraz jej dwuliniowy odpowiednik, który można\ntraktować jak ilościowy opis zależności dwóch zmiennych losowych.Definicja 14.1  Niech \\(X\\) \\(Y\\) będą zmiennymi losowymi całkowalnymi z kwadratem.\nLiczbę\n\\[\n  {\\rm Cov}(X,Y) = \\mathbb{E}\\left[ (X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y]) \\right]\n  \\] nazywamy kowariancją \\(X\\) \\(Y\\).Jeżeli \\(\\mathrm{Cov}(X,Y)=0\\), \\(X\\) \\(Y\\) nazywamy nieskorelowanymi.Zauważmy, że przy powyższych założeniach kowariancja jest dobrze zdefiniowana.Lemma 14.1  (Nierówność Schwarza) Dla zmiennych losowych \\(X\\) \\(Y\\),\n\\[\n\\mathbb{E}[|XY|] \\le \\big( \\mathbb{E}[X^2] \\big)^{1/2}\\big( \\mathbb{E}[ Y^2] \\big)^{1/2},\n\\]Proof. Pozostawiamy jako zadanie.\n□Zastępując \\(X\\) przez \\(X -\\mathbb{E}[X]\\) oraz \\(Y\\) przez \\(Y -\\mathbb{E}[Y]\\) otrzymujemy\n\\[\n{\\rm Cov}(X,Y) \\le \\big( \\mathbb{V}ar[ X] \\big)^{1/2}\\big( \\mathbb{V}ar[ Y] \\big)^{1/2}.\n\\]\nJeżeli więc \\(X\\) \\(Y\\) są całkowalne z kwadratem, \\(\\mathrm{Cov}(X,Y)<\\infty\\).Twierdzenie 14.1  Niech \\(X\\), \\(Y\\) \\(Z\\) będą zmiennymi losowymi całkowalnymi z kwadratem\\(\\mathrm{ Cov}(X,Y) = \\mathbb{E}[XY] -\\mathbb{E}[X] \\mathbb{E}[Y]\\).\\(\\mathrm{Cov}(X,X) = \\mathbb{V}ar [X]\\).\\(\\mathrm{Cov} (X,Y) = \\mathrm{Cov} (Y,X)\\).Kowariancja jest operatorem dwuliniowym\\[\\begin{equation*}\n    {\\rm Cov}(aX+,Z) = {\\rm Cov}(X,Z)\n  + b  {\\rm Cov}(Y,Z).\n\\end{equation*}\\]Proof. Ćwiczenie.\n□Przykład 14.1  Niech \\(X\\) \\(Y\\) będą takie, że dwuwymiarowy wektor losowy \\(\\vec{X}=(X,Y)\\)\nma dwuwymiarowy rozkład normalny z parametrami \\(\\vec{m}=(0,0)\\) oraz\n\\[\\begin{equation*}\n    \\Sigma = \\left( \\begin{array}{cc} 1 & \\rho \\\\ \\rho & 1 \\end{array} \\right)\n\\end{equation*}\\]\ndla \\(\\rho \\(-1,1)\\).\nPrzypomnijmy, że wektor \\(\\vec{X}\\) ma rozkład o gęstości\n\\[\\begin{equation*}\nf_\\vec{X}(x,y) =\n\\frac{1}{2 \\pi \\mathrm{det}(\\Sigma)^{1/2}} e^{ -\\langle \\Sigma^{-1}(x,y)^T, (x,y)^T \\rangle /2}\n\\end{equation*}\\]\nSprawdzimy najpierw ile wynosi średnia \\(X\\). Stosując podstawienie \\(z=-x\\) \\(w=-y\\)\notrzymujemy\n\\[\\begin{align*}\n    \\mathbb{E}[X] & = \\int_{\\mathbb{R}^2} x f_{\\vec{X}}(x,y) \\mathrm{d}xy \\\\\n    & = \\int_{\\mathbb{R}^2} -z f_{\\vec{X}}(z,w) \\mathrm{d}zw.\n\\end{align*}\\]\nWobec czego \\(\\mathbb{E}[X]=0\\). Podobnie sprawdzamy, że \\(\\mathbb{E}[Y]=0\\).\nIle wynosi \\(\\mathrm{Cov}(X,Y)\\)? Na pierwszy rzut oka wyrażenie całkowe\n\\[\\begin{align*}\n    &\\mathrm{Cov}(X,Y) = \\\\ &\\int_{\\mathbb{R}^2}\n\\frac{xy}{2 \\pi \\mathrm{det}(\\Sigma)^{1/2}} e^{ -\\langle \\Sigma^{-1}(x,y)^T, (x,y)^T \\rangle /2}\n\\mathrm{d}xy\n\\end{align*}\\]\nwygląda niezachęcająco. Aby się z nim efektywnie uporać musimy przedstawić\nwyraz wykładniczy w prostszej postaci. Sprowadza się analizy wykładnika\n\\[\\begin{equation*}\n\\langle \\Sigma^{-1}(x,y)^T, (x,y)^T \\rangle\n\\end{equation*}\\]\nczyli formy kwadratowej związanej z macierzą \\(\\Sigma\\). W pierwszym kroku diagonalizujemy\n\\(\\Sigma\\). Otrzymując\n\\[\\begin{equation*}\n    \\Sigma = Q \\left( \\begin{array}{cc} 1+\\rho & 0 \\\\ 0 & 1-\\rho \\end{array} \\right) Q^T,\n\\end{equation*}\\]\ngdzie\n\\[\\begin{equation*}\n    Q = \\left( \\begin{array}{cc} \\sqrt{2}/2 & \\sqrt{2}/2 \\\\ \\sqrt{2}/2 & -\\sqrt{2}/2 \\end{array} \\right)\n\\end{equation*}\\]\njest macierzą symetrii względem prostej \\(y=\\tan(\\pi/8)x\\). Jeżeli zatem położymy\n\\[\\begin{equation}\n\\left( \\begin{array}{c} z  \\\\ w \\end{array} \\right) = Q^T\n\\left( \\begin{array}{c} x  \\\\ y \\end{array} \\right)\n\\tag{14.1}\n\\end{equation}\\]\n\n\\[\\begin{equation*}\n\\langle \\Sigma^{-1}(x,y)^T, (x,y)^T \\rangle =\\\\\n\\left\\langle\\left( \\begin{array}{cc} 1/(1+\\rho) & 0 \\\\ 0 & 1/(1-\\rho) \\end{array} \\right)  \n\\left( \\begin{array}{c} z  \\\\ w \\end{array} \\right) ,\n\\left( \\begin{array}{c} z  \\\\ w \\end{array} \\right) \\right\\rangle \\\\\n=\\frac{z^2}{1+\\rho} + \\frac{w^2}{1-\\rho}.\n\\end{equation*}\\]\nWspółrzędne \\((z,w)\\) odpowiednie podstawienie dla naszej całki. Zauważmy, że \\(\\mathrm{det}(Q)=1\\)\noraz, że\n\\[\\begin{equation*}\nxy=\\frac{z^2-w^2}{2}.\n\\end{equation*}\\]\nStąd, stosując podstawienie (14.1) w całce otrzymujemy\n\\[\\begin{equation*}\n\\mathrm{Cov}(X,Y) = \\\\\\int_{\\mathbb{R}^2} \\frac{z^2-w^2}{2}\n\\frac{1}{\\sqrt{2\\pi}(1+\\rho)} e^{-z^2/2(1+\\rho)} \\\\\n\\frac{1}{\\sqrt{2\\pi}(1-\\rho)} e^{-w^2/2(1-\\rho)} \\mathrm{d}zw.\n\\end{equation*}\\]\nWykorzystując znane nam już tożsamości\n\\[\\begin{equation*}\n\\int_\\mathbb{R}\\frac{1}{\\sqrt{2\\pi}(1+\\rho)} e^{-z^2/2(1+\\rho)}\\mathrm{d}z =\\\\\n\\int_\\mathbb{R}\\frac{1}{\\sqrt{2\\pi}(1-\\rho)} e^{-w^2/2(1-\\rho)} \\mathrm{d}w=1\n\\end{equation*}\\]\noraz\n\\[\\begin{equation*}\n\\int_\\mathbb{R}\\frac{z^2}{\\sqrt{2\\pi}(1+\\rho)} e^{-z^2/2(1+\\rho)}\\mathrm{d}z = 1+\\rho\n\\end{equation*}\\]\\[\\begin{equation*}\n\\int_\\mathbb{R}\\frac{w^2}{\\sqrt{2\\pi}(1-\\rho)} e^{-w^2/2(1-\\rho)} \\mathrm{d}w=1-\\rho\n\\end{equation*}\\]\nOtrzymujemy\n\\[\\begin{equation*}\n\\mathrm{Cov}(X,Y) = \\frac{1+\\rho}{2} - \\frac{1-\\rho}{2} = \\rho.\n\\end{equation*}\\]Remark. Przy użyciu powyższych rachunków można pokazać, że zmienne losowe \\(Z\\) \\(W)\\) dane przez\n\\[\\begin{equation*}\n\\left( \\begin{array}{c} Z  \\\\ W \\end{array} \\right) = Q^T\n\\left( \\begin{array}{c} X  \\\\ Y \\end{array} \\right)\n\\end{equation*}\\]\nsą niezależne. Jeżeli wylosujemy \\(100\\) punktów z rozkładu normalnego z powyższego przykładu\n(dla \\(\\rho=2/3\\)) widzimy, że współrzędnych \\(x,y\\) punkty układają się wzdłuż prostej \\(x=y\\).\n\nwspółrzędnych \\(zw\\) natomiast punkty układają się wzdłuż osi \\(z\\).Remark. Jeżeli \\(X,Y\\) są niezależne, \\(\\mathrm{Cov}(X,Y)=0\\) (wynika z pkt. 1 powyższego twierdzenia),\nale nie jest prawdziwa odwrotna implikacja.\nKowariancja mierzy jak bardzo zmienne losowe \\(X\\), \\(Y\\) są zależne.Twierdzenie 14.2  Jeżeli \\(\\mathrm{E}[ X_i^2]<\\infty\\) dla \\(=1,\\ldots, n\\), \n\\[\n  \\mathbb{V}ar(X_1+\\ldots+ X_n) = \\sum_{k=1}^n \\mathbb{V}ar X_k \\\\+ 2 \\sum_{k<l}{\\rm  Cov}(X_k,X_l).\n  \\]\nW szczególności jeżeli \\(X_i\\) są wzajemnie nieskorelowane, \n\\[\n  \\mathbb{V}ar(X_1+\\ldots+ X_n) = \\sum_{k=1}^n \\mathbb{V}ar X_k .\n  \\]Proof. Teza wynika z bezpośredniego rachunku. Mamy\n\\[\\begin{align*}\n  &\\mathbb{V}ar(X_1+\\ldots+ X_n)    \\\\\n  = & \\mathbb{E}\\Big[  X_1+\\ldots + X_n - \\mathbb{E}\\big[X_1+\\ldots + X_n \\big]  \\Big]^2\\\\\n  = & \\mathbb{E}\\bigg[ \\sum_{j=1}^n (X_j -\\mathbb{E} X_j) \\bigg]^2\\\\\n  = &  \\sum_{j=1}^n\\mathbb{E}(X_j-\\mathbb{E} X_j)^2 \\\\\n  & + 2\\sum_{k<l} \\mathbb{E}\\big[(X_k-\\mathbb{E} X_k)(X_l-\\mathbb{E} X_l,)\\big]\\\\\n  = & \\sum_{k=1}^n \\mathbb{V}ar X_k + 2 \\sum_{k<l}{\\rm Cov}(X_k,X_l).\n  \\end{align*}\\]\n□Przykład 14.2  Niech \\(\\sigma\\) będzie losową permutacją liczb \\(1,\\ldots,n\\) niech\n\\(X\\) oznacza liczbę punktów stałych \\(\\sigma\\). Szukamy \\(\\mathbb{V}ar [X]\\).Niech \\[X_i = \\left\\{\\begin{array}{cc}\n                 1 & \\mbox{ jeżeli $$ jest punktem  stałym} \\\\\n                 0 & \\mbox{ jeżeli $$ nie jest punktem  stałym}\n               \\end{array}\n\\right.\n\\] Wtedy \\(X = \\sum_{=1}^n X_i\\). Wyliczamy\n\\[\\begin{align*}\n\\mathbb{E} [X_i] &= \\frac 1n,\\quad \\mathbb{E} [X] = 1,\\\\\n\\mathbb{E} [X_iX_j] &= \\mathbb{P}[X_i X_j = 1] = \\frac{1}{n(n-1)},\\quad \\mbox{dla }\\=j \\\\\n\\mathbb{V}ar [X_i] &= \\mathbb{E} \\left[X_i^2\\right] - (\\mathbb{E} [X_i])^2 = \\frac{n-1}{n^2}.\n\\end{align*}\\]\nWówczas z powyższego twierdzenia\n\\[\\begin{align*}\n\\mathbb{V}ar X = &  n\\cdot \\frac{n-1}{n^2} \\\\\n& +  2\\cdot \\frac{n(n-1)}2\\cdot\\bigg( \\frac 1{n(n-1)}  - \\frac 1{n^2} \\bigg)\n\\\\ =& \\frac{n-1}n + n\\bigg(1 - \\frac{n-1}{n}\\bigg) = 1.\n\\end{align*}\\]","code":""},{"path":"regresja-liniowa.html","id":"regresja-liniowa","chapter":"15 Regresja liniowa","heading":"15 Regresja liniowa","text":"Dane są dwie zmienne losowe \\(X\\) \\(Y\\).\nZałóżmy, że są one skorelowane. Możemy myśleć, że \\(Y\\) zależy od \\(X\\)\n(np. \\((X,Y)\\) (wzrost, waga), (liczba klientów, dochód), (przebieg samochodu, zużycie paliwa),\n(indeks Dow Jones, indeks Wig 20)).\nChcemy na podstawie obserwacji \\(X(\\omega_0)\\) wyznaczyć \\(Y(\\omega_0)\\).\nW tym celu szukamy funkcji \\(f\\) takiej, że \\(Y = f(X)\\).Regresja liniowa polega na tym, że szuka się funkcji liniowej.\nInnymi słowy pracujemy przy założeniu, że zależność \\(Y\\) od \\(X\\) jest liniowa.Przykład 15.1  Niech \\(X(\\omega)\\) \\(Y(\\omega)\\) będzie budżetem filmu \\(\\omega\\). Za zbiór zdarzeń elementarnych\nprzyjmujemy \\(\\Omega\\) będące zbiorem filmów (bazą internetową) zawierającą informacje o budżecie.\nDla potrzeby tego przykładu posłużymy się bazą TMDB.\nPoniżej przedstawiamy wstępną analizę naszych zmiennych wykonaną w języku R.Na pierwszy rzut oka widzimy, że dane układają się symetrycznie wzdłuż osi budżetu. Sugeruje \nniezależność bądź bardzo niska korelację.Przykład 15.2  Rozważmy podobny przykład, lecz tym razem badać będziemy \\(X\\) będące rokiem produkcji \\(Y\\) będące\nśrednią ocen na IMBD. Za \\(\\Omega\\) obieramy bazę\n1000 najwyżej ocenianych filmów z IMDB. Należy mieć na względzie, że\ntutaj oceny liczone są z dokładnością jednego miejsca po przecinku.W tym przypadku widzimy tendencję spadkową.Zanim dokończymy analizę przedstawionych wyżej przypadków omówimy problem ogólny.\nBędziemy chcieli wyznaczyć liniową zależność między \\(X\\) \\(Y\\). Pytamy dla jakich\nwartości \\(,b \\\\mathbb{R}\\) zmienna\n\\[\\begin{equation*}\n\\hat{Y} = aX+b\n\\end{equation*}\\]\nnajlepiej przybliża \\(Y\\)?\nBłąd naszego przybliżenia będziemy liczyli w sensie średniokwadratowym\nChcemy zminimalizować wartość\n\\[\ng(,b) = \\mathbb{E} \\left[ \\left( Y - \\hat{Y} \\right)^2 \\right].\n\\]\nW tym celu liczymy pochodne cząstkowe szukamy punktu,\nw którym zeruje się gradient\n\\[\\begin{align*}\n   \\frac{\\partial g}{\\partial } & = 2a\\mathbb{E}\\left[ X^2 \\right] -\n   2\\mathbb{E} [XY] + 2 b \\mathbb{E}[X] = 0,\\\\\n   \\frac{\\partial g}{\\partial b} & =  2b - 2\\mathbb{E}[ Y] + 2a \\mathbb{E}[ X]  = 0.\n\\end{align*}\\]\nRozwiązując powyższy układ równań otrzymujemy\n\\[\n= \\frac{{\\rm Cov}(X,Y)}{\\mathbb{V}ar[ X] }, \\qquad\nb= -\\frac{{\\rm Cov}(X,Y)}{\\mathbb{V}ar[ X]} \\mathbb{E}[ X] + \\mathbb{E}[ Y].\n\\]\nOznacza , że szukana przez nas funkcja \\(f\\) ma postać\n\\[\nf(x) = y = \\frac{{\\rm Cov}(X,Y)}{\\mathbb{V}ar[ X]}(x-\\mathbb{E}[ X]) + \\mathbb{E}[Y].\n\\]\nCzyli\n\\[\n\\hat{Y}(\\omega) =\n\\frac{{\\rm Cov}(X,Y)}{\\mathbb{V}ar[ X]}(X(\\omega)-\\mathbb{E}[ X]) + \\mathbb{E}[Y].\n\\]\nZauważmy, że wyznaczenia prostej regresji nie trzeba znać całego rozkładu\nłącznego \\((X,Y)\\), ale wystarczy \\(\\mathbb{E}[ X]\\), \\(\\mathbb{E}[ Y]\\), \\(\\mathbb{V}ar[ X]\\),\nCov\\((X,Y)\\). Zauważmy też, że jeżeli \\(X\\) \\(Y\\) są dodatnio skorelowane, czyli \\(\\mathrm{Cov}(X,Y)>0\\),\n“rosnące wartości \\(X\\) oznaczają rosnące wartości \\(Y\\)’’Przykład 15.3  Wróćmy przykładu oceny filmu jego budżetu. Mając\nnasze dane możemy obliczyć\nśrednie, kowariancję oraz odpowiednią wariancję. Zakładamy, że wylosowanie każdego filmu\njest jednakowo prawdopodobne.Ocena = 6.210116 + 4.41581e-10 * budżetWidzimy, że w tym wypadku współczynnik korelacji jest bardzo mały.\nPrzy tego typu wynikach nie ma statystycznej istotności:\nnie możemy stwierdzić, że budżet cokolwiek tłumaczy.Przykład 15.4  Na koniec wróćmy przykładu oceny filmu roku produkcji.Ocena = 11.03454 + -0.001549247 * rokW tym przypadku widzimy, że wpływ roku jest mały (ale istotnie większy niż w przypadku budżetu!).Przykład 15.5  Rozwazmy ostatni przykład. Jak budżet filmu ma się jego dochodów?Dochody = -5579407 + 2.956959 * budżetWidzimy, że w tym wypadku współczynnik korelacji jest duży.","code":"\n# wczytujemy i czyścimy dane\nmovies <- read.csv(\"R/tmdb_5000_movies.csv\")\nmovies_clean <- subset(movies, budget > 0 & !is.na(vote_average))\n\n# Ustawiamy tło i kolory na wykresie\npar(bg = \"#002b36\", col.axis = \"#eee8d5\", col.lab = \"#eee8d5\", \n    col.main = \"#eee8d5\", fg = \"#eee8d5\")\n\n# Zaznaczamy punkty\nplot(movies_clean$budget, movies_clean$vote_average,\n     main = \"Ocena filmu vs budżet\",\n     xlab = \"Budżet (USD)\",\n     ylab = \"Średnia ocena\",\n     pch = 20,\n     col = \"#eee8d5\")\nimdb <- read.csv(\"R/imdb_top_1000.csv\")\nimdb_clean <- subset(imdb, grepl(\"^[0-9]{4}$\", \n                 Released_Year) & !is.na(IMDB_Rating))\n\n\npar(bg = \"#002b36\", col.axis = \"#eee8d5\", col.lab = \"#eee8d5\", \n    col.main = \"#eee8d5\", fg = \"#eee8d5\")\n\n# Punkty\nplot(imdb_clean$Released_Year, imdb_clean$IMDB_Rating,\n     main = \"Ocena IMDb względem roku produkcji\",\n     xlab = \"Rok produkcji\",\n     ylab = \"Ocena IMDb\",\n     pch = 20,\n     col = \"#eee8d5\")\n# wczytujemy i czyścimy dane\nmovies <- read.csv(\"R/tmdb_5000_movies.csv\")\nmovies_clean <- subset(movies, budget > 0 & !is.na(vote_average))\n\n# liczymy wartości oczekiwane\nEx <- mean(movies_clean$budget)\nEy <- mean(movies_clean$vote_average)\n\n# Kowariancja\nCovXY <- sum( (movies_clean$budget - Ex) * \n         (movies_clean$vote_average - Ey))\n\n# wariancja X -budżetu\nVarX <- sum( (movies_clean$budget - Ex)^2 )\n\n# liczymy współczynniki\na <- CovXY / VarX\nb <- Ey - a * Ex\n\ncat('<span style=\"color: #eee8d5;\">Ocena = ', \n    b, ' + ', a, ' * budżet<\/span><br>')\n# Ustawiamy tło i kolory na wykresie\npar(bg = \"#002b36\", col.axis = \"#eee8d5\", col.lab = \"#eee8d5\", \n    col.main = \"#eee8d5\", fg = \"#eee8d5\")\n\n# Zaznaczamy punkty\nplot(movies_clean$budget, movies_clean$vote_average,\n     main = \"Ocena filmu vs budżet\",\n     xlab = \"Budżet (USD)\",\n     ylab = \"Średnia ocena\",\n     pch = 20,\n     col = \"#eee8d5\")\n\n# Prosta regresji\n\n# Zakres budżetu (x)\nx_vals <- range(movies_clean$budget)\n# Odpowiednie wartości y\ny_vals <- b + a * x_vals\n\nlines(x_vals, y_vals, col = \"#2aa198\", lwd = 2)\nimdb <- read.csv(\"R/imdb_top_1000.csv\")\nimdb_clean <- subset(imdb, grepl(\"^[0-9]{4}$\", \n                 Released_Year) & !is.na(IMDB_Rating))\nimdb_clean$Released_Year <- as.numeric(imdb_clean$Released_Year)\n\n# liczymy wartości oczekiwane\nEx <- mean(imdb_clean$Released_Year)\nEy <- mean(imdb_clean$IMDB_Rating)\n\n# Kowariancja\nCovXY <- sum( (imdb_clean$Released_Year - Ex) * \n         (imdb_clean$IMDB_Rating - Ey))\n\n# wariancja X -budżetu\nVarX <- sum( (imdb_clean$Released_Year - Ex)^2 )\n\n# liczymy współczynniki\na <- CovXY / VarX\nb <- Ey - a * Ex\n\ncat('<span style=\"color: #eee8d5;\">Ocena = ', \n    b, ' + ', a, ' * rok<\/span><br>')\npar(bg = \"#002b36\", col.axis = \"#eee8d5\", col.lab = \"#eee8d5\", \n    col.main = \"#eee8d5\", fg = \"#eee8d5\")\n\n# Punkty\nplot(imdb_clean$Released_Year, imdb_clean$IMDB_Rating,\n     main = \"Ocena IMDb względem roku produkcji\",\n     xlab = \"Rok produkcji\",\n     ylab = \"Ocena IMDb\",\n     pch = 20,\n     col = \"#eee8d5\")\n\n# Prosta regresji\n\n# Zakres budżetu (x)\nx_vals <- range(imdb_clean$Released_Year)\n# Odpowiednie wartości y\ny_vals <- b + a * x_vals\n\nlines(x_vals, y_vals, col = \"#2aa198\", lwd = 2)\n# wczytujemy i czyścimy dane\nmovies <- read.csv(\"R/tmdb_5000_movies.csv\")\nmovies_clean <- subset(movies, budget > 0 & !is.na(vote_average))\n\n# liczymy wartości oczekiwane\nEx <- mean(movies_clean$budget)\nEy <- mean(movies_clean$revenue)\n\n# Kowariancja\nCovXY <- sum( (movies_clean$budget - Ex) * \n         (movies_clean$revenue - Ey))\n\n# wariancja X -budżetu\nVarX <- sum( (movies_clean$budget - Ex)^2 )\n\n# liczymy współczynniki\na <- CovXY / VarX\nb <- Ey - a * Ex\n\ncat('<span style=\"color: #eee8d5;\">Dochody = ', \n    b, ' + ', a, ' * budżet<\/span><br>')\n# Ustawiamy tło i kolory na wykresie\npar(bg = \"#002b36\", col.axis = \"#eee8d5\", col.lab = \"#eee8d5\", \n    col.main = \"#eee8d5\", fg = \"#eee8d5\")\n\n# Zaznaczamy punkty\nplot(movies_clean$budget, movies_clean$revenue,\n     main = \"Dochody filmu vs budżet\",\n     xlab = \"Budżet (USD)\",\n     ylab = \"Dochody (USD)\",\n     pch = 20,\n     col = \"#eee8d5\")\n\n# Prosta regresji\n# Zakres budżetu (x)\nx_vals <- range(movies_clean$budget)\n# Odpowiednie wartości y\ny_vals <- b + a * x_vals\n\nlines(x_vals, y_vals, col = \"#2aa198\", lwd = 2)"},{"path":"parametry-wielowymiarowe.html","id":"parametry-wielowymiarowe","chapter":"16 Parametry wielowymiarowe","heading":"16 Parametry wielowymiarowe","text":"Omówimy pokrótce wielowymiarowe parametry wektorów losowych.\nOd tej pory przyjmujemy konwencję, że wszystkie rozważane wektory są pionowe.","code":""},{"path":"parametry-wielowymiarowe.html","id":"wektor-średnich","chapter":"16 Parametry wielowymiarowe","heading":"16.1 Wektor średnich","text":"Definicja 16.1  Niech \\(\\vec{X}=(X_1, \\ldots , X_d)^T\\) będzie \\(d\\)-wymiarowym wektorem losowym. Powiemy, że\n\\(\\vec{X}\\) ma wartość oczekiwaną\nże wszystkie zmienne losowe \\(X_1, \\ldots , X_d\\) mają\nwartości oczekiwane. Wówczas wektor\n\\[\n  \\mathbb{E}\\left[ \\vec{X} \\right] = (\\mathbb{E} [X_1],\\ldots,\\mathbb{E} [X_d])\n  \\]\nnazywamy wartością oczekiwaną zmiennej losowej \\(\\vec{X}\\).Przykład 16.1  Niech \\(\\vec{Y}=(Y_1, \\ldots, Y_d)^T\\)\nbędzie wektorem losowym o \\(d\\)-wymiarowym wektorem losowym z rozkładem \\(\\mathcal{N}(\\vec{m}, \\Sigma)\\),\ngdzie \\(\\vec{m} = (m_1, m_2, \\ldots, m_d)^T\\).\nPrzypomnijmy, że oznacza , że ma gęstość zadaną przez\n\\[\\begin{equation*}\nf_{\\vec{Y}}\\left(\\vec{y} \\right) = \\frac{1}{(2\\pi)^{d/2} \\mathrm{det}(\\Sigma)^{1/2}}\n\\exp \\left\\{ - \\langle \\Sigma^{-1}(\\vec{y}-\\vec{m}), \\vec{y}-\\vec{m} \\rangle/2 \\right\\}\n\\end{equation*}\\]\nAby wyznaczyć wektor \\(\\mathbb{E}[\\vec{Y}]\\) ustalmy \\(j\\[d] = \\{1,2, \\ldots, d\\}\\) napiszmy\nstosując podstawienie \\(\\vec{y}=\\vec{z}+\\vec{m}\\), że\n\\[\\begin{align*}\n    \\mathbb{E}[Y_j] & = \\int_{\\mathbb{R}^d} y_j f_{\\vec{Y}}\\left(\\vec{y} \\right) \\mathrm{d}\\vec{y} \\\\\n        & = \\int_{\\mathbb{R}^d} (z_j +m_j) f_{\\vec{Y}}\\left(\\vec{z} +\\vec{m} \\right) \\mathrm{d}\\vec{z}\n\\end{align*}\\]\nZauważmy, że\n\\[\\begin{equation*}\nf_{\\vec{Y}}\\left(\\vec{z}+\\vec{m} \\right) = \\frac{1}{(2\\pi)^{d/2} \\mathrm{det}(\\Sigma)^{1/2}}\n\\exp \\left\\{ - \\langle \\Sigma^{-1}\\vec{z}, \\vec{z}\\rangle/2 \\right\\}\n\\end{equation*}\\]\njest symetryczna względem zera (\\(f_{vec{Y}} (\\vec{z}+\\vec{m}) = f_{vec{Y}}(-\\vec{z}+\\vec{m})\\)) gęstością rozkładu\n\\(\\mathcal{N}(\\vec{0}, \\Sigma)\\). W szczególności\n\\[\\begin{equation*}\n    \\int_{\\mathbb{R}^d} f_{\\vec{Y}}\\left(\\vec{z} +\\vec{m} \\right) \\mathrm{d}\\vec{z}=1\n\\end{equation*}\\]\noraz\n\\[\\begin{equation*}\n    \\int_{\\mathbb{R}^d} z_jf_{\\vec{Y}}\\left(\\vec{z} +\\vec{m} \\right) \\mathrm{d}\\vec{z}=0.\n\\end{equation*}\\]\nOstatecznie stąd \\(\\mathbb{E}[Y_j]=m_j\\) co za tym idzie\n\\[\\begin{equation*}\n    \\mathbb{E}\\left[\\vec{Y} \\right] = \\vec{m}.\n\\end{equation*}\\]Twierdzenie 16.1  Niech \\(\\vec{X}=(X_1, \\ldots , X_d)^T\\) będzie \\(d\\)-wymiarowym wektorem losowym. Wówczas \\(\\vec{X}\\) ma wartość oczekiwaną\nwtedy tylko wtedy, gdy zmienna losowa\n\\[\\begin{equation*}\n    \\left\\|\\vec{X} \\right\\| = \\sqrt{\\sum_{j=1}^d X_j^2}\n  \\end{equation*}\\]\nma wartość oczekiwaną.\nWówczas\n\\[\\begin{equation}\n     \\left\\|\\mathbb{E}\\left[\\vec{X}\\right]\\right\\|\\le \\mathbb{E}\\left[\\left\\|\\vec{X}\\right\\|\\right].\n  \\tag{16.1}\n  \\end{equation}\\]\nJeżeli \\(= (A_{,j})_{\\leq m, j\\leq d}\\) jest macierzą \\(m \\times d\\), \\(\\vec{Y}\\) jest \\(m\\)-wymiarowym\nwektorem losowym o średniej\n\\[\\begin{equation*}\n    \\mathbb{E}\\left[\\vec{Y} \\right] = \\mathbb{E}\\left[\\vec{Y}\\right].\n  \\end{equation*}\\]\nJeżeli \\(\\vec{Y}\\) jest wektorem losowy posiadającym wartość oczekiwaną, \n\\[\\begin{equation*}\n\\mathbb{E}\\left[\\vec{X}+b\\vec{Y}\\right] = \\mathbb{E}\\left[\\vec{X}\\right] + b \\mathbb{E}\\left[\\vec{Y}\\right]\n\\end{equation*}\\]\ndla dowolnych rzeczywistych \\(\\) \\(b\\).Proof. Pierwszy postulat wynika z nierówności\n\\[\\begin{equation*}\n        |X_j| \\le \\left\\|\\vec{X}\\right\\| \\le \\sum_{=1}^d|X_i|.\n  \\end{equation*}\\]\nDruga nierówność jest konsekwencją podaddytywności pierwiastka: \\(\\sqrt{x+y} \\leq \\sqrt{x}+\\sqrt{y}\\) dla dowolnych\nnieujemnych \\(x\\) \\(y\\).\nAby uzasadnić (16.1) rozważmy dowolny wektor długości jeden \\(\\vec{v}=(v_1,\\ldots,v_d)^T\\).\nMamy\n\\[\n  \\langle \\mathbb{E}\\left[ \\vec{X}\\right],\\vec{v} \\rangle = \\sum_{j=1}^d \\mathbb{E} [X_j] \\cdot v_j\n   = \\mathbb{E}\\left[ \\langle \\vec{X},\\vec{v} \\rangle\\right] \\le\n   \\mathbb{E}\\left[ \\left\\|\\vec{X}\\right\\|\\left\\|\\vec{v}\\right\\|\\right] =\n   \\mathbb{E}\\left[\\left\\|\\vec{X}\\right\\|\\right].  \\]\nPrzyjmując \\(v = \\mathbb{E}\\vec{X}/ |\\vec{X}|\\) otrzymujemy (16.1).\nNiech teraz \\(\\) będzie dowolną macierzą \\(m\\times d\\). Przypomnijmy, że wówczas \\(j\\)-ta współrzędna\nwektora \\(\\vec{Y}\\) jest równa\n\\[\\begin{equation*}\n    \\left(\\vec{Y} \\right)_j = \\sum_{k=1}^dA_{,k}Y_k.\n  \\end{equation*}\\]\nMamy zatem\n\\[\\begin{equation*}\n    \\mathbb{E} \\left[\\left(\\vec{Y} \\right)_j\\right]\n    =\\mathbb{E} \\left[\\sum_{k=1}^d A_{j,k}Y_k\\right]\n    =\\sum_{k=1}^d A_{j,k}\\mathbb{E} \\left[Y_k\\right]\n    = \\left(\\mathbb{E}\\left[\\vec{Y} \\right]\\right)_j.\n  \\end{equation*}\\]\nOstatnia własność wynika wprost w liniowości wartości oczekiwanej zmiennych losowych.\n□","code":""},{"path":"parametry-wielowymiarowe.html","id":"macierz-kowariancji","chapter":"16 Parametry wielowymiarowe","heading":"16.2 Macierz kowariancji","text":"Definicja 16.2  Powiemy, że wektor losowy \\(\\vec{X}=(X_1, \\ldots, X_d)\\) jest całkowalny z kwadratem jeżeli\nwszystkie zmienne \\(X_1, \\ldots , X_d\\) są całkowalne z kwadratem.Rozumując analogicznie jak w ostatnim twierdzeniu łatwo pokazać, że wektor\n\\(\\vec{X}\\) jest całkowalny z kwadratem wtedy tylko wtedy, gdy zmienna losowa \\(\\|\\vec{X}\\|\\) jest\ncałkowalna z kwadratem.Definicja 16.3  Niech \\(\\vec{X}=(X_1,\\ldots,X_n)\\) będzie \\(n\\)-wymiarowym wektorem losowym całkowalnym z kwadratem.\nMacierz \\(Q^{\\vec{X}} = \\left( Q^{\\vec{X}}_{,j} \\right)_{,j\\leq n}\\) daną przez\n\\[\\begin{equation*}\n    Q^{\\vec{X}}_{,j} = \\mathrm{Cov}(X_i, X_j)\n\\end{equation*}\\]\nnazywamy macierzą kowariancji wektora \\(\\vec{X}\\).Macierz kowariancji jest wielowymiarowym uogólnieniem wariancji.\nMamy\n\\[\n  Q^{\\vec{X}} = \\left[\n  \\begin{array}{cccc}\n    {\\rm Cov}(X_1,X_1) & {\\rm Cov}(X_1,X_2) & \\ldots & {\\rm Cov}(X_1,X_n) \\\\\n    {\\rm Cov}(X_2,X_1) & \\cdots  &  &  \\\\\n    \\vdots &  & \\ddots &  \\\\\n    {\\rm Cov}(X_n,X_1) & \\ldots  &  & {\\rm Cov}(X_n,X_n)\n  \\end{array}\n  \\right]\n  \\]Jeżeli zmienne losowe \\(X_i\\) są nieskorelowane, \\(Q\\) jest macierzą diagonalną.Twierdzenie 16.2  Macierz kowariancji \\(Q^{\\vec{X}}\\) wektora losowego \\(\\vec{X}\\) jest symetryczna oraz\nnieujemnie określona (tzn. dla każdych \\(t_1,\\ldots, t_n\\), \\(\\sum t_it_j {Q^{\\vec{X}}_{ij}}\\ge 0\\)).\nDodatkowo, jeżeli \\(\\) jest macierzą \\(m \\times n\\), macierz kowariancji\nwektora losowego \\(\\vec{Y}=\\vec{X}\\) jest równa\n\\[\\begin{equation*}\n    Q^{\\vec{Y}} = Q^{\\vec{X}} = Q^{\\vec{X}} ^T.\n\\end{equation*}\\]\nWreszcie, jeżeli \\(\\vec{Z}=\\vec{X}+\\vec{}\\), dla ustalonego wektora \\(\\vec{} \\\\mathbb{R}^d\\), \n\\[\\begin{equation*}\nQ^{\\vec{Z}} = Q^{\\vec{X}+\\vec{}} = Q^{\\vec{X}}\n\\end{equation*}\\]Proof. Macierz jest symetryczna, bo \\({\\rm Cov}(X_i,X_j) = {\\rm Cov}(X_j,X_i)\\). dowodu drugiej części twierdzenia weźmy\ndowolny ciąg \\(t_1,\\ldots,t_n\\) zdefiniujmy \\(Y = \\sum_{j=1}^n t_j X_j\\). Wtedy\n\\[\\begin{multline*}\n  0\\le \\mathbb{V}ar [Y] = \\mathbb{E} \\left[ \\left(   \\sum_{j=1}^n t_j(X_j - \\mathbb{E} [X_j]) \\right)^2    \\right]\\\\\n  = \\sum_{,j=1}^n \\mathbb{E} \\big[ t_i(X_i - \\mathbb{E} X_i) t_j (X_j - \\mathbb{E}[ X_j])  \\big]\n  = \\sum_{,j=1}^n t_i t_j {\\rm Cov} (X_i, X_j).\n  \\end{multline*}\\]\nAby uzasadnić ostatni wzór zauważmy, że operację wartości oczekiwanej możemy w naturalny sposób\nrozszerzyć macierzy losowych.\nZauważmy też, że mnożąc przez siebie wektor pionowy długości \\(n\\) wektor poziomy długości \\(n\\)\notrzymujemy macierz \\(n\\times n\\). Dokładniej\n\\[\\begin{equation*}\n    \\left(\\vec{X} - \\mathbb{E}\\left[\\vec{X}\\right] \\right)\n    \\left(\\vec{X} - \\mathbb{E}\\left[\\vec{X}\\right] \\right)^T= \\\\\n  \\left[\n  \\begin{array}{cccc}\n    (X_1 - \\mathbb{E}[X_1]) (X_1-\\mathbb{E}[X_1]) & \\ldots & \\ldots\n& (X_1-\\mathbb{E}[X_1])(X_n-\\mathbb{E}[X_n]) \\\\\n    (X_2-\\mathbb{E}[X_2])(X_1-\\mathbb{E}[X_1]) & \\cdots  &  &  \\\\\n    \\vdots &  & \\ddots &  \\\\\n    (X_n-\\mathbb{E}[X_n])(X_1-\\mathbb{E}[X_1]) & \\ldots  &  & (X_n-\\mathbb{E}[X_n])(X_n-\\mathbb{E}[X_n])\n  \\end{array}\n  \\right]\n\\end{equation*}\\]\nCzyli\n\\[\\begin{equation*}\n    Q^{\\vec{X}} = \\mathbb{E} \\left[\\left(\\vec{X} - \\mathbb{E}\\left[\\vec{X}\\right] \\right)\n    \\left(\\vec{X} - \\mathbb{E}\\left[\\vec{X}\\right] \\right)^T\\right]\n\\end{equation*}\\]\nMamy zatem\\[\\begin{align*}\n    Q^{\\vec{X}} = &\n    \\mathbb{E} \\left[\\left(\\vec{X} - \\mathbb{E}\\left[\\vec{X}\\right] \\right)\n    \\left(\\vec{X} - \\mathbb{E}\\left[\\vec{X}\\right] \\right)^T\\right] \\\\\n    = & \\mathbb{E} \\left[\\left(\\vec{X} - \\mathbb{E}\\left[\\vec{X}\\right] \\right)\n    \\left(\\vec{X} - \\mathbb{E}\\left[\\vec{X}\\right] \\right)^T\\right]^T \\\\ = & Q^{\\vec{X}}^T.\n\\end{align*}\\]\nOstatnia własność wynika z niezmienniczości kowariancji na przesunięcia, tj.\n\\[\\begin{equation*}\n    \\mathrm{Cov}(Z_i, Z_j) =\n    \\mathrm{Cov}(X_i+a_i, X_j+a_j) =\n    \\mathrm{Cov}(X_i, X_j).\n\\end{equation*}\\]\n□Przykład 16.2  Załóżmy, że \\(n\\)-wymiarowy wektor \\(\\vec{X}=(X_1, X_2, \\ldots, X_n)\\)\nma rozkład \\(\\mathcal{N}(\\vec{0}, \\mathrm{Id})\\).\nWówczas\n\\[\\begin{equation*}\nf_{\\vec{X}}(\\vec{x}) = \\frac{1}{(2\\pi)^{n/2}} e^{-(x_1^2+\\ldots + x_n^2)/2} = \\prod_{j=1}^n \\frac{1}{\\sqrt{2\\pi}} e^{-x_j^2/2}.\n\\end{equation*}\\]\nSkoro każdy składnik produktu gęstość standardowego rozkładu normalnego, \\(X_1, \\ldots, X_n\\) są niezależne\nze standardowym rozkładem normalnym. W szczególności zmienne te są nieskorelowane o wariancji jeden, czyli\n\\[\\begin{equation*}\n    Q^{\\vec{X}} = \\mathrm{Id}.\n\\end{equation*}\\]\nNiech teraz \\(\\) będzie odwracalną macierzą, \\(\\vec{m}\\) ustalonym wektorem.\nRozważmy \\(\\vec{Y} = \\vec{X}+\\vec{m}\\). Wiemy, że wówczas \\(\\vec{Y}\\) ma rozkład \\(\\mathcal{N}(\\vec{m}, \\Sigma)\\), gdzie\n\\(\\Sigma=AA^T\\).\nMamy\n\\[\\begin{equation*}\n    Q^{\\vec{Y}} = Q^{\\vec{X}+\\vec{m}} = Q^{\\vec{X}} = Q^{\\vec{X}}^T = AA^T=\\Sigma.\n\\end{equation*}\\]","code":""},{"path":"nierówności.html","id":"nierówności","chapter":"17 Nierówności","heading":"17 Nierówności","text":"Przypomnijmy nierówność, która pojawiła się podczas wykładu z Miary Całki:Twierdzenie 17.1  (Nierówność H\"oldera) Jeżeli \\(\\mathbb{E}\\left[ |X|^p \\right] <\\infty\\) \\(\\mathbb{E}\\left[ |Y|^q\\right] <\\infty\\)\ndla \\(p,q>1\\) takich, że \\(1/p+1/q=1\\) , \n\\[\n\\mathbb{E} \\left[ |XY| \\right] \\le \\left( \\mathbb{E}\\left[ |X|^p \\right]\\right)^{1/p}\n\\left( \\mathbb{E}\\left[ |Y|^q\\right]\\right)^{1/q}.\n\\]","code":""},{"path":"nierówności.html","id":"nierówność-czebyszewa","chapter":"17 Nierówności","heading":"17.1 Nierówność Czebyszewa","text":"W rachunku prawdopodobieństwa częściej używana jest nierówność Czebyszewa, która w najogólniejszej\nwersji mówiTwierdzenie 17.2  (Nierówność Czebyszewa) Niech \\(X\\) będzie zmienną losową niech \\(f:[0,\\infty) \\mapsto [0,\\infty)\\) będzie niemalejącą funkcją taką, że\n\\(f(x)>0\\) dla każdego \\(x>0\\). Wtedy dla każdego \\(\\lambda > 0\\)\n\\[\n   \\mathbb{P}[|X|\\ge \\lambda] \\le \\frac{\\mathbb{E} [f(|X|)]}{f(\\lambda)}.\n   \\]Proof. Piszemy\n\\[\n\\mathbb{P}[|X|\\ge \\lambda] = \\mathbb{E}\\left[{\\bf 1}_{\\{|X|\\ge \\lambda\\}} \\right]\n\\le \\frac 1{f(\\lambda)} \\mathbb{E}\\left[ f(|X|){\\bf 1}_{\\{|X|>\\lambda\\}}  \\right]\n\\le \\frac{\\mathbb{E}[ f(|X|)]}{f(\\lambda)}.\n\\]\n□Zazwyczaj używa się powyższej nierówności w szczególnych przypadkach. Jeżeli zmienna losowa \\(X\\) jest nieujemna, \\(f(x)=x\\),\nnierówność przyjmuje klasyczną postać.Wniosek 17.1  Jeżeli \\(X\\geq0\\), dla każdej \\(\\lambda >0\\) mamy\n\\[\\begin{equation}\n    \\mathbb{P}[X \\geq \\lambda] \\leq \\frac{\\mathbb{E}[X]}{\\lambda}.\n    \\tag{17.1}\n\\end{equation}\\]Jeżeli natomiast zastąpimy \\(X\\) przez \\(X -\\mathbb{E}[X]\\) przyjmiemy \\(f(x)=x^2\\), w nierówności Czebyszewa\npojawia się wariancja \\(X\\).Wniosek 17.2  Jeżeli \\(X\\) jest całkowalna z kwadratem, dla każdej \\(\\lambda>0\\) mamy\n\\[\\begin{equation}\n      \\mathbb{P}\\left[ |X-\\mathbb{E} [X]|\\ge \\lambda  \\right] \\le \\frac{\\mathbb{V}ar[X]}{\\lambda^2}.\n      \\tag{17.2}\n\\end{equation}\\]Zobaczy teraz jak te dwie nierówności działają w praktyce.Przykład 17.1  Niech \\(n\\\\mathbb{N}\\). Rozważmy \\(n\\) rzutów symetryczną monetą. Niech \\(X\\) będzie liczbą otrzymanych orłów.\nChcemy oszacować prawdopodobieństwo, że \\(X\\) wynosi co najmniej \\(3n/4\\). Aby zastosować nierówność (17.1)\nprzypomnijmy, że \\(\\mathbb{E}[X]=n/2\\). Rzeczywiście, \\(X = \\sum_{j=1}^n X_j\\), gdzie \\(X_j\\\\{0,1\\}\\) \n\\(X_j=1\\) wtedy tylko wtedy,\ngdy w \\(j\\)-tym rzucie wypadł orzeł. Wówczas\n\\[\\begin{equation*}\n\\mathbb{P}[X_j=1] = \\mathbb{P}[X_j=1]=1/2.\n\\end{equation*}\\]\nStąd \\(\\mathbb{E}[X_j]=1/2\\) co za tym idzie \\(\\mathbb{E}[X]=n/2\\). Stosując nierówność (17.1) otrzymujemy\n\\[\\begin{equation*}\n    \\mathbb{P}[X> 3n/4] \\leq \\frac{4}{3n}\\mathbb{E}[X] = \\frac 23.\n\\end{equation*}\\]\nAby porównać z nierównością (17.2) przypomnijmy jeszcze, że \\(\\mathbb{V}ar[X]=n/4\\). Skoro\n\\[\\begin{equation*}\n    \\mathbb{V}ar[X_j]=\\mathbb{E}\\left[X_j^2\\right] - \\mathbb{E}[X_j]^2 = \\frac 12 -\\frac 14 = \\frac 14.\n\\end{equation*}\\]\nSkoro zmienne \\(\\{X_j\\}_j\\) są niezależne, \n\\[\\begin{equation*}\n    \\mathbb{V}ar[X] = \\sum_{j=1}^n \\mathbb{V}ar[X_j] = n/4.\n\\end{equation*}\\]\nWracając nierówności (17.2) otrzymujemy\n\\[\\begin{align*}\n    \\mathbb{P}[X>3n/4] & = \\mathbb{P}[X - \\mathbb{E}[X]> n/4] \\\\\n    & \\leq \\mathbb{P}[|X - \\mathbb{E}[X]|> n/4] \\leq \\frac{4}{n}.\n\\end{align*}\\]","code":""},{"path":"nierówności.html","id":"nierówność-chernoffa","chapter":"17 Nierówności","heading":"17.2 Nierówność Chernoffa","text":"Przy analizie ostatniego przykładu pojawia się naturalne pytanie o , czy można otrzymać lepsze szacowania.Definicja 17.1  Funkcją generująca momenty zmiennej losowej \\(X\\) nazywamy \\(M_X \\colon \\mathbb{R} \\(0, +\\infty]\\) zadaną przez\n\\[\\begin{equation*}\n    M_X(\\beta) = \\mathbb{E} \\left[ e^{\\beta X} \\right].\n\\end{equation*}\\]Zanim przejdziemy zastosowań \\(M_X\\) wyjaśnimy jej nazwę. Dla \\(n\\\\mathbb{N}\\), \\(n\\)-tym momentem zmiennej losowej \\(X\\)\nnazywamy \\(\\mathbb{E}\\left[X^n\\right]\\). Załóżmy, że dla pewnej dodatniej \\(\\beta_0\\), \\(M_X(\\beta_0)\\),\n\\(M_X(-\\beta_0)<\\infty\\).\nWówczas\n\\[\\begin{equation*}\n\\mathbb{E}\\left[e^{\\beta_0|X|} \\right] \\leq \\mathbb{E}\\left[e^{\\beta_0X} + e^{-\\beta_0X} \\right]<\\infty.\n\\end{equation*}\\]\nDla \\(h<\\beta_0/2\\) mamy\n\\[\\begin{equation*}\n    \\left| \\frac{e^{hX}-1}{h} \\right| \\leq |X| e^{hX} \\leq C e^{2h|X|} \\leq C e^{\\beta_0|X|}.\n\\end{equation*}\\]\nSkoro\n\\[\\begin{equation*}\n    \\frac{M_X(h) - 1}{h} =\n    \\mathbb{E}\\left[ \\frac{e^{hX}-1}{h} \\right]\n\\end{equation*}\\]\nkorzystając z twierdzenia o zbieżności ograniczonej\n\\[\\begin{equation*}\n    M_X'(0) = \\mathbb{E}[X].\n\\end{equation*}\\]\nRozumując analogicznie pokazujemy, że jeżeli \\(M_X(\\beta_0)\\), \\(M_X(-\\beta_0)<\\infty\\) dla każdego\n\\(n \\\\mathbb{N}\\),\n\\[\\begin{equation*}\n    M^{(n)}_X(0) = \\mathbb{E}\\left[X^n \\right].\n\\end{equation*}\\]\nStąd, dla \\(|\\beta|<\\beta_0\\),\n\\[\\begin{equation*}\n    M_X(\\beta) = \\sum_{j=0}^\\infty \\frac{\\beta^n}{j!}\\mathbb{E}\\left[X^n \\right].\n\\end{equation*}\\]Przykład 17.2  Rozważmy \\(X\\) o rozkładzie \\(\\mathrm{Exp}(\\alpha)\\) dla \\(\\alpha>0\\).\nDla \\(\\beta<\\alpha\\),\n\\[\\begin{equation*}\nM_X(\\beta) = \\int_0^\\infty \\alpha e^{-\\alpha x} e^{\\beta x} \\mathrm{d}x = \\frac{\\alpha}{\\alpha-\\beta}\n\\end{equation*}\\]\noraz \\(M_X(\\beta) = \\infty\\) dla \\(\\beta \\geq \\alpha\\). Wykres \\(M_X\\) dla \\(\\alpha=3\\) wygląda następująco.Widzimy, że dla \\(n\\\\mathbb{N}\\),\n\\[\\begin{equation*}\n    M_X^{(n)}(\\beta) =\\alpha n!(\\alpha-\\beta)^{-n-1}\n\\end{equation*}\\]\nco za tym idzie\n\\[\\begin{equation*}\n    \\mathbb{E}\\left[X^n \\right]= M_X^{(n)}(0) = \\frac{n!}{\\alpha^n}.\n\\end{equation*}\\]Wykorzystując funkcję tworzącą momenty możemy wyciągnąć jeszcze jeden wniosek z nierówności Czebyszewa.Wniosek 17.3  (Nierówność Chernoffa) Dla dowolnej \\(\\lambda \\geq 0\\),\n\\[\\begin{equation*}\n    \\mathbb{P}\\left[X \\geq \\lambda \\right] \\leq \\inf_{\\beta >0} e^{-\\lambda \\beta}M_X(\\beta).\n\\end{equation*}\\]Proof. Ustalmy dowolną \\(\\beta>0\\). Dla \\(f(x) = e^{x\\beta}\\) mamy\n\\[\\begin{equation*}\n\\mathbb{P}[X\\geq \\lambda ]\\leq e^{-\\lambda \\beta}\\mathbb{E}\\left[e^{\\beta X} \\right] = e^{-\\lambda \\beta}M_X(\\beta).\n\\end{equation*}\\]\nTeza wynika teraz przez rozważenie kresu dolnego dla \\(\\beta>0\\).\n□Przykład 17.3  Wróćmy jeszcze raz prawdopodobieństwa wyrzucenia więcej niż \\(3n/4\\) orłów przy \\(n\\) rzutach symetryczną monetą.\nPrzypomnijmy, że liczba otrzymanych orłów \\(X\\) zapisuje się jako \\(X = \\sum_{j=1}^nX_j\\), gdzie zmienne\n\\(\\{X_j\\}_j\\) są niezależne \n\\[\\begin{equation*}\n    \\mathbb{P}\\left[X_j=0 \\right] =\n    \\mathbb{P}\\left[X_j=1 \\right] = 1/2.\n\\end{equation*}\\]\nStąd\n\\[\\begin{equation*}\n    M_X(\\beta) = \\mathbb{E} \\left[e^{\\beta X_1}e^{\\beta X_2}\\cdots e^{\\beta X_n} \\right] = M_{X_1}(\\beta)^n.\n\\end{equation*}\\]\nMamy\n\\[\\begin{equation*}\n    M_{X_1}(\\beta) = \\frac{e^{\\beta} +1}{2}.\n\\end{equation*}\\]Różniczkując funkcję pod kresem sprawdzamy, że minimum jest przyjęte dla \\(\\beta = \\log(3)\\). Stąd\n\\[\\begin{equation*}\n    \\mathbb{P}[X \\geq 3n/4] \\leq  (0.88)^n\n\\end{equation*}\\]Przykład 17.4  Widzimy, że im mniejszy \\(\\epsilon\\) tym punkt realizujący minimum jest bliżej zera.\nZauważmy, że\n\\[\\begin{equation*}\n\\frac{\\mathrm{d}}{\\mathrm{d}\\beta}\n\\left.e^{-\\beta \\epsilon} \\frac{e^{\\beta/2} +e^{-\\beta/2}}{2} \\right|_{\\beta=0} = -\\epsilon<0.\n\\end{equation*}\\]\nStąd\n\\[\\begin{equation*}\n\\inf_{\\beta>0} e^{-\\beta \\epsilon} \\frac{e^{\\beta/2} +e^{-\\beta/2}}{2} =\\gamma_\\epsilon<1.\n\\end{equation*}\\]\nCzyli\n\\[\\begin{equation*}\n    \\mathbb{P}[X\\geq (1/2+\\epsilon)n]\\leq \\gamma_\\epsilon^n.\n\\end{equation*}\\]\nPrzez symetrię (zamieniając orły na reszki)\n\\[\\begin{equation*}\n    \\mathbb{P}[X\\leq (1/2-\\epsilon)n]\\leq \\gamma_\\epsilon^n.\n\\end{equation*}\\]\nStąd\n\\[\\begin{equation*}\n    \\mathbb{P}[|X/n -1/2|\\geq \\epsilon]\\leq 2\\gamma_\\epsilon^n.\n\\end{equation*}\\]","code":""},{"path":"lista-1-rozgrzewka.html","id":"lista-1-rozgrzewka","chapter":"Lista 1: Rozgrzewka","heading":"Lista 1: Rozgrzewka","text":"Zadania na ćwiczenia: 2025-02-24","code":""},{"path":"lista-1-rozgrzewka.html","id":"zadania-do-samodzielnego-rozwiązania","chapter":"Lista 1: Rozgrzewka","heading":"Zadania do samodzielnego rozwiązania","text":"Na szachownicy o wymiarach \\(n\\times n\\) umieszczono \\(8\\) nierozróżnialnych wież, w taki sposób aby żadne dwie się nie biły.\nNa ile sposobów można zrobić? Jak zmieni się wynik, gdy wieże będą rozróżnialne?\n\nOdpowiedź\n\nJeżeli wieże nie są rozróżnialne \\({n \\choose 8}^28!\\)\njeżeli wieże są rozróżnialne \\({n \\choose 8}^2(8!)^2\\).\nNa ile sposobów można rozmieścić \\(n\\) kul w \\(k\\) urnach jeżeli: () kule są rozróżnialne, (b) kule są nierozróżnialne.\n\nOdpowiedź\n\n\n\n\\(n^k\\) b \\({n+k-1 \\choose k-1}\\)\n\n\n\\(n^k\\) b \\({n+k-1 \\choose k-1}\\)\nIle jest liczb mniejszych od \\(1000\\) podzielnych przez jedną z liczb \\(3\\), \\(5\\), \\(7\\)?\n\nOdpowiedź\n\n\n\n\\(542\\)\n\n\n\\(542\\)\nNa płaszczyznie danych jest pięć punktów kratowych (o obu współrzędnych całkowitych).\nWykazać, ze środek jednego z odcinków łączacych te punkty również jest kratowy\n\nOdpowiedź\n\n\n\nNależy pokazać, że istnieje para punktów na płaszczyźnie, której współrzędne mają tę samą parzystość.\nTeza wynika z zastosowania zasady szufladkowej Dirichleta.\n\n\nNależy pokazać, że istnieje para punktów na płaszczyźnie, której współrzędne mają tę samą parzystość.\nTeza wynika z zastosowania zasady szufladkowej Dirichleta.\nOblicz prawdopodobieństwo zdarzenia, że w potasowanej talii \\(52\\) kart wszystkie cztery asy znajdują się koło siebie.\n\nOdpowiedź\n\n\n\n\\(4!/(52\\cdot 51\\cdot 50)\\)\n\n\n\\(4!/(52\\cdot 51\\cdot 50)\\)\n","code":""},{"path":"lista-1-rozgrzewka.html","id":"zadania-na-ćwiczenia","chapter":"Lista 1: Rozgrzewka","heading":"Zadania na ćwiczenia","text":"Na ile sposobów można ustawić \\(7\\) krzeseł białych \\(3\\) czerwone przy okrągłym stole?Na ile sposobów można ustawić \\(7\\) krzeseł białych \\(3\\) czerwone przy okrągłym stole?Zsumuj\n\\[\\begin{equation*}\n\\sum_{k=0}^m{n+k \\choose j}\n\\end{equation*}\\]\ngdzie \\(m,n,j \\\\mathbb{N}\\) są liczbami naturalnymi.\nZapisz \\(k^2\\) jako \\(a_2{k \\choose 2 }+ a_1{ k \\choose 1} + a_0{k \\choose 0}\\).\nWykorzystaj policzenia \\(\\sum_{k=0}^n{k^2}\\)Zsumuj\n\\[\\begin{equation*}\n\\sum_{k=0}^m{n+k \\choose j}\n\\end{equation*}\\]\ngdzie \\(m,n,j \\\\mathbb{N}\\) są liczbami naturalnymi.\nZapisz \\(k^2\\) jako \\(a_2{k \\choose 2 }+ a_1{ k \\choose 1} + a_0{k \\choose 0}\\).\nWykorzystaj policzenia \\(\\sum_{k=0}^n{k^2}\\)Wachlarzem rzędu \\(n\\) nazywamy graf o wierzchołkach \\(\\{0, 1, \\ldots , n\\}\\) z \\(2n-1\\) krawędziami zdefiniowanymi\nnastępująco: wierzchołek \\(0\\) połączony jest krawędzią z każdym z pozostałych wierzchołków wierzchołek \\(k\\) połączony\njest krawędzią z wierzchołkiem \\(k+1\\) dla każdego \\(1 \\leq k<n\\). Dla przykładu wachlarz rzędu \\(5\\) wygląda następująco\n\n\n\nPrzyjmując \\(S_0=0\\), ile wynosi \\(S_n\\) liczba drzew rozpinających na takim grafie?\nDrzewem rozpinającym nazywamy spójny podgraf bez cykli zawierający wszystkie wierzchołki\n\nWskazówka\n\n\n\nZnajdź zależność rekurencyjną na \\(S_n\\).\n\nWachlarzem rzędu \\(n\\) nazywamy graf o wierzchołkach \\(\\{0, 1, \\ldots , n\\}\\) z \\(2n-1\\) krawędziami zdefiniowanymi\nnastępująco: wierzchołek \\(0\\) połączony jest krawędzią z każdym z pozostałych wierzchołków wierzchołek \\(k\\) połączony\njest krawędzią z wierzchołkiem \\(k+1\\) dla każdego \\(1 \\leq k<n\\). Dla przykładu wachlarz rzędu \\(5\\) wygląda następującoPrzyjmując \\(S_0=0\\), ile wynosi \\(S_n\\) liczba drzew rozpinających na takim grafie?\nDrzewem rozpinającym nazywamy spójny podgraf bez cykli zawierający wszystkie wierzchołki\n\nWskazówka\n\nZnajdź zależność rekurencyjną na \\(S_n\\).\nW klasie jest \\(15\\) uczniów. Na każdej lekcji odpytywany jest losowo jeden z nich.\nOblicz prawdopodobieństwo, że podczas \\(16\\) lekcji zostanie przepytany każdy z nich.W klasie jest \\(15\\) uczniów. Na każdej lekcji odpytywany jest losowo jeden z nich.\nOblicz prawdopodobieństwo, że podczas \\(16\\) lekcji zostanie przepytany każdy z nich.W Totolotku losuje się \\(6\\) z \\(49\\) liczb. Jakie jest prawdopodobieństwo, że żadne dwie nie będą dwiema\nkolejnymi liczbami naturalnymi?W Totolotku losuje się \\(6\\) z \\(49\\) liczb. Jakie jest prawdopodobieństwo, że żadne dwie nie będą dwiema\nkolejnymi liczbami naturalnymi?Stefan Banach w każdej z kieszeni trzymał po pudełku zapałek.\nPoczątkowo każde z nich zawierało \\(n\\) zapałek. Za każdym razem kiedy Banach potrzebował zapałki sięgał losowo jednej\nz kieszeni wyciągał jedną zapałkę.\nOblicz prawdopodobieństwo, że w momencie gdy sięgnął po puste pudełko, w drugim pozostało jeszcze \\(k\\) zapałek.Stefan Banach w każdej z kieszeni trzymał po pudełku zapałek.\nPoczątkowo każde z nich zawierało \\(n\\) zapałek. Za każdym razem kiedy Banach potrzebował zapałki sięgał losowo jednej\nz kieszeni wyciągał jedną zapałkę.\nOblicz prawdopodobieństwo, że w momencie gdy sięgnął po puste pudełko, w drugim pozostało jeszcze \\(k\\) zapałek.Oblicz\n\\[\\begin{equation*}\n\\int_{-\\infty}^{\\infty} e^{-x^2} \\: \\mathrm{d}x.\n   \\end{equation*}\\]\n\nWskazówka\n\n\n\nPodnieś całkę kwadratu zastosuj współrzędne biegunowe.\n\nOblicz\n\\[\\begin{equation*}\n\\int_{-\\infty}^{\\infty} e^{-x^2} \\: \\mathrm{d}x.\n   \\end{equation*}\\]\n\nWskazówka\n\nPodnieś całkę kwadratu zastosuj współrzędne biegunowe.\nGrupa składająca się z \\(2n\\) pań \\(2n\\) panów została podzielona na dwie równoliczne grupy.\nZnaleźć prawdopodobieństwo, że każda z tych grup składa się z takiej samej liczby pań panów.\nPrzybliżyć prawdopodobieństwo za pomocą wzoru Stirlinga.Grupa składająca się z \\(2n\\) pań \\(2n\\) panów została podzielona na dwie równoliczne grupy.\nZnaleźć prawdopodobieństwo, że każda z tych grup składa się z takiej samej liczby pań panów.\nPrzybliżyć prawdopodobieństwo za pomocą wzoru Stirlinga.","code":""},{"path":"lista-1-rozgrzewka.html","id":"zadania-dodatkowe","chapter":"Lista 1: Rozgrzewka","heading":"Zadania dodatkowe","text":"Pewien sułtan więził 100 osób. Pewnego dnia postanowił ich zgładzić.\nJako, że był znany ze swego miłosierdzia dał im ostatnią szansę.\nPostawił przed nimi następujące zadanie. Każdemu więźniowi przyporządkował liczbę.\nNastępnie w pokoju obok umieścił w rzędzie kolejno \\(100\\) pudełek każdego z nich włożył losową liczbę od \\(1\\) \\(100\\)\n(w każdym pudełku inną). Wieżniowie po kolei, pojedynczo, wchodzą pokoju z pudełkami.\nMogą otworzyć \\(50\\) pudełek, aby znaleźć swój numer, ale pokój muszą pozostawić dokładnie w takim samym stanie w jakim\ngo zastali. Następnie opuszczają pokój wychodząc innym wyjściem nie mają możliwości skontaktowania się z\npozostałymi osobami. Więźniowie zostaną ocaleni, jeżeli z nich każdy znajdzie swój numer.\nJeżeli każdy z nich otwiera losowe 50 pudełek, szanse ich przeżycia wynoszą\n\\(2^{-100} \\approx 7,8*10^{-31}\\). Czy mają oni lepszą strategię?","code":""},{"path":"lista-2-aksjomaty-rachunku-prawdopodobieństwa.html","id":"lista-2-aksjomaty-rachunku-prawdopodobieństwa","chapter":"Lista 2: Aksjomaty rachunku prawdopodobieństwa","heading":"Lista 2: Aksjomaty rachunku prawdopodobieństwa","text":"Zadania na ćwiczenia: 2025-03-03","code":""},{"path":"lista-2-aksjomaty-rachunku-prawdopodobieństwa.html","id":"zadania-do-samodzielnego-rozwiązania-1","chapter":"Lista 2: Aksjomaty rachunku prawdopodobieństwa","heading":"Zadania do samodzielnego rozwiązania","text":"Ze zbioru \\([100]=\\{1,  \\ldots  100 \\}\\) wylosowano ze zwracaniem dwie liczby \\(L\\) \\(M\\).\nZdefiniuj odpowiednią przestrzeń probabilistyczną.\nOblicz prawdopodobieństwo, że średnia arytmetyczna \\(L\\) \\(M\\) jest ściśle mniejsza niż \\(30\\).\n\nOdpowiedź\n\n\n\n\\(\\Omega = [100]^2\\), \\(\\mathcal{F}=2^\\Omega\\), \\(\\mathbb{P}[\\{(l,m)\\} ]=100^{-2}\\).\nSzukane prawdopodobieństwo \\(58\\cdot 59/20000\\).\n\n\n\\(\\Omega = [100]^2\\), \\(\\mathcal{F}=2^\\Omega\\), \\(\\mathbb{P}[\\{(l,m)\\} ]=100^{-2}\\).\nSzukane prawdopodobieństwo \\(58\\cdot 59/20000\\).\nWybrano losowy punkt \\((x,y)\\) z kwadratu \\([0,1]\\times [0,1]\\). Zdefiniuj odpowiednią przestrzeń probabilistyczną. Oblicz prawdopodobieństwo, że\n\\(x\\) jest liczbą wymierną;\nobie liczby \\(x\\) \\(y\\) są niewymierne;\nspełniona jest nierówność \\(x^2+y^2 < 1\\);\nspełniona jest równość \\(x^2+y^2 = 1\\).\n\nOdpowiedź\n\n\n\n\\(\\Omega = [0,1]^2\\), \\(\\mathcal{F} = \\mathcal{B}([0,1]^2)\\), \\(\\mathbb{P}\\) jest dwuwymiarową\nmiarą Lebesgue’. ) 0; b) 1; c) \\(\\pi/4\\); d) 0\n\n\n\\(x\\) jest liczbą wymierną;obie liczby \\(x\\) \\(y\\) są niewymierne;spełniona jest nierówność \\(x^2+y^2 < 1\\);spełniona jest równość \\(x^2+y^2 = 1\\).\n\nOdpowiedź\n\n\n\n\\(\\Omega = [0,1]^2\\), \\(\\mathcal{F} = \\mathcal{B}([0,1]^2)\\), \\(\\mathbb{P}\\) jest dwuwymiarową\nmiarą Lebesgue’. ) 0; b) 1; c) \\(\\pi/4\\); d) 0\n\n\n\\(\\Omega = [0,1]^2\\), \\(\\mathcal{F} = \\mathcal{B}([0,1]^2)\\), \\(\\mathbb{P}\\) jest dwuwymiarową\nmiarą Lebesgue’. ) 0; b) 1; c) \\(\\pi/4\\); d) 0\nW kwadracie \\([0,1] \\times [0,1]\\) wybrano losowo dwa punkty \\(\\) \\(B\\).\nZdefiniuj odpowiednią przestrzeń probabilistyczną. Oblicz prawdopodobieństwo, że\nodcinek \\(AB\\) przecina przekątną łączącą wierzchołki \\((0,0)\\) \\((1,1)\\);\nodległość punktu \\(\\) od \\((1,1)\\) jest mniejsza niż 1,\nodległość punktu \\(B\\) od \\((1,1)\\) jest większa niż 1;\noba punkty leżą pod parabolą \\(y = - x(x-1)\\).\n\nOdpowiedź\n\n\n\n\\(\\Omega = [0,1]^4\\), \\(\\mathcal{F} = \\mathcal{B}([0,1]^4)\\), \\(\\mathbb{P}\\) jest czterowymiarową\nmiarą Lebesgue’. . \\(1/2\\) b. \\(\\pi/4\\) c. \\(1-\\pi/4\\) d. \\(1/36\\).\n\n\nodcinek \\(AB\\) przecina przekątną łączącą wierzchołki \\((0,0)\\) \\((1,1)\\);odległość punktu \\(\\) od \\((1,1)\\) jest mniejsza niż 1,odległość punktu \\(B\\) od \\((1,1)\\) jest większa niż 1;oba punkty leżą pod parabolą \\(y = - x(x-1)\\).\n\nOdpowiedź\n\n\n\n\\(\\Omega = [0,1]^4\\), \\(\\mathcal{F} = \\mathcal{B}([0,1]^4)\\), \\(\\mathbb{P}\\) jest czterowymiarową\nmiarą Lebesgue’. . \\(1/2\\) b. \\(\\pi/4\\) c. \\(1-\\pi/4\\) d. \\(1/36\\).\n\n\n\\(\\Omega = [0,1]^4\\), \\(\\mathcal{F} = \\mathcal{B}([0,1]^4)\\), \\(\\mathbb{P}\\) jest czterowymiarową\nmiarą Lebesgue’. . \\(1/2\\) b. \\(\\pi/4\\) c. \\(1-\\pi/4\\) d. \\(1/36\\).\nNiech \\(\\cup B \\cup C = \\Omega\\), \\(\\mathbb{P}[B] = 2 \\mathbb{P}[]\\),\n\\(\\mathbb{P}[C] = 3 \\mathbb{P}[]\\), \\(\\mathbb{P}[\\cap B] = \\mathbb{P}[\\cap C] =\n\\mathbb{P}[B \\cap C]\\). Pokaż, że \\(1/6 \\leq \\mathbb{P}[] \\leq 1/4\\),\nprzy czym oba ograniczenia są osiągalne.\n\nOdpowiedź\n\n\n\nMamy \\(1 = \\mathbb{P}[\\Omega] \\leq \\mathbb{P}[]\\) \\(+\\mathbb{P}[B]+\\mathbb{P}[C] = 6 \\mathbb{P}[]\\).\nStąd \\(\\mathbb{P}[]\\geq 1/6\\). ograniczenie jest osiągnięte, jeżeli zbiory \\(\\), \\(B\\) \\(C\\) są\nrozłączne. Z drugiej strony\n\\(1 = \\mathbb{P}[\\Omega] = \\mathbb{P}[C] + \\mathbb{P}[B\\setminus C]\\)\n\\(= \\mathbb{P}[C] +\\mathbb{P}[B] - \\mathbb{P}[B\\cap C] \\geq 4\\mathbb{P}[]\\).\nStąd \\(\\mathbb{P}[]\\leq 1/4\\).\nOgraniczenie jest osiągnięte kiedy \\(= B \\cap C\\).\n\n\nMamy \\(1 = \\mathbb{P}[\\Omega] \\leq \\mathbb{P}[]\\) \\(+\\mathbb{P}[B]+\\mathbb{P}[C] = 6 \\mathbb{P}[]\\).\nStąd \\(\\mathbb{P}[]\\geq 1/6\\). ograniczenie jest osiągnięte, jeżeli zbiory \\(\\), \\(B\\) \\(C\\) są\nrozłączne. Z drugiej strony\n\\(1 = \\mathbb{P}[\\Omega] = \\mathbb{P}[C] + \\mathbb{P}[B\\setminus C]\\)\n\\(= \\mathbb{P}[C] +\\mathbb{P}[B] - \\mathbb{P}[B\\cap C] \\geq 4\\mathbb{P}[]\\).\nStąd \\(\\mathbb{P}[]\\leq 1/4\\).\nOgraniczenie jest osiągnięte kiedy \\(= B \\cap C\\).\nPokaż, że jeżeli \\(\\{A_i\\}_{\\ge 1}\\) jest rodziną zdarzeń takich, że\n\\(\\mathbb{P}[A_i]=1\\) dla \\(\\ge 1\\), \\(\\mathbb{P}\\left[\\bigcap_{=1}^{\\infty} A_i\\right]=1\\).\n\nOdpowiedź\n\n\n\nJest wniosek z zadania 6.\n\n\nJest wniosek z zadania 6.\n","code":""},{"path":"lista-2-aksjomaty-rachunku-prawdopodobieństwa.html","id":"zadania-na-ćwiczenia-1","chapter":"Lista 2: Aksjomaty rachunku prawdopodobieństwa","heading":"Zadania na ćwiczenia","text":"Udowodnij nierówność Boole’\n\\[\n\\mathbb{P}\\left[\\bigcup_{=1}^n A_i\\right] \\; \\leq \\; \\sum_{=1}^n \\mathbb{P}[A_i].\n\\]\nWywnioskuj, że\n\\[\n\\mathbb{P}\\left[\\bigcap_{=1}^n A_i\\right] \\; \\geq \\; 1-  \\sum_{=1}^n \\mathbb{P}\\left[A_i^c\\right].\n\\]Udowodnij nierówność Boole’\n\\[\n\\mathbb{P}\\left[\\bigcup_{=1}^n A_i\\right] \\; \\leq \\; \\sum_{=1}^n \\mathbb{P}[A_i].\n\\]\nWywnioskuj, że\n\\[\n\\mathbb{P}\\left[\\bigcap_{=1}^n A_i\\right] \\; \\geq \\; 1-  \\sum_{=1}^n \\mathbb{P}\\left[A_i^c\\right].\n\\]Udowodnij nierówność Bonferroniego\n\\[\\begin{equation*}\n     \\mathbb{P}\\left[\\bigcup_{=1}^n A_i\\right] \\; \\geq \\;\n     \\sum_{=1}^n \\mathbb{P}[A_i] - \\sum_{1 \\leq < j \\leq n} \\mathbb{P}[A_i \\cap A_j]\n\\end{equation*}\\]Udowodnij nierówność Bonferroniego\n\\[\\begin{equation*}\n     \\mathbb{P}\\left[\\bigcup_{=1}^n A_i\\right] \\; \\geq \\;\n     \\sum_{=1}^n \\mathbb{P}[A_i] - \\sum_{1 \\leq < j \\leq n} \\mathbb{P}[A_i \\cap A_j]\n\\end{equation*}\\]Udowodnij wzór włączeń wyłączeń\n\\[\\begin{multline*}\n  \\mathbb{P}[A_1\\cup A_2\\cup \\ldots \\cup A_n] = \\sum_{=1}^n\\mathbb{P}[A_i]+ \\\\\n-\\sum_{<j}\\mathbb{P}[A_i\\cap A_j] + \\sum_{<j<k}\\mathbb{P}[A_i\\cap A_j\\cap A_k]\n  \\\\ + \\ldots +(-1)^{n+1} \\mathbb{P}[A_1\\cap A_2\\cap \\ldots \\cap A_n]\n\\end{multline*}\\]Udowodnij wzór włączeń wyłączeń\n\\[\\begin{multline*}\n  \\mathbb{P}[A_1\\cup A_2\\cup \\ldots \\cup A_n] = \\sum_{=1}^n\\mathbb{P}[A_i]+ \\\\\n-\\sum_{<j}\\mathbb{P}[A_i\\cap A_j] + \\sum_{<j<k}\\mathbb{P}[A_i\\cap A_j\\cap A_k]\n  \\\\ + \\ldots +(-1)^{n+1} \\mathbb{P}[A_1\\cap A_2\\cap \\ldots \\cap A_n]\n\\end{multline*}\\]Dziesięć małżeństw usiadło losowo przy okrągłym stole.\nOblicz prawdopodobieństwo, że żaden mąż nie siedzi przy swojej żonie.Dziesięć małżeństw usiadło losowo przy okrągłym stole.\nOblicz prawdopodobieństwo, że żaden mąż nie siedzi przy swojej żonie.Podczas imprezy mikołajkowej wszystkie \\(n\\) prezentów pozbawiono karteczek z imieniem adresata\nlosowo rozdano uczestnikom. Niech \\(p_n\\) oznacza prawdopodobieństwo,\nże dokładnie jedna osoba dostanie własny prezent.\nOblicz \\(p_n\\) oraz \\(\\lim_{n \\\\infty} p_n\\).Podczas imprezy mikołajkowej wszystkie \\(n\\) prezentów pozbawiono karteczek z imieniem adresata\nlosowo rozdano uczestnikom. Niech \\(p_n\\) oznacza prawdopodobieństwo,\nże dokładnie jedna osoba dostanie własny prezent.\nOblicz \\(p_n\\) oraz \\(\\lim_{n \\\\infty} p_n\\).Rzucamy symetryczną monetą chwili otrzymania orła.\nZdefiniuj odpowiednią przestrzeń probabilistyczną.\nJaka jest szansa, że liczba rzutów będzie parzysta?\npodzielna przez \\(3\\)? podzielna przez \\(m\\)?Rzucamy symetryczną monetą chwili otrzymania orła.\nZdefiniuj odpowiednią przestrzeń probabilistyczną.\nJaka jest szansa, że liczba rzutów będzie parzysta?\npodzielna przez \\(3\\)? podzielna przez \\(m\\)?Z przedziału \\([0,1]\\) wybrano losowo dwa punkty, które podzieliły go na trzy odcinki.\nObliczyć prawdopodobieństwo, że z tych odcinków można skonstruować trójkąt.Z przedziału \\([0,1]\\) wybrano losowo dwa punkty, które podzieliły go na trzy odcinki.\nObliczyć prawdopodobieństwo, że z tych odcinków można skonstruować trójkąt.Na nieskończoną szachownicę o boku \\(\\) rzucamy monetę o średnicy \\(2r < \\).\nZnaleźć prawdopodobieństwo, że\nmoneta znajdzie się całkowicie wnętrzu jednego z pól;\nmoneta przetnie się z co najwyżej jednym bokiem pola na szachownicy.\nNa nieskończoną szachownicę o boku \\(\\) rzucamy monetę o średnicy \\(2r < \\).\nZnaleźć prawdopodobieństwo, żemoneta znajdzie się całkowicie wnętrzu jednego z pól;moneta przetnie się z co najwyżej jednym bokiem pola na szachownicy.Igłę o długości \\(l\\) rzucono na podłogę z desek o szerokości \\(\\geq l\\).\nZnajdź prawdopodobieństwo, że igła przetnie krawędź deski.Igłę o długości \\(l\\) rzucono na podłogę z desek o szerokości \\(\\geq l\\).\nZnajdź prawdopodobieństwo, że igła przetnie krawędź deski.","code":""},{"path":"lista-2-aksjomaty-rachunku-prawdopodobieństwa.html","id":"zadania-dodatkowe-1","chapter":"Lista 2: Aksjomaty rachunku prawdopodobieństwa","heading":"Zadania dodatkowe","text":"Niech \\((\\Omega, \\mathcal{F})\\) będzie przestrzenią mierzalną.\nUzasadnij, że \\(\\sigma\\)-ciało \\(\\mathcal{F}\\) nie może być nieskończoną przeliczalną rodziną zbiorów.Niech \\((\\Omega, \\mathcal{F})\\) będzie przestrzenią mierzalną.\nUzasadnij, że \\(\\sigma\\)-ciało \\(\\mathcal{F}\\) nie może być nieskończoną przeliczalną rodziną zbiorów.Oznaczmy przez \\(\\mathcal B_0\\) ciało składające się ze skończonych sum\nrozłącznych przedziałów \\((,b]\\) zawartych w odcinku \\((0,1]\\). Określmy na\n\\(\\mathcal B_0\\) funkcję \\(\\mathbb{P}\\) taką, że \\(\\mathbb{P}() = 1\\) lub \\(0\\) w zależności od tego,\nczy zbiór \\(\\) zawiera przedział postaci \\((1/2,1/2+\\varepsilon]\\) dla pewnego \\(\\varepsilon>0\\),\nczy też nie. Pokaż, że \\(\\mathbb{P}\\) jest miarą addytywną, ale nie przeliczalnie addytywną.Oznaczmy przez \\(\\mathcal B_0\\) ciało składające się ze skończonych sum\nrozłącznych przedziałów \\((,b]\\) zawartych w odcinku \\((0,1]\\). Określmy na\n\\(\\mathcal B_0\\) funkcję \\(\\mathbb{P}\\) taką, że \\(\\mathbb{P}() = 1\\) lub \\(0\\) w zależności od tego,\nczy zbiór \\(\\) zawiera przedział postaci \\((1/2,1/2+\\varepsilon]\\) dla pewnego \\(\\varepsilon>0\\),\nczy też nie. Pokaż, że \\(\\mathbb{P}\\) jest miarą addytywną, ale nie przeliczalnie addytywną.","code":""},{"path":"lista-3-prawdopodobieństwo-warunkowe.html","id":"lista-3-prawdopodobieństwo-warunkowe","chapter":"Lista 3: Prawdopodobieństwo warunkowe","heading":"Lista 3: Prawdopodobieństwo warunkowe","text":"Zadania na ćwiczenia: 2025-03-10Lista zadań w formacie pdf","code":""},{"path":"lista-3-prawdopodobieństwo-warunkowe.html","id":"zadania-do-samodzielnego-rozwiązania-2","chapter":"Lista 3: Prawdopodobieństwo warunkowe","heading":"Zadania do samodzielnego rozwiązania","text":"W urnie znajduje się \\(20\\) kul białych \\(5\\) czarnych.\nLosujemy po jednej kuli aż momentu, gdy wylosujemy czarną kulę.\nJakie jest prawdopodobieństwo, że wykonamy \\(k\\) losowań,\njeżeli\nlosujemy bez zwracania\nlosujemy ze zwracaniem?\n\nOdpowiedź\n\n\n\nbez zwracania:\n\\[\n\\prod_{j=0}^{k-2} \\frac{20-j}{25-j} \\cdot \\frac{5}{25-k+1}\n\\]\nze zwracaniem: \\((4/5)^{k-1}/5\\)\n\n\nlosujemy bez zwracanialosujemy ze zwracaniem?\n\nOdpowiedź\n\n\n\nbez zwracania:\n\\[\n\\prod_{j=0}^{k-2} \\frac{20-j}{25-j} \\cdot \\frac{5}{25-k+1}\n\\]\nze zwracaniem: \\((4/5)^{k-1}/5\\)\n\n\nbez zwracania:\n\\[\n\\prod_{j=0}^{k-2} \\frac{20-j}{25-j} \\cdot \\frac{5}{25-k+1}\n\\]\nze zwracaniem: \\((4/5)^{k-1}/5\\)\nDwoje graczy, Maciek Dawid, dostało po \\(13\\) kart z \\(52\\).\nMaciek zobaczył przypadkowo u Dawida\nasa pik,\njakiegoś asa czarnego koloru,\njakiegoś asa.\nObliczyć prawdopodobieństwo, że Maciek nie ma asa.\n\nOdpowiedź\n\n\n\n,b,c \\({48 \\choose 13}/{51 \\choose 13}\\)\n\n\nasa pik,jakiegoś asa czarnego koloru,jakiegoś asa.\nObliczyć prawdopodobieństwo, że Maciek nie ma asa.\n\nOdpowiedź\n\n\n\n,b,c \\({48 \\choose 13}/{51 \\choose 13}\\)\n\n\n,b,c \\({48 \\choose 13}/{51 \\choose 13}\\)\nTrzech strzelców oddało niezależnie po jednym strzale tego samego celu.\nPrawdopodobieństwa trafień wynoszą odpowiednio \\(p_1\\), \\(p_2\\), \\(p_3\\).\nWyznacz prawdopodobieństwo, że pierszy strzelec trafił, jeżeli cel został trafiony\ndokładnie jednym pociskiem\ndokładnie dwoma pociskami;\ntrzema pociskami.\n\nOdpowiedź\n\n\n\n\\[\\begin{align*}\n& \\frac{p_1(1-p_2)(1-p_3)}{ p_1(1-p_2)(1-p_3) + p_2(1-p_1)(1-p_3)+p_3(1-p_1)(1-p_2)}\\\\\nb & \\frac{p_1(1-p_2)p_3+(1-p_1)p_2p_3}{p_1p_2(1-p_3)+p_1(1-p_2)p_3 +(1-p_1)p_2p_3} \\\\ c& 1\n\\end{align*}\\]\n\n\ndokładnie jednym pociskiemdokładnie dwoma pociskami;trzema pociskami.\n\nOdpowiedź\n\n\n\n\\[\\begin{align*}\n& \\frac{p_1(1-p_2)(1-p_3)}{ p_1(1-p_2)(1-p_3) + p_2(1-p_1)(1-p_3)+p_3(1-p_1)(1-p_2)}\\\\\nb & \\frac{p_1(1-p_2)p_3+(1-p_1)p_2p_3}{p_1p_2(1-p_3)+p_1(1-p_2)p_3 +(1-p_1)p_2p_3} \\\\ c& 1\n\\end{align*}\\]\n\n\n\\[\\begin{align*}\n& \\frac{p_1(1-p_2)(1-p_3)}{ p_1(1-p_2)(1-p_3) + p_2(1-p_1)(1-p_3)+p_3(1-p_1)(1-p_2)}\\\\\nb & \\frac{p_1(1-p_2)p_3+(1-p_1)p_2p_3}{p_1p_2(1-p_3)+p_1(1-p_2)p_3 +(1-p_1)p_2p_3} \\\\ c& 1\n\\end{align*}\\]\nPodaj przykłady zdarzeń takich, że\n\\(\\mathbb{P}[|B] < \\mathbb{P}[]\\),\n\\(\\mathbb{P}[|B] = \\mathbb{P}[]\\)\n\\(\\mathbb{P}[|B] > \\mathbb{P}[]\\).\n\nOdpowiedź\n\n\n\nRozważmy\n\\(\\mathbb{P}[\\cap B]=\\alpha\\), \\(\\mathbb{P}[B \\cap ^c]=\\beta\\),\n\\(\\mathbb{P}[\\setminus B]=\\gamma\\).\nSzukamy takiego doboru liczb \\(\\alpha, \\beta \\gamma\\)\ntakich, że \\(\\alpha+\\beta\\), \\(\\alpha+\\gamma \\leq 1\\) oraz\n\\(\\alpha /(\\alpha+\\beta)\\) było mniejsz/większe/równe \\(\\alpha+\\gamma\\).\n\n\n\\(\\mathbb{P}[|B] < \\mathbb{P}[]\\),\\(\\mathbb{P}[|B] = \\mathbb{P}[]\\)\\(\\mathbb{P}[|B] > \\mathbb{P}[]\\).\n\nOdpowiedź\n\n\n\nRozważmy\n\\(\\mathbb{P}[\\cap B]=\\alpha\\), \\(\\mathbb{P}[B \\cap ^c]=\\beta\\),\n\\(\\mathbb{P}[\\setminus B]=\\gamma\\).\nSzukamy takiego doboru liczb \\(\\alpha, \\beta \\gamma\\)\ntakich, że \\(\\alpha+\\beta\\), \\(\\alpha+\\gamma \\leq 1\\) oraz\n\\(\\alpha /(\\alpha+\\beta)\\) było mniejsz/większe/równe \\(\\alpha+\\gamma\\).\n\n\nRozważmy\n\\(\\mathbb{P}[\\cap B]=\\alpha\\), \\(\\mathbb{P}[B \\cap ^c]=\\beta\\),\n\\(\\mathbb{P}[\\setminus B]=\\gamma\\).\nSzukamy takiego doboru liczb \\(\\alpha, \\beta \\gamma\\)\ntakich, że \\(\\alpha+\\beta\\), \\(\\alpha+\\gamma \\leq 1\\) oraz\n\\(\\alpha /(\\alpha+\\beta)\\) było mniejsz/większe/równe \\(\\alpha+\\gamma\\).\n","code":""},{"path":"lista-3-prawdopodobieństwo-warunkowe.html","id":"zadania-na-ćwiczenia-2","chapter":"Lista 3: Prawdopodobieństwo warunkowe","heading":"Zadania na ćwiczenia","text":"W populacji jest \\(15\\%\\) dyslektyków.\nJeżeli w teście diagnostycznym uczeń popełni \\(6\\) lub więcej błędów, zostaje uznany za dyslektyka.\nKażdy dyslektyk na pewno popełni co najmniej \\(6\\) błędów.\nRównież nie-dyslektyk może popełnić co najmniej \\(6\\) błędów dzieje się z prawdopodobieństwem \\(0,1\\).\nJasiu popełnił \\(6\\) błędów.\nOblicz prawdopodobieństwo, że jest dyslektykiem.\nJakie jest prawdopodobieństwo, że w kolejnym teście też popełni co najmniej \\(6\\) błędów?W populacji jest \\(15\\%\\) dyslektyków.\nJeżeli w teście diagnostycznym uczeń popełni \\(6\\) lub więcej błędów, zostaje uznany za dyslektyka.\nKażdy dyslektyk na pewno popełni co najmniej \\(6\\) błędów.\nRównież nie-dyslektyk może popełnić co najmniej \\(6\\) błędów dzieje się z prawdopodobieństwem \\(0,1\\).\nJasiu popełnił \\(6\\) błędów.\nOblicz prawdopodobieństwo, że jest dyslektykiem.\nJakie jest prawdopodobieństwo, że w kolejnym teście też popełni co najmniej \\(6\\) błędów?Dawid Maciek grają w pokera. Maciek ma silną rękę zaczął od \\(5\\) dolarów.\nPrawdopodobieństwo, że Dawid ma silniejsze karty wynosi \\(0,1\\).\nGdyby Dawid miał mocniejsze/ słabsze karty podbiłby stawkę z prawdopodobieństwem \\(0,9 / 0,1\\).\nDawid podbił stawkę. Jakie jest prawdopodobieństwo, że ma lepsze karty?Dawid Maciek grają w pokera. Maciek ma silną rękę zaczął od \\(5\\) dolarów.\nPrawdopodobieństwo, że Dawid ma silniejsze karty wynosi \\(0,1\\).\nGdyby Dawid miał mocniejsze/ słabsze karty podbiłby stawkę z prawdopodobieństwem \\(0,9 / 0,1\\).\nDawid podbił stawkę. Jakie jest prawdopodobieństwo, że ma lepsze karty?W pewnej fabryce telewizorów każdy z aparatów może być wadliwy z prawdopodobieństwem \\(p\\).\nW fabryce są trzy stanowiska kontroli wyprodukowany telewizor trafia na każde ze stanowisk\nz jednakowym prawdopodobieństwem. \\(\\)-te stanowisko wykrywa wadliwy telewizor z\nprawdopodobieństwem \\(p_i\\) (\\(= 1, 2, 3\\)).\nTelewizory nie odrzucone w fabryce trafiają hurtowni tam poddawane są dodatkowej kontroli,\nktóra wykrywa wadliwy telewizor z prawdopodobieństwem \\(p_0\\).\nObliczyć prawdopodobieństwo tego, że dany nowowyprodukowany telewizor znajdzie się w sprzedaży (tzn. przejdzie przez obie kontrole).\nPrzypuśćmy, że telewizor jest już w sklepie. Jakie jest prawdopodobieństwo, że jest wadliwy?\nW pewnej fabryce telewizorów każdy z aparatów może być wadliwy z prawdopodobieństwem \\(p\\).\nW fabryce są trzy stanowiska kontroli wyprodukowany telewizor trafia na każde ze stanowisk\nz jednakowym prawdopodobieństwem. \\(\\)-te stanowisko wykrywa wadliwy telewizor z\nprawdopodobieństwem \\(p_i\\) (\\(= 1, 2, 3\\)).\nTelewizory nie odrzucone w fabryce trafiają hurtowni tam poddawane są dodatkowej kontroli,\nktóra wykrywa wadliwy telewizor z prawdopodobieństwem \\(p_0\\).Obliczyć prawdopodobieństwo tego, że dany nowowyprodukowany telewizor znajdzie się w sprzedaży (tzn. przejdzie przez obie kontrole).Przypuśćmy, że telewizor jest już w sklepie. Jakie jest prawdopodobieństwo, że jest wadliwy?Mamy dwie urny \\(50\\) kul. Połowa z kul jest biała, połowa czarna.\nJak rozłożyć kule urn, aby zmaksymalizować prawdopodobieństwo zdarzenia,\nże losowo wybrana kula z losowej urny jest biała (najpierw losujemy urnę,\npotem z wybranej urny losujemy kulę)?Mamy dwie urny \\(50\\) kul. Połowa z kul jest biała, połowa czarna.\nJak rozłożyć kule urn, aby zmaksymalizować prawdopodobieństwo zdarzenia,\nże losowo wybrana kula z losowej urny jest biała (najpierw losujemy urnę,\npotem z wybranej urny losujemy kulę)?Rzucamy trzema sześciennymi kostkami gry.\nNastępnie rzucamy ponownie tymi kostkami, na których nie wypadły “jedynki”.\nObliczyć prawdopodobieństwo, że na wszystkich trzech kostkach będą “jedynki”.Rzucamy trzema sześciennymi kostkami gry.\nNastępnie rzucamy ponownie tymi kostkami, na których nie wypadły “jedynki”.\nObliczyć prawdopodobieństwo, że na wszystkich trzech kostkach będą “jedynki”.Przypuśćmy, że \\(1/20\\) wszystkich kości gry jest sfałszowana zawsze wypada na nich szóstka. Wybieramy losowo trzy kostki rzucamy nimi. Oblicz\nprawdopodobieństwo wyrzucenia w sumie 18 oczek;\nprawdopodobieństwo, że co najmniej jedna kostka była sfałszowana, jeżeli wyrzuciliśmy 18 oczek;\nPrzypuśćmy, że \\(1/20\\) wszystkich kości gry jest sfałszowana zawsze wypada na nich szóstka. Wybieramy losowo trzy kostki rzucamy nimi. Obliczprawdopodobieństwo wyrzucenia w sumie 18 oczek;prawdopodobieństwo, że co najmniej jedna kostka była sfałszowana, jeżeli wyrzuciliśmy 18 oczek;Kierowcy dzielą się na ostrożnych (jest ich $ 95 % $ taki kierowca powoduje w ciągu roku wypadek z prawdopodobieństwem $ 0.01 $) piratów (jest ich $ 5 % $ taki kierowca powoduje w ciągu roku wypadek z prawdopodobieństwem $ 0.5 $). Wybrany losowo kierowca nie spowodował wypadku w pierwszym drugim roku. Obliczyć prawdopodobieństwo warunkowe, że spowoduje wypadek w trzecim roku.\\Kierowcy dzielą się na ostrożnych (jest ich $ 95 % $ taki kierowca powoduje w ciągu roku wypadek z prawdopodobieństwem $ 0.01 $) piratów (jest ich $ 5 % $ taki kierowca powoduje w ciągu roku wypadek z prawdopodobieństwem $ 0.5 $). Wybrany losowo kierowca nie spowodował wypadku w pierwszym drugim roku. Obliczyć prawdopodobieństwo warunkowe, że spowoduje wypadek w trzecim roku.\\W zabawę w ‘głuchy telefon’ gra \\(n\\) osób: \\(L_1, \\ldots, L_n\\). Pierwsza osoba \\(L_1\\) otrzymuje\ninformację w postaci ‘tak’ lub ‘nie’ przekazuje ją \\(L_2\\). Osoba \\(L_2\\) przekazują ją dalej, z prawdopodobieństwem \\(p\\)\ntaką samą, z prawdopodobieństwem \\(1-p\\) przeciwną, itd. Każdy uczestnik przekazuje kolejnemu informację,\nktórą uzyskał w prawdopodobieństwem \\(p\\) przeciwną z prawdopodobieństwem \\(1-p\\).\nOblicz prawdopodobieństwo \\(q_n\\), że osoba \\(L_n\\) otrzyma prawidłową informację.\nOblicz \\(\\lim_n q_n\\).W zabawę w ‘głuchy telefon’ gra \\(n\\) osób: \\(L_1, \\ldots, L_n\\). Pierwsza osoba \\(L_1\\) otrzymuje\ninformację w postaci ‘tak’ lub ‘nie’ przekazuje ją \\(L_2\\). Osoba \\(L_2\\) przekazują ją dalej, z prawdopodobieństwem \\(p\\)\ntaką samą, z prawdopodobieństwem \\(1-p\\) przeciwną, itd. Każdy uczestnik przekazuje kolejnemu informację,\nktórą uzyskał w prawdopodobieństwem \\(p\\) przeciwną z prawdopodobieństwem \\(1-p\\).\nOblicz prawdopodobieństwo \\(q_n\\), że osoba \\(L_n\\) otrzyma prawidłową informację.\nOblicz \\(\\lim_n q_n\\).","code":""},{"path":"lista-3-prawdopodobieństwo-warunkowe.html","id":"zadania-dodatkowe-2","chapter":"Lista 3: Prawdopodobieństwo warunkowe","heading":"Zadania dodatkowe","text":"Na rodzinie wszystkich podzbiorów \\(\\mathbb{N}\\) określamy miarę probabilistyczną \\(\\mathbb{P}_n\\)\nwzorem\n\\[\n\\mathbb{P}_n() = \\frac{|\\{m:\\; 1\\le m\\le n, m\\\\}|}{n}.\n\\] Mówimy, że zbiór \\(\\) ma gęstość\n\\[\nD() = \\lim_n \\mathbb{P}_n()\n\\]\njeżeli istnieje powyższa granica. Niech \\(\\mathcal D\\) oznacza rodzinę zbiorów posiadających gęstość.\nPokaż, że \\(D\\) jest skończenie addytywna na \\(\\mathcal D\\), ale nie jest przeliczalnie addytywna.\nCzy \\(\\mathcal D\\) jest \\(\\sigma\\)-ciałem?\nWykaż, że jeżeli \\(x\\[0,1]\\), istnieje zbiór \\(\\) taki, że \\(D() = x\\).\nPokaż, że \\(D\\) jest skończenie addytywna na \\(\\mathcal D\\), ale nie jest przeliczalnie addytywna.Czy \\(\\mathcal D\\) jest \\(\\sigma\\)-ciałem?Wykaż, że jeżeli \\(x\\[0,1]\\), istnieje zbiór \\(\\) taki, że \\(D() = x\\).Niech \\(\\Omega\\) będzie przestrzenią przeliczalnych ciągów 0-1, tj. \\(\\Omega = \\{0,1\\}^\\mathbb{N}\\).\nDla \\(\\omega\\\\Omega\\) oznaczmy przez \\(\\omega_n\\) wartość \\(n\\)-tej składowej.\nDla ustalonego ciągu \\(u=(u_1,\\ldots,u_n)\\\\{0,1\\}^n\\) niech\n\\[C_u =\\{\\omega:\\; \\omega_i = u_i; =1,\\ldots,n\\}. \\]\nZbiór \\(C_u\\) nazywamy cylindrem rzędu \\(n\\). Każdemu takiemu zbiorowi przypisujemy\nmiarę probabilistyczną~\\(\\mathbb{P}\\) równą \\(2^{-n}\\).\nOznaczmy przez \\(\\mathcal F_0\\) ciało składające się ze zbioru pustego oraz skończonych\nsum rozłącznych cylindrów. W naturalny sposób definiujemy \\(\\mathbb{P}\\) na \\(\\mathcal F_0\\).\nPokaż, że miara \\(\\mathbb P\\) jest przeliczalnie addytywna na \\(\\mathcal F_0\\).\nUtożsamiając \\(\\Omega\\) z przedziałem (0,1] porównaj miarę \\(\\mathbb P\\) z miarą Lebesgue’.\nPokaż, że miara \\(\\mathbb P\\) jest przeliczalnie addytywna na \\(\\mathcal F_0\\).Utożsamiając \\(\\Omega\\) z przedziałem (0,1] porównaj miarę \\(\\mathbb P\\) z miarą Lebesgue’.","code":""},{"path":"lista-4-niezależność-i-lemat-borela-cantellego.html","id":"lista-4-niezależność-i-lemat-borela-cantellego","chapter":"Lista 4: Niezależność i lemat Borela-Cantellego","heading":"Lista 4: Niezależność i lemat Borela-Cantellego","text":"Zadania na ćwiczenia: 2025-03-17Lista zadań w formacie PDF","code":""},{"path":"lista-4-niezależność-i-lemat-borela-cantellego.html","id":"zadania-do-samodzielnego-rozwiązania-3","chapter":"Lista 4: Niezależność i lemat Borela-Cantellego","heading":"Zadania do samodzielnego rozwiązania","text":"Maciek rzuca dwiema kośćmi. Pierwsza kość jest dobrze wyważona.\nNa drugiej szóstka wypada dwa razy częściej niż pozostałe liczby. Jakie jest prawdopodobieństwo,\nże suma oczek będzie większa niż 10?\n\nOdpowiedź\n\n\n\n\\(5/42\\)\n\n\n\\(5/42\\)\nLosujemy jednostajnie punkt \\((x,y)\\) z kwadratu \\([0,1]^2\\). Niech\n\\(= \\{ |x -y|\\leq 1/3\\}\\), \\(B = \\{x\\leq 1/2\\}\\) oraz \\(C = \\{ y\\geq 1/2\\}\\).\nCzy zdarzenia \\(\\), \\(B\\) \\(C\\) są niezależne? Czy są niezależne parami?\n\nOdpowiedź\n\n\n\nZdarzenia nie są niezależne. Są niezależne parami.\n\n\nZdarzenia nie są niezależne. Są niezależne parami.\nNiech \\(\\Omega = [-2,2]\\) oraz \\(A_n = [(-1)^n-1/n, (-1)^n+1/n]\\).\nZnajdź \\(\\limsup_n A_n\\)\nOdpowiedź\n\n\n\n\\(\\{-1,1\\}\\).\n\n\n\\(\\{-1,1\\}\\).\nZdarzenia \\(A_1, A_2, \\ldots , A_{2025}\\) są niezależne mają jednakowe prawdopodobieństwo \\(p\\).\nJaka jest szansa, że zajdzie dokładnie jedno?\n\nOdpowiedź\n\n\n\n\\(2025p(1-p)^{2024}\\)\n\n\n\\(2025p(1-p)^{2024}\\)\nZdarzenia \\(A_1, \\dots, A_n, \\dots\\) są niezależne mają równe prawdopodobieństwa.\nJaka jest szansa, że zajdzie skończenie wiele zdarzeń \\(A_n\\)?\n\nOdpowiedź\n\n\n\nNiech \\(\\mathbb{P}[A_k]=p\\). Jeżeli \\(p>0\\), szukana szansa wynosi zero.\n\n\nNiech \\(\\mathbb{P}[A_k]=p\\). Jeżeli \\(p>0\\), szukana szansa wynosi zero.\nRzucono \\(10\\) razy kostką. Jakie jest prawdopodobieństwo otrzymania\n\\(6\\) oczek co najmniej raz?\n\\(5\\) oczek dokładnie \\(3\\) razy?\n\\(6\\) oczek co najmniej raz?\\(5\\) oczek dokładnie \\(3\\) razy?\n0.8385, b. 0.155","code":""},{"path":"lista-4-niezależność-i-lemat-borela-cantellego.html","id":"zadania-na-ćwiczenia-3","chapter":"Lista 4: Niezależność i lemat Borela-Cantellego","heading":"Zadania na ćwiczenia","text":"Wykonujemy dwa eksperymenty: rzucamy kością, na której liczby parzyste wypadają dwa\nrazy częściej niż liczby nieparzyste (wszystkie parzyste są tak samo prawdopodobne wszystkie nieparzyste są tak samo prawdopodobne) losujemy liczbę z przedziału \\([0,1]\\).\nSkonstruuj przestrzeń probabilistyczną opisująca wykonanie tych eksperymentów\nniezależnie. Jakie jest prawdopodobieństwo, że suma otrzymanych liczb jest\nmniejsza niż \\(5/2\\)?Wykonujemy dwa eksperymenty: rzucamy kością, na której liczby parzyste wypadają dwa\nrazy częściej niż liczby nieparzyste (wszystkie parzyste są tak samo prawdopodobne wszystkie nieparzyste są tak samo prawdopodobne) losujemy liczbę z przedziału \\([0,1]\\).\nSkonstruuj przestrzeń probabilistyczną opisująca wykonanie tych eksperymentów\nniezależnie. Jakie jest prawdopodobieństwo, że suma otrzymanych liczb jest\nmniejsza niż \\(5/2\\)?Pokaż, że zdarzenia \\(A_1\\), …, \\(A_n\\) są niezależne wtedy tylko wtedy, gdy \\(\\sigma\\)-ciała\n\\(\\mathcal{F}_j=\\sigma(A_j) = \\{\\emptyset, \\Omega, A_j, A_j^c\\}\\) dla \\(j \\[n]\\) są niezależne.Pokaż, że zdarzenia \\(A_1\\), …, \\(A_n\\) są niezależne wtedy tylko wtedy, gdy \\(\\sigma\\)-ciała\n\\(\\mathcal{F}_j=\\sigma(A_j) = \\{\\emptyset, \\Omega, A_j, A_j^c\\}\\) dla \\(j \\[n]\\) są niezależne.Niech \\(\\Omega=[0,1]\\), \\(\\mathcal{F}=\\mathcal{B}(\\mathbb{R})\\) \\(\\mathbb{P}=\\lambda_1\\)\njednowymiarowa miara Lebesgue’. Rozważmy zdarzenia\n\\[\\begin{align*}\nA_1& =[0,1/2),\\\\\n     A_2 &= [0,1/4)\\cup [1/2,3/4), \\\\\nA_3 &= [0,1/8)\\cup[2/8, 3/8)\\cup[4/8,5/8)\\cup[6/8,7/8),\n     ....\n\\end{align*}\\]\nPokaż, że zdarzenia \\(\\{A_n\\}_{n \\\\mathbb{N}}\\) są niezależne.Niech \\(\\Omega=[0,1]\\), \\(\\mathcal{F}=\\mathcal{B}(\\mathbb{R})\\) \\(\\mathbb{P}=\\lambda_1\\)\njednowymiarowa miara Lebesgue’. Rozważmy zdarzenia\n\\[\\begin{align*}\nA_1& =[0,1/2),\\\\\n     A_2 &= [0,1/4)\\cup [1/2,3/4), \\\\\nA_3 &= [0,1/8)\\cup[2/8, 3/8)\\cup[4/8,5/8)\\cup[6/8,7/8),\n     ....\n\\end{align*}\\]\nPokaż, że zdarzenia \\(\\{A_n\\}_{n \\\\mathbb{N}}\\) są niezależne.Student musi poprawić oceny niedostateczne z dwóch przedmiotów. Szansa poprawienia oceny z\npierwszego przedmiotu w jednej próbie wynosi \\(p\\), z drugiego — \\(q\\).\nŻeby móc poprawić drugą ocenę, trzeba najpierw poprawić pierwszą. Poszczególne próby\npoprawiania są niezależne. Wiadomo, że po piętnastu próbach poprawiania oceny student\njeszcze nie poprawił oceny z drugiego przedmiotu.\nJaka jest szansa – pod tym warunkiem – że nie poprawił jeszcze oceny z pierwszego przedmiotu?Student musi poprawić oceny niedostateczne z dwóch przedmiotów. Szansa poprawienia oceny z\npierwszego przedmiotu w jednej próbie wynosi \\(p\\), z drugiego — \\(q\\).\nŻeby móc poprawić drugą ocenę, trzeba najpierw poprawić pierwszą. Poszczególne próby\npoprawiania są niezależne. Wiadomo, że po piętnastu próbach poprawiania oceny student\njeszcze nie poprawił oceny z drugiego przedmiotu.\nJaka jest szansa – pod tym warunkiem – że nie poprawił jeszcze oceny z pierwszego przedmiotu?Rzucamy \\(2n\\) razy symetryczną monetą. Niech \\(O_{2n}\\) (odpowiednio \\(R_{2n}\\))\noznacza liczbę orłów (odpowiednio reszek). Dla ustalonego \\(k\\) obliczyć\\[\n\\lim_{n \\\\infty} \\mathbb{P}(|O_{2n} - R_{2n}| \\leq 2k).\n\\]Rzucamy \\(2n\\) razy symetryczną monetą. Niech \\(O_{2n}\\) (odpowiednio \\(R_{2n}\\))\noznacza liczbę orłów (odpowiednio reszek). Dla ustalonego \\(k\\) obliczyć\\[\n\\lim_{n \\\\infty} \\mathbb{P}(|O_{2n} - R_{2n}| \\leq 2k).\n\\]Niech \\(A_n\\) będą zdarzeniami niezależnymi, przy czym \\(\\mathbb{P}(A_n) = p_n \\(0,1)\\).\nWykaż, że zachodzi co najmniej jedno ze zdarzeń \\(A_n\\) wtedy tylko wtedy,\ngdy z prawdopodobieństwem 1 zachodzi nieskończenie wiele zdarzeń \\(A_n\\).Niech \\(A_n\\) będą zdarzeniami niezależnymi, przy czym \\(\\mathbb{P}(A_n) = p_n \\(0,1)\\).\nWykaż, że zachodzi co najmniej jedno ze zdarzeń \\(A_n\\) wtedy tylko wtedy,\ngdy z prawdopodobieństwem 1 zachodzi nieskończenie wiele zdarzeń \\(A_n\\).Rzucamy nieskończenie wiele razy monetą, w której orzeł wypada z prawdopodobieństwem \\(p \\geq 1/2\\).\nNiech \\(A_n\\) oznacza zdarzenie, że pomiędzy rzutem \\(2^n\\) \\(2^{n+1}\\) otrzymano ciąg \\(n\\)\nkolejnych orłów. Pokaż, że zdarzenia \\(A_n\\) z prawdopodobieństwem \\(1\\) zachodzą\nnieskończenie wiele razy.Rzucamy nieskończenie wiele razy monetą, w której orzeł wypada z prawdopodobieństwem \\(p \\geq 1/2\\).\nNiech \\(A_n\\) oznacza zdarzenie, że pomiędzy rzutem \\(2^n\\) \\(2^{n+1}\\) otrzymano ciąg \\(n\\)\nkolejnych orłów. Pokaż, że zdarzenia \\(A_n\\) z prawdopodobieństwem \\(1\\) zachodzą\nnieskończenie wiele razy.Niech \\(\\{A_n\\}_{n \\\\mathbb{N}}\\) będzie ciągiem zdarzeń.\nPokaż, że jeśli \\(\\mathbb{P}(A_n) \\0\\) oraz\n\\[\n\\sum_{n=1}^{\\infty} P(A_n^c \\cap A_{n+1}) < \\infty,\n\\]\n\n\\[\n\\mathbb{P}(\\limsup_n A_n) = 0.\n\\]\nZnajdź przykład ciągu zdarzeń \\(A_n\\), którego można zastosować wynik z punktu ,\nale nie można zastosować lematu Borela-Cantellego.\nNiech \\(\\{A_n\\}_{n \\\\mathbb{N}}\\) będzie ciągiem zdarzeń.Pokaż, że jeśli \\(\\mathbb{P}(A_n) \\0\\) oraz\n\\[\n\\sum_{n=1}^{\\infty} P(A_n^c \\cap A_{n+1}) < \\infty,\n\\]\n\n\\[\n\\mathbb{P}(\\limsup_n A_n) = 0.\n\\]Znajdź przykład ciągu zdarzeń \\(A_n\\), którego można zastosować wynik z punktu ,\nale nie można zastosować lematu Borela-Cantellego.","code":""},{"path":"lista-4-niezależność-i-lemat-borela-cantellego.html","id":"zadania-dodatkowe-3","chapter":"Lista 4: Niezależność i lemat Borela-Cantellego","heading":"Zadania dodatkowe","text":"Rzucamy nieskończenie wiele razy symetryczną monetą.\nNiech \\(A_n\\) oznacza zdarzenie, że w pierwszych \\(n\\) rzutach było tyle samo orłów reszek.\nWykazać, że z prawdopodobieństwem 1 zachodzi nieskończenie wiele zdarzeń \\(A_n\\).","code":""},{"path":"lista-5-zmienne-losowe.html","id":"lista-5-zmienne-losowe","chapter":"Lista 5: Zmienne losowe","heading":"Lista 5: Zmienne losowe","text":"Zadania na ćwiczenia: 2025-03-24Lista zadań w formacie PDF","code":""},{"path":"lista-5-zmienne-losowe.html","id":"zadania-do-samodzielnego-rozwiązania-4","chapter":"Lista 5: Zmienne losowe","heading":"Zadania do samodzielnego rozwiązania","text":"Losujemy bez zwracania dwie liczby ze zbioru {1,2,3,4,5}.\nNiech \\(X\\) będzie sumą wylosowanych liczb - wyznacz jego rozkład oraz dystrybuantę.\n\nOdpowiedź\n\n\n\n\\[\\begin{align*}\n\\mu_X =&  0.1\\delta_3+0.1\\delta_4+0.2\\delta_5+0.2\\delta_6\\\\ &+0.2\\delta_7+0.1\\delta_8+0.1\\delta_9\\\\\nF_X(t) =&  0.1\\mathbf{1}_{[3,4)}(t) +0.2\\mathbf{1}_{[4,5)}(t)+0.4\\mathbf{1}_{[5,6)}(t) +\n    0.6\\mathbf{1}_{[6,7)}(t)\\\\ &+0.8\\mathbf{1}_{[7,8)}(t) +0.9 \\mathbf{1}_{[8,9)}(t) + \\mathbf{1}_{[9, +\\infty)}(t)\n\\end{align*}\\]\n\n\n\\[\\begin{align*}\n\\mu_X =&  0.1\\delta_3+0.1\\delta_4+0.2\\delta_5+0.2\\delta_6\\\\ &+0.2\\delta_7+0.1\\delta_8+0.1\\delta_9\\\\\nF_X(t) =&  0.1\\mathbf{1}_{[3,4)}(t) +0.2\\mathbf{1}_{[4,5)}(t)+0.4\\mathbf{1}_{[5,6)}(t) +\n    0.6\\mathbf{1}_{[6,7)}(t)\\\\ &+0.8\\mathbf{1}_{[7,8)}(t) +0.9 \\mathbf{1}_{[8,9)}(t) + \\mathbf{1}_{[9, +\\infty)}(t)\n\\end{align*}\\]\nLosujemy jednostajnie punkt z okręgu o promieniu jeden. Niech \\(X\\) będzie odległością\nwylosowanego punktu od środka okręgu. Uzasadnij, że \\(X\\) jest zmienną losową. Znajdź jej dystrybuantę.\n\nOdpowiedź\n\n\n\nPrzyjmujemy \\(\\Omega = \\{ (x,y) \\: : \\: x^2+y^2 <1\\}\\), \\(\\mathcal{F} = \\mathcal{B}(\\Omega)\\),\n\\(\\mathbb{P} = \\pi^{-1}\\lambda_1\\).\n\\[\\begin{align*}\n   X^{-1}[(-\\infty,t]] &= \\Omega\\cap \\{(x,y) \\: : \\: x^2+y^2<t^2\\}\\\\mathcal{F}\\\\\n   F_X(t) &= \\mathbf{1}_{[0,1]}(t)t^2 + \\mathbf{1}_{[ 1,\\infty)}(t).\n\\end{align*}\\]\n\n\nPrzyjmujemy \\(\\Omega = \\{ (x,y) \\: : \\: x^2+y^2 <1\\}\\), \\(\\mathcal{F} = \\mathcal{B}(\\Omega)\\),\n\\(\\mathbb{P} = \\pi^{-1}\\lambda_1\\).\n\\[\\begin{align*}\n   X^{-1}[(-\\infty,t]] &= \\Omega\\cap \\{(x,y) \\: : \\: x^2+y^2<t^2\\}\\\\mathcal{F}\\\\\n   F_X(t) &= \\mathbf{1}_{[0,1]}(t)t^2 + \\mathbf{1}_{[ 1,\\infty)}(t).\n\\end{align*}\\]\nNiech \\(F_1, F_2\\) będą dystrybuantami (spełniają założenia Twierdzenia 6.3).\nSprawdzić, czy następujące funkcje są jednowymiarowymi dystrybuantami (spełniają założenia Twierdzenia 6.3):\n\\(F_1 F_2\\);\n\\(F_1 + F_2\\);\n\\(F_1 - F_2\\);\n\\(F_1 / F_2\\);\n\\(\\max(F_1, F_2)\\);\n\\(\\min(F_1, F_2)\\).\nOdpowiedź\n\n\n\n,e,f.\n\n\n\\(F_1 F_2\\);\\(F_1 + F_2\\);\\(F_1 - F_2\\);\\(F_1 / F_2\\);\\(\\max(F_1, F_2)\\);\\(\\min(F_1, F_2)\\).\nOdpowiedź\n\n\n\n,e,f.\n\n\n,e,f.\nPodaj przykład przestrzeni probabilistycznej \\((\\Omega, {\\mathcal F}, {\\mathbb P})\\) funkcji\n\\(X: \\Omega\\ \\mathbb{R}\\), która nie jest zmienną losową.\n\nOdpowiedź\n\n\n\nNiech \\(\\Omega=\\{1,2\\}\\), \\(\\mathcal{F} = \\{\\emptyset, \\Omega\\}\\), \\(X(\\omega)=\\omega\\).\n\n\nNiech \\(\\Omega=\\{1,2\\}\\), \\(\\mathcal{F} = \\{\\emptyset, \\Omega\\}\\), \\(X(\\omega)=\\omega\\).\nDystrybuanta zmiennej losowej \\(X\\) dana jest wzorem\n\\[\\begin{equation*}\nF_X(x) \\; = \\; (0.1 + x)\\mathbf{1}_{[0,0.5)}(x) + (0.4 + x)\\mathbf{1}_{[0.5,0.55)}(x)\n+ \\mathbf{1}_{[0.55,\\infty)}(x) \\; .\n\\end{equation*}\\]\nNiech \\(\\mu_X\\) będzie rozkładem \\(X\\). Wyznacz\n\\(\\mu_X(\\{ 1/2 \\})\\),\n\\(\\mu_X([0,1/2])\\),\n\\(\\mu_X((0,0.55))\\).\n\nOdpowiedź\n\n\n\n0.3, b 0.9, c. 0.85\n\n\\(\\mu_X(\\{ 1/2 \\})\\),\\(\\mu_X([0,1/2])\\),\\(\\mu_X((0,0.55))\\).\n0.3, b 0.9, c. 0.85\nZmienna losowa \\(X\\) ma rozkład zadany przez\n\\[\\begin{equation*}\n\\mu_X() = \\int_A c (1-x)^2 \\mathbf{1}_{[0,1]}(x) \\mathrm{d}x.\n\\end{equation*}\\]\nZnajdź wartość parametru \\(c\\).\n\nOdpowiedź\n\n\n\n\\(3\\).\n\n\n\\(3\\).\nLosujemy jednostajnie punkt z przedziału \\((0,1)\\). Niech \\(U\\) będzie wylosowanym punktem. Wyznacz dystrybuantę\nzmiennej losowej \\(X = -\\log(U)\\).\n\nOdpowiedź\n\n\n\n\\(F_X(t)=1-e^{-t}\\).\n\n\n\\(F_X(t)=1-e^{-t}\\).\n","code":""},{"path":"lista-5-zmienne-losowe.html","id":"zadania-na-ćwiczenia-4","chapter":"Lista 5: Zmienne losowe","heading":"Zadania na ćwiczenia","text":"Dana jest przestrzeń probabilistyczna \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) oraz funkcja\n\\(X: \\Omega \\\\mathbb{R}\\). Uzasadnij, że jeżeli \\(X^{-1}(,b)\\\\mathcal{F}\\) dla dowolnych\n\\(,b\\\\mathbb{R}\\), \\(X\\) jest zmienną losową.Dana jest przestrzeń probabilistyczna \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) oraz funkcja\n\\(X: \\Omega \\\\mathbb{R}\\). Uzasadnij, że jeżeli \\(X^{-1}(,b)\\\\mathcal{F}\\) dla dowolnych\n\\(,b\\\\mathbb{R}\\), \\(X\\) jest zmienną losową.Dystrybuanta zmiennej losowej \\(X\\) dana jest wzorem:\n\\[\nF_X(t) = \\left\\{\n\\begin{array}{cc}\n   0 &  \\mbox{ dla } t<0\\\\\n   t^2 \\ \\  &  \\mbox{ dla } 0\\leq t < 1/2\\\\\n   1/4 & \\mbox{ dla } 1/2 \\leq t < 4 \\\\\n   1 & \\mbox{ dla }t\\geq 4.\n\\end{array}\n\\right.\n\\]\nOblicz \\(\\mathbb{P}[X=5]\\), \\(\\mathbb{P}[X=4]\\), \\(\\mathbb{P}[1/3 < X \\leq 5]\\), \\(\\mathbb{P}[0<X<1]\\).Dystrybuanta zmiennej losowej \\(X\\) dana jest wzorem:\n\\[\nF_X(t) = \\left\\{\n\\begin{array}{cc}\n   0 &  \\mbox{ dla } t<0\\\\\n   t^2 \\ \\  &  \\mbox{ dla } 0\\leq t < 1/2\\\\\n   1/4 & \\mbox{ dla } 1/2 \\leq t < 4 \\\\\n   1 & \\mbox{ dla }t\\geq 4.\n\\end{array}\n\\right.\n\\]\nOblicz \\(\\mathbb{P}[X=5]\\), \\(\\mathbb{P}[X=4]\\), \\(\\mathbb{P}[1/3 < X \\leq 5]\\), \\(\\mathbb{P}[0<X<1]\\).Niech \\(X_1,\\ldots, X_n\\) będą zmiennymi losowymi określonymi na przestrzeni probabilistycznej\n\\((\\Omega,\\mathcal{F},\\mathbb{P})\\) niech \\(B_1,B_2,\\ldots, B_n \\\\mathcal{F}\\) będzie rozbiciem \\(\\Omega\\)\n(tzn. zbiory te są rozłączne ich sumą jest \\(\\Omega\\)).\nNiech \\(Z(\\omega) = X_i(\\omega)\\) dla \\(\\omega\\B_i\\). Uzasadnij, że \\(Z\\) jest zmienną losową.Niech \\(X_1,\\ldots, X_n\\) będą zmiennymi losowymi określonymi na przestrzeni probabilistycznej\n\\((\\Omega,\\mathcal{F},\\mathbb{P})\\) niech \\(B_1,B_2,\\ldots, B_n \\\\mathcal{F}\\) będzie rozbiciem \\(\\Omega\\)\n(tzn. zbiory te są rozłączne ich sumą jest \\(\\Omega\\)).\nNiech \\(Z(\\omega) = X_i(\\omega)\\) dla \\(\\omega\\B_i\\). Uzasadnij, że \\(Z\\) jest zmienną losową.Punkt \\(x\\) nazywamy atomem rozkładu \\(\\mu\\) na \\(\\mathbb{R}\\), gdy \\(\\mu(\\{x\\}) > 0\\).\nPokaż, że rozkład prawdopodobieństwa \\(\\mu\\) może mieć co najwyżej przeliczalną liczbę atomów.\nCzy zbiór atomów może mieć punkt skupienia?\nCzy zbiór atomów może być gęsty w \\(\\mathbb{R}\\)?\nPunkt \\(x\\) nazywamy atomem rozkładu \\(\\mu\\) na \\(\\mathbb{R}\\), gdy \\(\\mu(\\{x\\}) > 0\\).Pokaż, że rozkład prawdopodobieństwa \\(\\mu\\) może mieć co najwyżej przeliczalną liczbę atomów.Czy zbiór atomów może mieć punkt skupienia?Czy zbiór atomów może być gęsty w \\(\\mathbb{R}\\)?Dane są dwie miary probabilistyczne \\(\\mu\\) \\(\\nu\\) na \\((\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))\\) takie, że dla dowolnej liczby \\(t>0\\) mamy \\(\\nu([-t,t]) = \\mu([-t,t])\\). Uzasadnić, że \\(\\mu() = \\nu()\\) dla dowolnego symetrycznego zbioru \\(\\\\mathcal{B}(\\mathbb{R})\\) (zbiór \\(\\) nazywamy symetrycznym, jeżeli \\(= -\\)).Dane są dwie miary probabilistyczne \\(\\mu\\) \\(\\nu\\) na \\((\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))\\) takie, że dla dowolnej liczby \\(t>0\\) mamy \\(\\nu([-t,t]) = \\mu([-t,t])\\). Uzasadnić, że \\(\\mu() = \\nu()\\) dla dowolnego symetrycznego zbioru \\(\\\\mathcal{B}(\\mathbb{R})\\) (zbiór \\(\\) nazywamy symetrycznym, jeżeli \\(= -\\)).Zmienna losowa \\(X\\) ma rozkład zadany przez\n\\[\n\\mu_X() = \\int_A \\frac{2}{\\pi}\\frac{1}{1+x^2} \\mathbf{1}_{[0, +\\infty)}(x) \\mathrm{d}x.\n\\]\nUdowodnij, że\n\\[\\begin{equation*}\nY(\\omega) = \\left\\{ \\begin{array}{cc} 1/X(\\omega), & X(\\omega)>0 \\\\  17, & X(\\omega)\\leq 0\\end{array} \\right.   \n\\end{equation*}\\]\nma ten sam rozkład, co \\(X\\).Zmienna losowa \\(X\\) ma rozkład zadany przez\n\\[\n\\mu_X() = \\int_A \\frac{2}{\\pi}\\frac{1}{1+x^2} \\mathbf{1}_{[0, +\\infty)}(x) \\mathrm{d}x.\n\\]\nUdowodnij, że\n\\[\\begin{equation*}\nY(\\omega) = \\left\\{ \\begin{array}{cc} 1/X(\\omega), & X(\\omega)>0 \\\\  17, & X(\\omega)\\leq 0\\end{array} \\right.   \n\\end{equation*}\\]\nma ten sam rozkład, co \\(X\\).(rozkład geometryczny) Wykonujemy doświadczenia Bernoulliego (z prawdopodobieństwem pojedynczego sukcesu \\(p\\))\naż chwili otrzymania pierwszego sukcesu. Niech \\(X\\) oznacza liczbę wykonanych doświadczeń,\n\\(Y\\) – czas oczekiwania na pierwszy sukces, czyli liczbę porażek przed pierwszym sukcesem. Wyznaczyć rozkłady zmiennych\nlosowych \\(X\\) \\(Y\\), tj. wyznaczyć funkcje \\(\\mathbb{P}[X=k]\\) oraz \\(\\mathbb{P}[Y=k]\\).(rozkład geometryczny) Wykonujemy doświadczenia Bernoulliego (z prawdopodobieństwem pojedynczego sukcesu \\(p\\))\naż chwili otrzymania pierwszego sukcesu. Niech \\(X\\) oznacza liczbę wykonanych doświadczeń,\n\\(Y\\) – czas oczekiwania na pierwszy sukces, czyli liczbę porażek przed pierwszym sukcesem. Wyznaczyć rozkłady zmiennych\nlosowych \\(X\\) \\(Y\\), tj. wyznaczyć funkcje \\(\\mathbb{P}[X=k]\\) oraz \\(\\mathbb{P}[Y=k]\\).(rozkład wykładniczy) Przypuśćmy, że doświadczenie opisane w poprzednim zadaniu\nwykonuje się \\(n\\) razy na sekundę,\nzaś prawdopodobieństwo sukcesu wynosi \\(\\lambda/n\\), \\(\\lambda > 0\\),\nczas oczekiwania na pierwszy sukces, \\(Y_n\\), mierzy się w sekundach.\nWyznaczyć dystrybuantę zmiennej losowej \\(Y_n\\) zbadać jej zachowanie gdy \\(n \\\\infty\\).(rozkład wykładniczy) Przypuśćmy, że doświadczenie opisane w poprzednim zadaniu\nwykonuje się \\(n\\) razy na sekundę,\nzaś prawdopodobieństwo sukcesu wynosi \\(\\lambda/n\\), \\(\\lambda > 0\\),\nczas oczekiwania na pierwszy sukces, \\(Y_n\\), mierzy się w sekundach.\nWyznaczyć dystrybuantę zmiennej losowej \\(Y_n\\) zbadać jej zachowanie gdy \\(n \\\\infty\\).","code":""},{"path":"lista-5-zmienne-losowe.html","id":"zadania-dodatkowe-4","chapter":"Lista 5: Zmienne losowe","heading":"Zadania dodatkowe","text":"Mówimy, że zmienna losowa \\(X\\) jest niezdegenerowana, gdy \\(\\mathbb{P}[X = ] < 1\\) dla każdego \\(\\\\mathbb{R}\\).\nWyznacz wszystkie liczby rzeczywiste \\(b, c\\) dla których istnieje niezdegenerowana zmienna losowa \\(X\\)\ntaka, że \\(X\\) ma taki sam rozkład jak \\(bX+c\\). Scharakteryzuj rozkład \\(X\\) w terminach \\(b\\) \\(c\\).","code":""},{"path":"lista-6-wartość-oczekiwana.html","id":"lista-6-wartość-oczekiwana","chapter":"Lista 6: Wartość oczekiwana","heading":"Lista 6: Wartość oczekiwana","text":"Zadania na ćwiczenia: 2025-03-31Lista zadań w formacie PDF","code":""},{"path":"lista-6-wartość-oczekiwana.html","id":"zadania-do-samodzielnego-rozwiązania-5","chapter":"Lista 6: Wartość oczekiwana","heading":"Zadania do samodzielnego rozwiązania","text":"W urnie jest \\(b \\geq 1\\) kul białych \\(c \\geq 1\\) czarnych.\nObliczyć \\(\\mathbb{E}[X]\\), jeśli \\(X\\) jest liczbą wylosowanych kul białych podczas\nlosowania bez zwracania \\(n\\) kul (\\(n \\leq b + c\\));\n\\(bn/(b+c)\\)\nZmienna losowa ma rozkład o gęstości \\(g(x) = 5x^4 \\mathbb{}_{[0,1]}(x)\\).\nObliczyć\n\\(\\mathbb{E}[X]\\)\n\\(\\mathbb{E}[1/(1+X^5)]\\).\nZmienna losowa ma rozkład o gęstości \\(g(x) = 5x^4 \\mathbb{}_{[0,1]}(x)\\).\nObliczyć\\(\\mathbb{E}[X]\\)\\(\\mathbb{E}[1/(1+X^5)]\\).\n5/6 b. \\(\\log(2)\\)Oblicz \\(\\mathbb{E}[X]\\) jeżeli \\(X\\) jest zmienną o rozkładzie:\nPoiss(\\(\\lambda\\)),\nExp(\\(\\lambda\\)),\nGeom(\\(p\\)).\n\\(\\mathcal{N}(\\mu, \\sigma^2)\\).\nOblicz \\(\\mathbb{E}[X]\\) jeżeli \\(X\\) jest zmienną o rozkładzie:Poiss(\\(\\lambda\\)),Exp(\\(\\lambda\\)),Geom(\\(p\\)).\\(\\mathcal{N}(\\mu, \\sigma^2)\\).\n\\(\\lambda\\) b. \\(1/\\lambda\\), c. \\(1/p\\), d. \\(\\mu\\)Zmienna losowa \\(X\\) ma rozkład jednostajny \\(U[0,1]\\).\nObliczyć \\(\\mathbb{E}[Y]\\) jeżeli\n\\(Y = e^X\\),\n\\(Y = \\cos^2(\\pi X)\\).\nZmienna losowa \\(X\\) ma rozkład jednostajny \\(U[0,1]\\).\nObliczyć \\(\\mathbb{E}[Y]\\) jeżeli\\(Y = e^X\\),\\(Y = \\cos^2(\\pi X)\\).\n\\(e-1\\) b. \\(1/2\\)Niech \\(F\\) będzie dystrybuantą zmiennej losowej \\(X\\), \\(f\\) jej gęstością.\nWyznaczyć dystrybuanty gęstości zmiennych losowych:\n\\(X + b\\) dla \\(> 0\\);\n\\(|X|\\);\n\\(X^2\\);\n\\(e^X\\);\nNiech \\(F\\) będzie dystrybuantą zmiennej losowej \\(X\\), \\(f\\) jej gęstością.\nWyznaczyć dystrybuanty gęstości zmiennych losowych:\\(X + b\\) dla \\(> 0\\);\\(|X|\\);\\(X^2\\);\\(e^X\\);\n\\[\\begin{align*}\n    . & F((t-b)/) & f((x-b)/)/\\\\\n    b. & F(t)-0F(-t) & f(x)-f(-x) \\\\\n    c. & F(\\sqrt{t}) - F(-\\sqrt{t}) & (f(\\sqrt{x}) +f(\\sqrt{x}))/(2\\sqrt{x}) \\\\\n    d. & F(\\log(t)) & f(\\log(x))/x 1_{y>0}\n\\end{align*}\\]\n","code":""},{"path":"lista-6-wartość-oczekiwana.html","id":"zadania-na-ćwiczenia-5","chapter":"Lista 6: Wartość oczekiwana","heading":"Zadania na ćwiczenia","text":"Monika wybrała się kasyna w Las Vegas mając przy sobie 255$.\nJako cel postawiła sobie wygranie \\(1\\) dolara w ruletkę wyjście z kasyna z kwotą 256$.\nPodczas tej wizyty obstawiała kolory. Wszystkie pola poza 0 00 są czerwone lub czarne\n(po 18 pól). Poprawne wskazanie koloru (z prawdopodobieństwem 18/38) podwaja zaryzykowaną kwotę.\nMonika zastosowała następującą strategię: postanowiła, że będzie grać kolejno\no 1$, 2$, 4$, 8$, 16$, 32$, 64$, 128$. Jeżeli w jednej z gier wygra,\nzabiera nagrodę opuszcza kasyno z 256 dolarami.\nObliczyć prawdopodobieństwo, że jej się powiodło. Obliczyć wartość oczekiwaną wygranej.Monika wybrała się kasyna w Las Vegas mając przy sobie 255$.\nJako cel postawiła sobie wygranie \\(1\\) dolara w ruletkę wyjście z kasyna z kwotą 256$.\nPodczas tej wizyty obstawiała kolory. Wszystkie pola poza 0 00 są czerwone lub czarne\n(po 18 pól). Poprawne wskazanie koloru (z prawdopodobieństwem 18/38) podwaja zaryzykowaną kwotę.\nMonika zastosowała następującą strategię: postanowiła, że będzie grać kolejno\no 1$, 2$, 4$, 8$, 16$, 32$, 64$, 128$. Jeżeli w jednej z gier wygra,\nzabiera nagrodę opuszcza kasyno z 256 dolarami.\nObliczyć prawdopodobieństwo, że jej się powiodło. Obliczyć wartość oczekiwaną wygranej.Losujemy cięciwę w okręgu o promieniu jeden poprzez wylosowanie jej środka.\nNiech \\(X_2\\) będzie długością wylosowanej cięciwy. Wyznacz rozkład \\(X_2\\).\nCzy rozkład ten jest absolutnie ciągły? Wyznacz gęstość. Znajdź \\(\\mathbb{E}[X_2]\\).Losujemy cięciwę w okręgu o promieniu jeden poprzez wylosowanie jej środka.\nNiech \\(X_2\\) będzie długością wylosowanej cięciwy. Wyznacz rozkład \\(X_2\\).\nCzy rozkład ten jest absolutnie ciągły? Wyznacz gęstość. Znajdź \\(\\mathbb{E}[X_2]\\).Każdy bok każda przekątna \\(2n\\)-kąta foremnego malujemy losowo na jeden z trzech kolorów.\nWybór każdego koloru jest jednakowo prawdopodobny, kolorowania różnych odcinków są niezależne.\nNiech \\(X\\) oznacza liczbę jednobarwnych trójkątów prostokątnych o wierzchołkach będących\nwierzchołkami \\(2n\\)-kąta. Obliczyć \\(\\mathbb{E}[X]\\).Każdy bok każda przekątna \\(2n\\)-kąta foremnego malujemy losowo na jeden z trzech kolorów.\nWybór każdego koloru jest jednakowo prawdopodobny, kolorowania różnych odcinków są niezależne.\nNiech \\(X\\) oznacza liczbę jednobarwnych trójkątów prostokątnych o wierzchołkach będących\nwierzchołkami \\(2n\\)-kąta. Obliczyć \\(\\mathbb{E}[X]\\).W urnie znajduje się 50 białych kul. Losujemy ze zwracaniem po jednej kuli,\nprzy czym wyciągniętą kulę malujemy na czerwono, jeśli jest biała.\nNiech \\(X\\) będzie liczbą czerwonych kul w urnie po 20 losowaniach.\nObliczyć \\(\\mathbb{E}[X]\\).W urnie znajduje się 50 białych kul. Losujemy ze zwracaniem po jednej kuli,\nprzy czym wyciągniętą kulę malujemy na czerwono, jeśli jest biała.\nNiech \\(X\\) będzie liczbą czerwonych kul w urnie po 20 losowaniach.\nObliczyć \\(\\mathbb{E}[X]\\).Na płaszczyźnie zaznaczono \\(n\\) punktów w taki sposób, że żadne trzy nie są współliniowe.\nKażda para punktów została połączona odcinkiem z prawdopodobieństwem \\(p\\).\nNiech \\(X\\) oznacza liczbę powstałych trójkątów.\nOblicz \\(\\mathbb{E}X\\).Na płaszczyźnie zaznaczono \\(n\\) punktów w taki sposób, że żadne trzy nie są współliniowe.\nKażda para punktów została połączona odcinkiem z prawdopodobieństwem \\(p\\).\nNiech \\(X\\) oznacza liczbę powstałych trójkątów.\nOblicz \\(\\mathbb{E}X\\).Pokaż, że jeżeli zmienna losowa \\(X\\) ma rozkład dyskretny skoncentrowany na liczbach całkowitych nieujemnych, \n\\[\n     \\mathbb{E}[X] = \\sum_{k=1}^{\\infty} \\mathbb{P}(X \\geq k).\n\\]Pokaż, że jeżeli zmienna losowa \\(X\\) ma rozkład dyskretny skoncentrowany na liczbach całkowitych nieujemnych, \n\\[\n     \\mathbb{E}[X] = \\sum_{k=1}^{\\infty} \\mathbb{P}(X \\geq k).\n\\]Niech \\(X\\) będzie zmienną losową o dystrybuancie \\(F\\). Wykaż, że\nJeżeli \\(X \\geq 0\\), \\(\\mathbb{E}[X]  =  \\int_0^\\infty \\mathbb{P}(X>t)\\: d t\n=  \\int_0^\\infty \\mathbb{P}(X \\geq t)\\:d t\\) przy czym istnienie\njednej strony implikuje istnienie drugiej ich równość.\nJeżeli \\(X\\) jest dowolną zmienną losową o skończonej wartości oczekiwanej,\n\\(\\mathbb{E}[X] =\n\\int_0^\\infty \\left( 1 - F(t) \\right)\\: d t - \\int_{-\\infty}^0 F(t)\\: d t\\).\nJeżeli \\(X \\geq 0\\), \\(\\varphi\\) jest rosnąca różniczkowalna,\n\\(\\varphi(0) = 0\\), \n\\(\\mathbb{E}[\\varphi(X)]  =  \\int_0^\\infty \\varphi '(t) \\mathbb{P}(X>t)\\: d t\\).\nW szczególności, jeżeli \\(r > 0\\), \\(\\mathbb{E}[X^r] =\n   \\int_0^\\infty r t^{r-1} \\mathbb{P}(X>t)\\: d t\\).\nNiech \\(X\\) będzie zmienną losową o dystrybuancie \\(F\\). Wykaż, żeJeżeli \\(X \\geq 0\\), \\(\\mathbb{E}[X]  =  \\int_0^\\infty \\mathbb{P}(X>t)\\: d t\n=  \\int_0^\\infty \\mathbb{P}(X \\geq t)\\:d t\\) przy czym istnienie\njednej strony implikuje istnienie drugiej ich równość.Jeżeli \\(X \\geq 0\\), \\(\\mathbb{E}[X]  =  \\int_0^\\infty \\mathbb{P}(X>t)\\: d t\n=  \\int_0^\\infty \\mathbb{P}(X \\geq t)\\:d t\\) przy czym istnienie\njednej strony implikuje istnienie drugiej ich równość.Jeżeli \\(X\\) jest dowolną zmienną losową o skończonej wartości oczekiwanej,\n\\(\\mathbb{E}[X] =\n\\int_0^\\infty \\left( 1 - F(t) \\right)\\: d t - \\int_{-\\infty}^0 F(t)\\: d t\\).Jeżeli \\(X\\) jest dowolną zmienną losową o skończonej wartości oczekiwanej,\n\\(\\mathbb{E}[X] =\n\\int_0^\\infty \\left( 1 - F(t) \\right)\\: d t - \\int_{-\\infty}^0 F(t)\\: d t\\).Jeżeli \\(X \\geq 0\\), \\(\\varphi\\) jest rosnąca różniczkowalna,\n\\(\\varphi(0) = 0\\), \n\\(\\mathbb{E}[\\varphi(X)]  =  \\int_0^\\infty \\varphi '(t) \\mathbb{P}(X>t)\\: d t\\).\nW szczególności, jeżeli \\(r > 0\\), \\(\\mathbb{E}[X^r] =\n   \\int_0^\\infty r t^{r-1} \\mathbb{P}(X>t)\\: d t\\).Jeżeli \\(X \\geq 0\\), \\(\\varphi\\) jest rosnąca różniczkowalna,\n\\(\\varphi(0) = 0\\), \n\\(\\mathbb{E}[\\varphi(X)]  =  \\int_0^\\infty \\varphi '(t) \\mathbb{P}(X>t)\\: d t\\).\nW szczególności, jeżeli \\(r > 0\\), \\(\\mathbb{E}[X^r] =\n   \\int_0^\\infty r t^{r-1} \\mathbb{P}(X>t)\\: d t\\).Udowodnić, że jeżeli \\(X \\geq 0\\), \\(\\sum_{n=1}^\\infty \\mathbb{P}(X \\geq n) \\; \\leq \\; \\\n\\mathbb{E}[X] \\; \\leq 1 + \\sum_{n=1}^\\infty \\mathbb{P}(X \\geq n)\\).Udowodnić, że jeżeli \\(X \\geq 0\\), \\(\\sum_{n=1}^\\infty \\mathbb{P}(X \\geq n) \\; \\leq \\; \\\n\\mathbb{E}[X] \\; \\leq 1 + \\sum_{n=1}^\\infty \\mathbb{P}(X \\geq n)\\).","code":""},{"path":"lista-7-powtórka-przed-kolokwium.html","id":"lista-7-powtórka-przed-kolokwium","chapter":"Lista 7: Powtórka przed kolokwium","heading":"Lista 7: Powtórka przed kolokwium","text":"Lista zadań w formacie PDF","code":""},{"path":"lista-7-powtórka-przed-kolokwium.html","id":"zadania-do-samodzielnego-rozwiązania-6","chapter":"Lista 7: Powtórka przed kolokwium","heading":"Zadania do samodzielnego rozwiązania","text":"Wybieramy losowo liczbę naturalną z przedziału \\([1, 1000]\\).\nObliczyć prawdopodobieństwo, że wybrana liczba jest podzielna przez co najmniej\njedną z liczb: \\(4\\), \\(6\\), \\(9\\).Wybieramy losowo liczbę naturalną z przedziału \\([1, 1000]\\).\nObliczyć prawdopodobieństwo, że wybrana liczba jest podzielna przez co najmniej\njedną z liczb: \\(4\\), \\(6\\), \\(9\\).Co jest bardziej prawdopodobne: otrzymanie co najmniej jednej jedynki przy\nrzucie \\(4\\) kostek, czy co najmniej raz dwóch jedynek na obu kostkach przy \\(24\\)\nrzutach obu kostek?Co jest bardziej prawdopodobne: otrzymanie co najmniej jednej jedynki przy\nrzucie \\(4\\) kostek, czy co najmniej raz dwóch jedynek na obu kostkach przy \\(24\\)\nrzutach obu kostek?Znaleźć prawdopodobieństwo, że przy wielokrotnym rzucaniu parą kostek sześciennych suma\noczek \\(8\\) wypadnie przed sumą oczek \\(7\\).Znaleźć prawdopodobieństwo, że przy wielokrotnym rzucaniu parą kostek sześciennych suma\noczek \\(8\\) wypadnie przed sumą oczek \\(7\\).Każda z \\(n\\) pałek została złamana na dwie części -\ndługą krótką. \\(2n\\) części połączono w \\(n\\) par, z których\nutworzono nowe pałki. Znaleźć prawdopodobieństwo, że\nczęści zostaną połączone w takich samych kombinacjach jak przed złamaniem;\nwszystkie długie części zostaną połączone z krótkimi.\nKażda z \\(n\\) pałek została złamana na dwie części -\ndługą krótką. \\(2n\\) części połączono w \\(n\\) par, z których\nutworzono nowe pałki. Znaleźć prawdopodobieństwo, żeczęści zostaną połączone w takich samych kombinacjach jak przed złamaniem;wszystkie długie części zostaną połączone z krótkimi.Niech \\(n \\\\mathbb{N}\\). Losujemy jednostajnie podzbiór\n\\([n]=\\{1,2, \\ldots , n\\}\\). Skonstruuj odpowiednią przestrzeń probabilistyczną.\nDla \\(k \\[n]\\) niech \\(A_k\\) będzie zdarzeniem, że wylosowany zbiór zawiera \\(k\\).\nPokaż, że zdarzenia \\(\\{A_k\\}_{k \\[n]}\\) są niezależne.Niech \\(n \\\\mathbb{N}\\). Losujemy jednostajnie podzbiór\n\\([n]=\\{1,2, \\ldots , n\\}\\). Skonstruuj odpowiednią przestrzeń probabilistyczną.\nDla \\(k \\[n]\\) niech \\(A_k\\) będzie zdarzeniem, że wylosowany zbiór zawiera \\(k\\).\nPokaż, że zdarzenia \\(\\{A_k\\}_{k \\[n]}\\) są niezależne.Niech \\(n \\\\mathbb{N}\\). Losujemy jednostajnie podzbiór \\(\\{1,2, \\ldots , n\\}\\).\nNiech \\(X\\) będzie liczebnością\nwylosowanego zbioru. Znajdź rozkład \\(X\\).Niech \\(n \\\\mathbb{N}\\). Losujemy jednostajnie podzbiór \\(\\{1,2, \\ldots , n\\}\\).\nNiech \\(X\\) będzie liczebnością\nwylosowanego zbioru. Znajdź rozkład \\(X\\).Zdarzenia \\(\\) \\(B\\) są niezależne oraz \\(\\cup B = \\Omega\\).\nWykazać, że \\(\\mathbb{P}() = 1\\) lub \\(\\mathbb{P}(B) = 1\\).Zdarzenia \\(\\) \\(B\\) są niezależne oraz \\(\\cup B = \\Omega\\).\nWykazać, że \\(\\mathbb{P}() = 1\\) lub \\(\\mathbb{P}(B) = 1\\).Niech \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) będzie przestrzenią probabilistyczną.\nDla zdarzeń \\(\\{A_k\\}_{k \\[n]}\\) \\(\\{\\epsilon_k\\}_{k\\[n]} \\\\{0,1\\}^n\\)\nniech\n\\[\\begin{equation*}\nA_k^{\\epsilon_k} = \\left\\{ \\begin{array}{cc} A_k, & \\epsilon_k=1 \\\\ A_k^c, & \\epsilon_k=0 \\end{array} \\right..\n\\end{equation*}\\]\nPokaż, że \\(\\{A_k\\}_{k \\[n]}\\) są niezależne wtedy tylko wtedy, gdy dla każdego\n\\(\\{\\epsilon_k\\}_{k \\[n]} \\\\{0,1\\}^n\\),\\[\\begin{equation*}\n\\mathbb{P}\\left[A_1^{\\epsilon_1}\\cap A_2^{\\epsilon_2}\\cap \\ldots \\cap A_{n}^{\\epsilon_n}\\right] =\n     \\mathbb{P}\\left[A_1^{\\epsilon_1}\\right]\n\\mathbb{P}\\left[A_2^{\\epsilon_2}\\right] \\cdots\n\\mathbb{P}\\left[A_{n}^{\\epsilon_n}\\right].\n\\end{equation*}\\]Niech \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) będzie przestrzenią probabilistyczną.\nDla zdarzeń \\(\\{A_k\\}_{k \\[n]}\\) \\(\\{\\epsilon_k\\}_{k\\[n]} \\\\{0,1\\}^n\\)\nniech\n\\[\\begin{equation*}\nA_k^{\\epsilon_k} = \\left\\{ \\begin{array}{cc} A_k, & \\epsilon_k=1 \\\\ A_k^c, & \\epsilon_k=0 \\end{array} \\right..\n\\end{equation*}\\]\nPokaż, że \\(\\{A_k\\}_{k \\[n]}\\) są niezależne wtedy tylko wtedy, gdy dla każdego\n\\(\\{\\epsilon_k\\}_{k \\[n]} \\\\{0,1\\}^n\\),\\[\\begin{equation*}\n\\mathbb{P}\\left[A_1^{\\epsilon_1}\\cap A_2^{\\epsilon_2}\\cap \\ldots \\cap A_{n}^{\\epsilon_n}\\right] =\n     \\mathbb{P}\\left[A_1^{\\epsilon_1}\\right]\n\\mathbb{P}\\left[A_2^{\\epsilon_2}\\right] \\cdots\n\\mathbb{P}\\left[A_{n}^{\\epsilon_n}\\right].\n\\end{equation*}\\]Niech \\(\\Omega\\) będzie zbiorem wszystkich grafów prostych na wierzchołkach\n\\(\\{1, 2, \\ldots , n\\}\\).\nRozważmy \\(\\mathcal{F} = 2^\\Omega\\) oraz \\(\\mathbb{P}[] = ||/|\\Omega|\\).\nDla każdej pary \\(<j\\) niech \\(A_{,j}\\) będzie zdarzeniem, że wylosowany graf\nzawiera krawędź \\(\\{,j\\}\\). Pokaż, że \\((A_{,j})_{<j}\\) są niezależne.Niech \\(\\Omega\\) będzie zbiorem wszystkich grafów prostych na wierzchołkach\n\\(\\{1, 2, \\ldots , n\\}\\).\nRozważmy \\(\\mathcal{F} = 2^\\Omega\\) oraz \\(\\mathbb{P}[] = ||/|\\Omega|\\).\nDla każdej pary \\(<j\\) niech \\(A_{,j}\\) będzie zdarzeniem, że wylosowany graf\nzawiera krawędź \\(\\{,j\\}\\). Pokaż, że \\((A_{,j})_{<j}\\) są niezależne.Rozważmy przestrzeń probabilistyczną z poprzedniego zadania. Niech \\(X\\) będzie liczbą trójkątów\nw wylosowanym grafie, tj. liczbą trójek \\(<j<k\\) takich, że wylosowany graf zawiera krawędzie\n\\(\\{,j\\}\\), \\(\\{j,k\\}\\) oraz \\(\\{k,\\}\\). Znajdź \\(\\mathbb{E}[X]\\).Rozważmy przestrzeń probabilistyczną z poprzedniego zadania. Niech \\(X\\) będzie liczbą trójkątów\nw wylosowanym grafie, tj. liczbą trójek \\(<j<k\\) takich, że wylosowany graf zawiera krawędzie\n\\(\\{,j\\}\\), \\(\\{j,k\\}\\) oraz \\(\\{k,\\}\\). Znajdź \\(\\mathbb{E}[X]\\).Niech\n\\[\\begin{equation*}\n\\Omega = \\{ \\omega=(\\omega_j)_{j \\[n]} \\\\mathbb{Z}^n \\: : \\: |\\omega_1| = |\\omega_{j}-\\omega_{j-1}|=1\\}\n\\end{equation*}\\]\nNiech \\(\\mathcal{F}=2^\\Omega\\) \\(\\mathbb{P}[] = ||/|\\Omega|\\).\nNiech \\(A_1 = \\{\\omega_1=1\\}\\).\nDla \\(k \\[n]\\), \\(k \\geq 2\\) połóżmy \\(A_k = \\{\\omega_k-\\omega_{k-1}=1\\}\\).\nPokaż, że \\(\\{A_k\\}_{k \\[n]}\\) są niezależne.Niech\n\\[\\begin{equation*}\n\\Omega = \\{ \\omega=(\\omega_j)_{j \\[n]} \\\\mathbb{Z}^n \\: : \\: |\\omega_1| = |\\omega_{j}-\\omega_{j-1}|=1\\}\n\\end{equation*}\\]\nNiech \\(\\mathcal{F}=2^\\Omega\\) \\(\\mathbb{P}[] = ||/|\\Omega|\\).\nNiech \\(A_1 = \\{\\omega_1=1\\}\\).\nDla \\(k \\[n]\\), \\(k \\geq 2\\) połóżmy \\(A_k = \\{\\omega_k-\\omega_{k-1}=1\\}\\).\nPokaż, że \\(\\{A_k\\}_{k \\[n]}\\) są niezależne.Rozważmy przestrzeń probabilistyczną z poprzedniego zadania. Rozważmy \\(X(\\omega) =\\omega_n\\). Znajdź rozkład \\(X\\).Rozważmy przestrzeń probabilistyczną z poprzedniego zadania. Rozważmy \\(X(\\omega) =\\omega_n\\). Znajdź rozkład \\(X\\).Na odcinku \\([0, 1]\\) umieszczono losowo punkty \\(A_1\\) , \\(A_2\\) \\(A_3\\) .\nObliczyć prawdopodobieństwo, że \\(A_1 \\leq A_2 \\leq A_3\\) .Na odcinku \\([0, 1]\\) umieszczono losowo punkty \\(A_1\\) , \\(A_2\\) \\(A_3\\) .\nObliczyć prawdopodobieństwo, że \\(A_1 \\leq A_2 \\leq A_3\\) .Niech \\(Q\\) będzie wielomianem rzeczywistym stopnia \\(2n\\) o losowych współczynnikach.\nKażdy współczynnik jest losowany niezależnie może wynieść \\(1\\) z prawdopodobienstawem \\(p\\)\noraz \\(-1\\) z prawdopodobieństwem \\(1-p\\). Znaleźć:\n\\(\\mathbb{P}[Q(2) > 0];\\)\n\\(\\mathbb{P}[Q(1) > 0]\\);\n\\(\\mathbb{P}[Q(2) > 0|Q(1) > 0]\\).\nNiech \\(Q\\) będzie wielomianem rzeczywistym stopnia \\(2n\\) o losowych współczynnikach.\nKażdy współczynnik jest losowany niezależnie może wynieść \\(1\\) z prawdopodobienstawem \\(p\\)\noraz \\(-1\\) z prawdopodobieństwem \\(1-p\\). Znaleźć:\\(\\mathbb{P}[Q(2) > 0];\\)\\(\\mathbb{P}[Q(1) > 0]\\);\\(\\mathbb{P}[Q(2) > 0|Q(1) > 0]\\).Przypomnijmy, że\n\\[\\begin{equation*}\n\\frac{\\pi^2}{6} = \\sum_{n=1}^\\infty \\frac{1}{n^2}.\n\\end{equation*}\\]\nRozważmy \\(\\Omega = \\mathbb{N}\\), \\(\\mathcal{F}=2^\\Omega\\) oraz \\(\\mathbb{P}[\\{n\\}]=6n^{-2}\\pi^{-2}\\).\nNiech \\(\\mathcal{P}\\) oznacza zbiór liczb pierwszych.\nPokaż, że \\(\\{p\\mathbb{N}\\}_{p \\\\mathcal{P}}\\) są niezależne. Wywnioskuj, że\n\\[\\begin{equation*}\n\\prod_{p \\\\mathcal{P}}\\left(1 -\\frac{1}{p^2} \\right) = \\frac{6}{\\pi^2}.\n\\end{equation*}\\]Przypomnijmy, że\n\\[\\begin{equation*}\n\\frac{\\pi^2}{6} = \\sum_{n=1}^\\infty \\frac{1}{n^2}.\n\\end{equation*}\\]\nRozważmy \\(\\Omega = \\mathbb{N}\\), \\(\\mathcal{F}=2^\\Omega\\) oraz \\(\\mathbb{P}[\\{n\\}]=6n^{-2}\\pi^{-2}\\).\nNiech \\(\\mathcal{P}\\) oznacza zbiór liczb pierwszych.\nPokaż, że \\(\\{p\\mathbb{N}\\}_{p \\\\mathcal{P}}\\) są niezależne. Wywnioskuj, że\n\\[\\begin{equation*}\n\\prod_{p \\\\mathcal{P}}\\left(1 -\\frac{1}{p^2} \\right) = \\frac{6}{\\pi^2}.\n\\end{equation*}\\]Rodzina \\(\\mathcal{R}_1\\) składa się z jednego zdarzenia \\(= \\{1,2\\}\\) zaś rodzina\n\\(\\mathcal{R}_2\\) z dwóch zdarzeń \\(B = \\{1,3\\}\\) \\(C = \\{2,3\\}\\).\nWiemy, że \\(\\Omega = \\{1,2,3,4\\}\\) że wszystkie zdarzenia elementarne są jednakowo prawdopodobne.\nCzy \\(\\sigma\\)-algebry \\(\\sigma(\\mathcal{R}_1)\\) \\(\\sigma(\\mathcal{R}_2)\\) są niezależne?Rodzina \\(\\mathcal{R}_1\\) składa się z jednego zdarzenia \\(= \\{1,2\\}\\) zaś rodzina\n\\(\\mathcal{R}_2\\) z dwóch zdarzeń \\(B = \\{1,3\\}\\) \\(C = \\{2,3\\}\\).\nWiemy, że \\(\\Omega = \\{1,2,3,4\\}\\) że wszystkie zdarzenia elementarne są jednakowo prawdopodobne.\nCzy \\(\\sigma\\)-algebry \\(\\sigma(\\mathcal{R}_1)\\) \\(\\sigma(\\mathcal{R}_2)\\) są niezależne?Czy funkcja\\[\nG(x) =\n\\begin{cases}\n0, & \\text{dla } x \\leq 0, \\\\\n1, & \\text{dla } x \\geq 1, \\\\\nx^2, & \\text{dla } 0 \\leq x < \\frac{1}{2}, \\\\\nx, & \\text{dla } \\frac{1}{2} \\leq x \\leq 1\n\\end{cases}\n\\]\njest dystrybuantą? Naszkicuj wykres funkcji \\(G\\) wyznacz jej uogólnioną funkcję odwrotną.\nZnajdź przestrzeń probabilistyczną \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) oraz zmienną losową \\(Y\\),\ndla której funkcja \\(G\\) jest dystrybuantą. Znajdź \\(\\mathbb{E}[e^{Y}]\\).Czy funkcja\\[\nG(x) =\n\\begin{cases}\n0, & \\text{dla } x \\leq 0, \\\\\n1, & \\text{dla } x \\geq 1, \\\\\nx^2, & \\text{dla } 0 \\leq x < \\frac{1}{2}, \\\\\nx, & \\text{dla } \\frac{1}{2} \\leq x \\leq 1\n\\end{cases}\n\\]\njest dystrybuantą? Naszkicuj wykres funkcji \\(G\\) wyznacz jej uogólnioną funkcję odwrotną.\nZnajdź przestrzeń probabilistyczną \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) oraz zmienną losową \\(Y\\),\ndla której funkcja \\(G\\) jest dystrybuantą. Znajdź \\(\\mathbb{E}[e^{Y}]\\).Niech \\(X\\) będzie zmienną losową o gęstości \\(\\alpha x^{\\alpha-1}\\mathbf{1}_{[0,1]}(x)\\). Niech\n\\(\\varphi(s) = \\max\\{ \\frac 14, s-s^2\\}\\). Oblicz \\(\\mathbb{E} \\varphi(X)\\).Niech \\(X\\) będzie zmienną losową o gęstości \\(\\alpha x^{\\alpha-1}\\mathbf{1}_{[0,1]}(x)\\). Niech\n\\(\\varphi(s) = \\max\\{ \\frac 14, s-s^2\\}\\). Oblicz \\(\\mathbb{E} \\varphi(X)\\).Niech \\(F\\) będzie dystrybuantą na \\(\\mathbb{R}\\). Pokaż, że dla dowolnego \\(\\geq 0\\),\n\\[\n             \\int_{\\mathbb{R}} F(x+) - F(x) \\mathrm{d}x =.\n         \\]Niech \\(F\\) będzie dystrybuantą na \\(\\mathbb{R}\\). Pokaż, że dla dowolnego \\(\\geq 0\\),\n\\[\n             \\int_{\\mathbb{R}} F(x+) - F(x) \\mathrm{d}x =.\n         \\]Z urny, w której jest \\(6\\) kul czarnych \\(4\\) białe losujemy kolejno bez zwracania po\njednej kuli tak długo, aż wylosujemy kulę czarną. Obliczyć wartość oczekiwaną\nliczby wylosowanych kul białych.Z urny, w której jest \\(6\\) kul czarnych \\(4\\) białe losujemy kolejno bez zwracania po\njednej kuli tak długo, aż wylosujemy kulę czarną. Obliczyć wartość oczekiwaną\nliczby wylosowanych kul białych.Rzucamy sześcienną kostką aż momentu, gdy wypadną z rzędu dwie „szóstki”.\nZnaleźć wartość oczekiwaną liczby rzutów.Rzucamy sześcienną kostką aż momentu, gdy wypadną z rzędu dwie „szóstki”.\nZnaleźć wartość oczekiwaną liczby rzutów.Niech \\(A_1, A_2, \\ldots, A_n\\) będą niezależnymi zdarzeniami o jednakowym\nprawdopodobieństwie \\(p_n\\). Przy pomocy nierówności Boole’oraz\nnierówności Bonferroniego oszacować (z góry z dołu)\n\\[ \\mathbf{P}_n \\; = \\; \\frac{\\mathbb{P}[\\bigcup_{=1}^n A_i]}{np_n} \\; . \\]\nJak sprawdza się szacowanie dla \\(p_n = 1/n\\)? Wyznaczyć \\(\\lim_{n \\\\infty} \\mathbf{P}_n\\)\nw przypadku, gdy \\(\\lim_{n \\\\infty} n p_n = 0\\).Niech \\(A_1, A_2, \\ldots, A_n\\) będą niezależnymi zdarzeniami o jednakowym\nprawdopodobieństwie \\(p_n\\). Przy pomocy nierówności Boole’oraz\nnierówności Bonferroniego oszacować (z góry z dołu)\n\\[ \\mathbf{P}_n \\; = \\; \\frac{\\mathbb{P}[\\bigcup_{=1}^n A_i]}{np_n} \\; . \\]\nJak sprawdza się szacowanie dla \\(p_n = 1/n\\)? Wyznaczyć \\(\\lim_{n \\\\infty} \\mathbf{P}_n\\)\nw przypadku, gdy \\(\\lim_{n \\\\infty} n p_n = 0\\).","code":""},{"path":"lista-8-wektory-losowe.html","id":"lista-8-wektory-losowe","chapter":"Lista 8: wektory losowe","heading":"Lista 8: wektory losowe","text":"Zadania na ćwiczenia: 2025-04-14Lista zadań w formacie PDF","code":""},{"path":"lista-8-wektory-losowe.html","id":"zadania-do-samodzielnego-rozwiązania-7","chapter":"Lista 8: wektory losowe","heading":"Zadania do samodzielnego rozwiązania","text":"Niech \\(\\vec{X}=(X_1, X_2)\\) będzie dwuwymiarowym wektorem losowym o standardowym rozkładzie normalnym.\nZnajdź rozkłady brzegowe.\n\nOdpowiedź\n\n\n\nOba rozkłady brzegowe standardowe rozkłady normalne.\n\nNiech \\(\\vec{X}=(X_1, X_2)\\) będzie dwuwymiarowym wektorem losowym o standardowym rozkładzie normalnym.\nZnajdź rozkłady brzegowe.\n\nOdpowiedź\n\nOba rozkłady brzegowe standardowe rozkłady normalne.\nNiech \\(\\vec{X}=(X,Y)\\) będzie dwuwymiarowym wektorem losowym o gęstości\n\\[\nf(x, y) =\n\\begin{cases}\n4x^2y + 2y^5 & 0 \\leq x, y \\leq 1, \\\\\n0 & \\mbox{w przeciwnym razie}\n\\end{cases}\n\\]\nSprawdź, że \\(f\\) jest rzeczywiście gęstością.\nOblicz \\(\\mathbb{P}[1/2 \\leq X \\leq 3/4, 1/4 \\leq Y \\leq 1/2]\\).\nZnajdź rozkłady brzegowe \\(X\\) \\(Y\\). Czy są one absolutnie ciągłe?\nJeżeli tak, oblicz ich gęstości.\nNiech \\(\\vec{X}=(X,Y)\\) będzie dwuwymiarowym wektorem losowym o gęstości\n\\[\nf(x, y) =\n\\begin{cases}\n4x^2y + 2y^5 & 0 \\leq x, y \\leq 1, \\\\\n0 & \\mbox{w przeciwnym razie}\n\\end{cases}\n\\]Sprawdź, że \\(f\\) jest rzeczywiście gęstością.Oblicz \\(\\mathbb{P}[1/2 \\leq X \\leq 3/4, 1/4 \\leq Y \\leq 1/2]\\).Znajdź rozkłady brzegowe \\(X\\) \\(Y\\). Czy są one absolutnie ciągłe?\nJeżeli tak, oblicz ich gęstości.\n\\(\\int_0^1 \\int_0^1 f(x, y) = 1\\)\\(\\frac{629}{16384}\\)\\[F_X(t) =\n\\begin{cases}\n0 & t < 0 \\\\\n\\frac{2 t^3 + t}{3} & 0 \\le t \\le 1\\\\\n1 & t > 1\n\\end{cases}\\]\n\\(f_X(t) = (2 t^2 + 1/3) \\mathbf{1}_{[0,1]}(t)\\)\n\\[F_Y (t) =\n\\begin{cases}\n0 & t<0 \\\\\n\\frac{2t^2 + t^6}{3} & t \\[0,1] \\\\\n1 & t > 1\n\\end{cases}\\]\n\\(f_Y(t) = (4/3 t + 2 t^5) \\mathbf{1}_{[0,1]} (t)\\)Niech \\((X,Y)\\) będzie dwuwymiarowym wektorem losowym o gęstości\n\\[\nf(x, y) =\n\\begin{cases}\nCye^{-xy} & 0 \\leq x, y \\leq 1, \\\\\n0 & \\text{w przeciwnym razie}\n\\end{cases}\n\\]\nOblicz wartość stałej \\(C\\).\nOblicz \\(\\mathbb{P}[1/2 \\leq X \\leq 3/4, 1/4 \\leq Y \\leq 1/2]\\).\nZnajdź rozkłady brzegowe \\(X\\) \\(Y\\). Czy są one absolutnie ciągłe? Jeżeli tak, oblicz ich gęstości.\nNiech \\((X,Y)\\) będzie dwuwymiarowym wektorem losowym o gęstości\n\\[\nf(x, y) =\n\\begin{cases}\nCye^{-xy} & 0 \\leq x, y \\leq 1, \\\\\n0 & \\text{w przeciwnym razie}\n\\end{cases}\n\\]Oblicz wartość stałej \\(C\\).Oblicz \\(\\mathbb{P}[1/2 \\leq X \\leq 3/4, 1/4 \\leq Y \\leq 1/2]\\).Znajdź rozkłady brzegowe \\(X\\) \\(Y\\). Czy są one absolutnie ciągłe? Jeżeli tak, oblicz ich gęstości.\n\\(C = e\\)\\(e (2(e^{-3/8} - e^{-1/4}) + 4 (e^{-3/16} - e^{-1/8}))\\)\\[F_X (t) =\n\\begin{cases}\n0 & t<0 \\\\\ne(1+ e^{-t}/t - 1/t) & t \\[0,1] \\\\\n1 & t > 1\n\\end{cases}\\]\n\\(f_X(t) = e(e^{-t}/t - e^{-t}/t^2 + 1/t^2)\\)\n\\[F_Y (t) =\n\\begin{cases}\n0 & t<0 \\\\\ne((e^{-t} - 1) + t)& t \\[0,1] \\\\\n1 & t > 1\n\\end{cases}\\]\n\\(f_Y(t) = e(-e^{-t} + 1)\\)Udowodnij, że wektor losowy \\(\\vec{X}=(X_1, X_2)\\) ma rozkład dyskretny wtedy tylko wtedy, gdy\nzmienne losowe \\(X_1\\) \\(X_2\\) mają rozkłady dyskretne.\n\nOdpowiedź\n\n\n\nZałóżmy, że \\(\\vec{X}\\) ma rozkład dyskretny. Wówczas\nistnieje przeliczalny zbiór \\(S\\subseteq \\mathbb{R}^2\\) taki, że\n\\[\\begin{equation*}\n\\mathbb{P}\\left[\\vec{X}\\S\\right]=1.\n\\end{equation*}\\]\nPokażemy jedynie, że \\(X_1\\) ma rozkład dyskretny. Dyskretność rozkładu \\(X_2\\) będzie\nwynikała z analogicznego argumentu. Rozważmy \\(S_1\\) będący rzutem \\(S\\) na pierwszą oś, tj.\n\\[\\begin{equation*}\nS_1 = \\{ x \\\\mathbb{R} \\: : \\: \\exists y \\\\mathbb{R}, \\: (x,y) \\S\\}\n\\subseteq \\mathbb{R}.\n\\end{equation*}\\]\nWówczas \\(|S_1|\\leq |S|\\). W szczególności \\(S_1\\) jest zbiorem przeliczalnym. Mamy\n\\[\\begin{equation*}\n\\left\\{ \\vec{X} \\S \\right\\} \\subseteq \\{X_1 \\S_1\\}.\n\\end{equation*}\\]\nRzeczywiście, niech \\(\\omega \\\\{\\vec{X}\\S\\}\\). Wówczas \\(\\vec{X}(\\omega) \\S\\).\nMamy \\(\\vec{X}(\\omega) = (X_1(\\omega), X_2(\\omega))\\). Innymi słowy dla \\(y =X_2(\\omega)\\) mamy\n\\((X_1(\\omega), y) \\S\\). Oznacza , że \\(X_1(\\omega) \\S_1\\), czyli \\(\\omega \\\\{ X_1\\S\\}\\).\ndowodzi postulowanej inkluzji. Z monotoniczności prawdopodobieństwa\n\\[\\begin{equation*}\n1=\\mathbb{P}\\left[\\vec{X}\\S \\right] \\leq \\mathbb{P}[X_1 \\S_1].\n\\end{equation*}\\]\nCo kończy dowód jednej implikacji.\nZałóżmy teraz, że rozkłady \\(X_1\\) \\(X_2\\) są dyskretne. Istnieją zatem przeliczalne\n\\(S_1\\), \\(S_2 \\subseteq \\mathbb{R}\\) takie, że\n\\[\\begin{equation*}\n\\mathbb{P}\\left[X_1\\S_1\\right]=\\mathbb{P}[X_2\\S_2]=1.\n\\end{equation*}\\]\nRozważmy zbiór \\(S = S_1 \\times S_2 \\subseteq \\mathbb{R}^2\\). Wówczas \\(S\\) jest zbiorem przeliczalnym.\nMamy\n\\[\\begin{equation*}\n\\left\\{ \\vec{X} \\S \\right\\} = \\{(X_1, X_2) \\S_1\\times S_2\\}\n= \\bigcup_{x \\S_1} \\{X_1 = x, \\: X_2\\S_2 \\}.\n\\end{equation*}\\]\nSkoro powyższa suma jest przeliczalna, zbiory rozłączne\n\\[\\begin{equation*}\n\\mathbb{P}\\left[\\vec{X} \\S \\right] = \\sum_{x \\S_1} \\mathbb{P}[X_1 =x, \\: X_2 \\S_2].\n\\end{equation*}\\]\nZauważmy, że \\(\\mathbb{P}[X_1=x, \\: X_2 \\S_2] = \\mathbb{P}[X_1=x]\\), ponieważ\n\\[\\begin{multline*}\n0 \\leq \\mathbb{P}[X_1 =x]-\\mathbb{P}[X_1=x,\\: X_2 \\S_2] \\\\=\n\\mathbb{P}[X_1=x, \\: X_2 \\notin S_2] \\leq \\mathbb{P}[X_2 \\notin S_2]=0.\n\\end{multline*}\\]\nMamy zatem\n\\[\\begin{equation*}\n\\mathbb{P}\\left[\\vec{X} \\S \\right] = \\sum_{x \\S_1} \\mathbb{P}[X_1 =x] = \\mathbb{P}[X_1\\S_1] =1.\n\\end{equation*}\\]\n\n\nZałóżmy, że \\(\\vec{X}\\) ma rozkład dyskretny. Wówczas\nistnieje przeliczalny zbiór \\(S\\subseteq \\mathbb{R}^2\\) taki, że\n\\[\\begin{equation*}\n\\mathbb{P}\\left[\\vec{X}\\S\\right]=1.\n\\end{equation*}\\]\nPokażemy jedynie, że \\(X_1\\) ma rozkład dyskretny. Dyskretność rozkładu \\(X_2\\) będzie\nwynikała z analogicznego argumentu. Rozważmy \\(S_1\\) będący rzutem \\(S\\) na pierwszą oś, tj.\n\\[\\begin{equation*}\nS_1 = \\{ x \\\\mathbb{R} \\: : \\: \\exists y \\\\mathbb{R}, \\: (x,y) \\S\\}\n\\subseteq \\mathbb{R}.\n\\end{equation*}\\]\nWówczas \\(|S_1|\\leq |S|\\). W szczególności \\(S_1\\) jest zbiorem przeliczalnym. Mamy\n\\[\\begin{equation*}\n\\left\\{ \\vec{X} \\S \\right\\} \\subseteq \\{X_1 \\S_1\\}.\n\\end{equation*}\\]\nRzeczywiście, niech \\(\\omega \\\\{\\vec{X}\\S\\}\\). Wówczas \\(\\vec{X}(\\omega) \\S\\).\nMamy \\(\\vec{X}(\\omega) = (X_1(\\omega), X_2(\\omega))\\). Innymi słowy dla \\(y =X_2(\\omega)\\) mamy\n\\((X_1(\\omega), y) \\S\\). Oznacza , że \\(X_1(\\omega) \\S_1\\), czyli \\(\\omega \\\\{ X_1\\S\\}\\).\ndowodzi postulowanej inkluzji. Z monotoniczności prawdopodobieństwa\n\\[\\begin{equation*}\n1=\\mathbb{P}\\left[\\vec{X}\\S \\right] \\leq \\mathbb{P}[X_1 \\S_1].\n\\end{equation*}\\]\nCo kończy dowód jednej implikacji.\nZałóżmy teraz, że rozkłady \\(X_1\\) \\(X_2\\) są dyskretne. Istnieją zatem przeliczalne\n\\(S_1\\), \\(S_2 \\subseteq \\mathbb{R}\\) takie, że\n\\[\\begin{equation*}\n\\mathbb{P}\\left[X_1\\S_1\\right]=\\mathbb{P}[X_2\\S_2]=1.\n\\end{equation*}\\]\nRozważmy zbiór \\(S = S_1 \\times S_2 \\subseteq \\mathbb{R}^2\\). Wówczas \\(S\\) jest zbiorem przeliczalnym.\nMamy\n\\[\\begin{equation*}\n\\left\\{ \\vec{X} \\S \\right\\} = \\{(X_1, X_2) \\S_1\\times S_2\\}\n= \\bigcup_{x \\S_1} \\{X_1 = x, \\: X_2\\S_2 \\}.\n\\end{equation*}\\]\nSkoro powyższa suma jest przeliczalna, zbiory rozłączne\n\\[\\begin{equation*}\n\\mathbb{P}\\left[\\vec{X} \\S \\right] = \\sum_{x \\S_1} \\mathbb{P}[X_1 =x, \\: X_2 \\S_2].\n\\end{equation*}\\]\nZauważmy, że \\(\\mathbb{P}[X_1=x, \\: X_2 \\S_2] = \\mathbb{P}[X_1=x]\\), ponieważ\n\\[\\begin{multline*}\n0 \\leq \\mathbb{P}[X_1 =x]-\\mathbb{P}[X_1=x,\\: X_2 \\S_2] \\\\=\n\\mathbb{P}[X_1=x, \\: X_2 \\notin S_2] \\leq \\mathbb{P}[X_2 \\notin S_2]=0.\n\\end{multline*}\\]\nMamy zatem\n\\[\\begin{equation*}\n\\mathbb{P}\\left[\\vec{X} \\S \\right] = \\sum_{x \\S_1} \\mathbb{P}[X_1 =x] = \\mathbb{P}[X_1\\S_1] =1.\n\\end{equation*}\\]\n","code":""},{"path":"lista-8-wektory-losowe.html","id":"zadania-na-ćwiczenia-6","chapter":"Lista 8: wektory losowe","heading":"Zadania na ćwiczenia","text":"Niech \\((X,Y)\\) będzie dwuwymiarowym wektorem losowym o rozkładzie zadanym gęstością\n\\[\nf(x, y) = C(x+y)\n\\]\ndla \\(0 \\leq y \\leq x \\leq 1\\), \\(f(x, y) = 0\\) poza tym zbiorem.\nZnajdź wartość \\(C\\). Znajdź rozkłady brzegowe.Niech \\((X,Y)\\) będzie dwuwymiarowym wektorem losowym o rozkładzie zadanym gęstością\n\\[\nf(x, y) = C(x+y)\n\\]\ndla \\(0 \\leq y \\leq x \\leq 1\\), \\(f(x, y) = 0\\) poza tym zbiorem.\nZnajdź wartość \\(C\\). Znajdź rozkłady brzegowe.Zmienna losowa \\((X,Y)\\) ma rozkład z gęstością\n\\[\ng(x, y) = C \\cdot xy \\cdot \\mathbf{1}_{[0,1]^2}(x, y)\n\\]\nWyznaczyć \\(C\\).\nObliczyć \\(\\mathbb{P}(X + Y \\leq 1)\\).\nWyznaczyć rozkład zmiennej losowej \\(X/Y\\).\nZmienna losowa \\((X,Y)\\) ma rozkład z gęstością\n\\[\ng(x, y) = C \\cdot xy \\cdot \\mathbf{1}_{[0,1]^2}(x, y)\n\\]Wyznaczyć \\(C\\).Obliczyć \\(\\mathbb{P}(X + Y \\leq 1)\\).Wyznaczyć rozkład zmiennej losowej \\(X/Y\\).Rzucamy trzy razy kostką. Niech \\(X_1\\) będzie liczbą wyrzuconych jedynek. Niech \\(X_2\\) będzie liczbą\nwyrzuconych dwójek.\nZnajdź rozkład wektora losowego \\(\\vec{X}=(X_1, X_2)\\).\nZnajdź \\(\\mathbb{P}[X_1=1, \\: X_2=2 \\: | \\: X_1+X_2=3]\\).\nRzucamy trzy razy kostką. Niech \\(X_1\\) będzie liczbą wyrzuconych jedynek. Niech \\(X_2\\) będzie liczbą\nwyrzuconych dwójek.Znajdź rozkład wektora losowego \\(\\vec{X}=(X_1, X_2)\\).Znajdź \\(\\mathbb{P}[X_1=1, \\: X_2=2 \\: | \\: X_1+X_2=3]\\).Losujemy liczbę \\(\\omega\\) z odcinka \\([0,2\\pi]\\) w sposób jednostajny.\nNiech \\(X_1(\\omega)=\\cos(\\omega)\\), \\(X_2(\\omega) = \\sin(\\omega)\\).\nZnajdź rozkład zmiennej \\(X_1\\).\nNiech \\(\\vec{X} = (X_1, X_2)\\). Znajdź\n\\(\\mathbb{P}\\left[\\vec{X} \\[1/\\sqrt{2},1]\\times [-2,3]\\right]\\).\nLosujemy liczbę \\(\\omega\\) z odcinka \\([0,2\\pi]\\) w sposób jednostajny.\nNiech \\(X_1(\\omega)=\\cos(\\omega)\\), \\(X_2(\\omega) = \\sin(\\omega)\\).Znajdź rozkład zmiennej \\(X_1\\).Niech \\(\\vec{X} = (X_1, X_2)\\). Znajdź\n\\(\\mathbb{P}\\left[\\vec{X} \\[1/\\sqrt{2},1]\\times [-2,3]\\right]\\).Losujemy punkt z trójkąta równobocznego \\(ABC\\). niech \\(X_1\\) będzie odległością wylosowanego punktu\nod boku \\(AB\\), \\(X_2\\) odległością wylosowanego punktu od boku \\(CB\\).\nZnajdź \\(\\mathbb{P}[X_1\\leq t]\\) dla \\(t \\[0. \\sqrt{3}/2]\\).\nZnajdź \\(\\mathbb{P} [X_2 \\leq s \\: | \\: X_1 \\leq t]\\) dla \\(s,t \\[0, \\sqrt{3}/2]\\).\nZnajdź dystrybuantę wektora losowego \\(\\vec{X} =(X_1, X_2)\\).\nLosujemy punkt z trójkąta równobocznego \\(ABC\\). niech \\(X_1\\) będzie odległością wylosowanego punktu\nod boku \\(AB\\), \\(X_2\\) odległością wylosowanego punktu od boku \\(CB\\).Znajdź \\(\\mathbb{P}[X_1\\leq t]\\) dla \\(t \\[0. \\sqrt{3}/2]\\).Znajdź \\(\\mathbb{P} [X_2 \\leq s \\: | \\: X_1 \\leq t]\\) dla \\(s,t \\[0, \\sqrt{3}/2]\\).Znajdź dystrybuantę wektora losowego \\(\\vec{X} =(X_1, X_2)\\).Załóżmy, że wektor losowy \\(\\vec{X}=(X_1, X_2)\\) ma dwuwymiarowy standardowy\nrozkład normalny. Rozważmy zmienne losowe \\(Y_1 = ax_1+bX_2\\) oraz\n\\(Y_2=-X_1/+X_2/b\\) dla \\(, b>0\\).\nZnajdź rozkład wektora losowego \\(\\vec{Y} = (Y_1, Y_2)\\).\nZnajdź rozkład zmiennej losowej \\(Y_1\\).\nZałóżmy, że wektor losowy \\(\\vec{X}=(X_1, X_2)\\) ma dwuwymiarowy standardowy\nrozkład normalny. Rozważmy zmienne losowe \\(Y_1 = ax_1+bX_2\\) oraz\n\\(Y_2=-X_1/+X_2/b\\) dla \\(, b>0\\).Znajdź rozkład wektora losowego \\(\\vec{Y} = (Y_1, Y_2)\\).Znajdź rozkład zmiennej losowej \\(Y_1\\).Niech \\(X = (X_1, X_2)\\) będzie wektorem z dwuwymiarowym rozkładem normalnym o parametrach \\(\\vec{m}=(0,0)\\) oraz\n\\[\\begin{equation*}\n\\Sigma = \\left( \\begin{array}{cc} 1 & \\rho \\\\ \\rho & 1 \\end{array} \\right).\n\\end{equation*}\\]\nPokaż, że \\(\\Sigma\\) jest dodatnio określona wtedy tylko wtedy, gdy \\(\\rho \\(-1,1)\\).\nNiech \\(f_{\\vec{X}}\\) będzie gęstością \\(\\vec{X}\\). Wyznacz poziomice \\(f_{\\vec{X}}\\),\n\\[\\begin{equation*}\n\\left\\{ (x,y) \\\\mathbb{R}^2 \\: : \\: f_{\\vec{X}}(x,y) = c \\right\\}, \\qquad c>0\n\\end{equation*}\\]\nw zależności od parametru \\(\\rho\\(-1,1)\\).\nNiech \\(X = (X_1, X_2)\\) będzie wektorem z dwuwymiarowym rozkładem normalnym o parametrach \\(\\vec{m}=(0,0)\\) oraz\n\\[\\begin{equation*}\n\\Sigma = \\left( \\begin{array}{cc} 1 & \\rho \\\\ \\rho & 1 \\end{array} \\right).\n\\end{equation*}\\]Pokaż, że \\(\\Sigma\\) jest dodatnio określona wtedy tylko wtedy, gdy \\(\\rho \\(-1,1)\\).Niech \\(f_{\\vec{X}}\\) będzie gęstością \\(\\vec{X}\\). Wyznacz poziomice \\(f_{\\vec{X}}\\),\n\\[\\begin{equation*}\n\\left\\{ (x,y) \\\\mathbb{R}^2 \\: : \\: f_{\\vec{X}}(x,y) = c \\right\\}, \\qquad c>0\n\\end{equation*}\\]\nw zależności od parametru \\(\\rho\\(-1,1)\\).Niech \\(\\vec{X}=(X_1, X_2)\\) będzie wektorem losowym o dwuwymiarowym standardowym rozkładzie normalnym.\nRozważamy wektor \\(\\vec{X}\\) współrzędnych biegunowych. Niech \\(R = \\sqrt{X_1^2+X_2^2}\\) niech\n\\(S = \\mathrm{arccos}(X_1/R)\\). Znajdź rozkład wektora losowego \\((R, S)\\).Niech \\(\\vec{X}=(X_1, X_2)\\) będzie wektorem losowym o dwuwymiarowym standardowym rozkładzie normalnym.\nRozważamy wektor \\(\\vec{X}\\) współrzędnych biegunowych. Niech \\(R = \\sqrt{X_1^2+X_2^2}\\) niech\n\\(S = \\mathrm{arccos}(X_1/R)\\). Znajdź rozkład wektora losowego \\((R, S)\\).","code":""},{"path":"lista-8-wektory-losowe.html","id":"zadania-dodatkowe-5","chapter":"Lista 8: wektory losowe","heading":"Zadania dodatkowe","text":"Wektor losowy \\(\\vec{U} = (X, Y, Z)\\) ma następującą własność:\njeżeli\\[\n^2 + b^2 + c^2 = 1,\n\\]\nzmienna losowa\n\\[\naX + + cZ\n\\]\nma rozkład jednostajny na \\([-1, 1]\\). Jaki rozkład ma wektor \\(\\vec{U}\\)?","code":""},{"path":"lista-9-niezależne-zmienne.html","id":"lista-9-niezależne-zmienne","chapter":"Lista 9: Niezależne zmienne","heading":"Lista 9: Niezależne zmienne","text":"Zadania na ćwiczenia: 2025-04-28Lista zadań w formacie PDF","code":""},{"path":"lista-9-niezależne-zmienne.html","id":"zadania-do-samodzielnego-rozwiązania-8","chapter":"Lista 9: Niezależne zmienne","heading":"Zadania do samodzielnego rozwiązania","text":"Załóżmy, że \\(\\xi_1, \\ldots, \\xi_n\\) są niezależnymi zmiennymi losowymi Bernoulliego, dla których\n\\[\n\\mathbb{P}(\\xi_k = 1) = p, \\quad \\mathbb{P}(\\xi_k = 0) = 1 - p, \\quad \\text{dla } 1 \\leq k \\leq n.\n\\]\nWyznacz prawdopodobieństwo warunkowe, że pierwsza jedynka („sukces”) pojawi się w \\(m\\)-tym kroku,\npod warunkiem, że w ciągu \\(n\\) kroków sukces wystąpił dokładnie raz.\n\nOdpowiedź\n\n\n\n\\(1/n\\)\n\nZałóżmy, że \\(\\xi_1, \\ldots, \\xi_n\\) są niezależnymi zmiennymi losowymi Bernoulliego, dla których\n\\[\n\\mathbb{P}(\\xi_k = 1) = p, \\quad \\mathbb{P}(\\xi_k = 0) = 1 - p, \\quad \\text{dla } 1 \\leq k \\leq n.\n\\]\nWyznacz prawdopodobieństwo warunkowe, że pierwsza jedynka („sukces”) pojawi się w \\(m\\)-tym kroku,\npod warunkiem, że w ciągu \\(n\\) kroków sukces wystąpił dokładnie raz.\n\nOdpowiedź\n\n\\(1/n\\)\nZmienne \\(X\\) \\(Y\\) są niezależne. \\(X\\) ma rozkład jednostajny na przedziale \\([0,1]\\),\n\\(Y\\) ma rozkład zadany przez \\(\\mathbb{P}[Y = -1] = 1/3\\), \\(\\mathbb{P}[Y = 2] = 2/3\\).\nOblicz \\(\\mathbb{P}[3X < Y]\\).\nWyznacz rozkład zmiennej \\(XY\\).\n\nOdpowiedź\n\n\n\n\\(4/9\\), b jednostajny na \\([-1,2]\\)\n\n\nZmienne \\(X\\) \\(Y\\) są niezależne. \\(X\\) ma rozkład jednostajny na przedziale \\([0,1]\\),\n\\(Y\\) ma rozkład zadany przez \\(\\mathbb{P}[Y = -1] = 1/3\\), \\(\\mathbb{P}[Y = 2] = 2/3\\).Oblicz \\(\\mathbb{P}[3X < Y]\\).Wyznacz rozkład zmiennej \\(XY\\).\n\nOdpowiedź\n\n\n\n\\(4/9\\), b jednostajny na \\([-1,2]\\)\n\n\n\\(4/9\\), b jednostajny na \\([-1,2]\\)\nZmienne losowe \\(X\\) \\(Y\\) są niezależne mają rozkłady wykładnicze z parametrami odpowiednio\n\\(\\lambda\\) \\(\\mu\\). Znajdź rozkład zmiennej losowej \\(X + Y\\).\n\nOdpowiedź\n\n\n\nJeżeli \\(\\mu \\neq \\lambda\\), \njest rozkład o gęstości \\(\\mu\\lambda (e^{-\\mu x} -e^{-\\lambda x})\\mathbf{1}_{[0, +\\infty)}(x)/(\\lambda -\\mu)\\)\nJeżeli \\(\\mu=\\lambda\\), jest rozkład o gęstości\n\\(\\lambda^2x e^{-\\lambda x}\\mathbf{1}_{[0, +\\infty)}(x)\\).\n\nZmienne losowe \\(X\\) \\(Y\\) są niezależne mają rozkłady wykładnicze z parametrami odpowiednio\n\\(\\lambda\\) \\(\\mu\\). Znajdź rozkład zmiennej losowej \\(X + Y\\).\n\nOdpowiedź\n\nJeżeli \\(\\mu \\neq \\lambda\\), \njest rozkład o gęstości \\(\\mu\\lambda (e^{-\\mu x} -e^{-\\lambda x})\\mathbf{1}_{[0, +\\infty)}(x)/(\\lambda -\\mu)\\)\nJeżeli \\(\\mu=\\lambda\\), jest rozkład o gęstości\n\\(\\lambda^2x e^{-\\lambda x}\\mathbf{1}_{[0, +\\infty)}(x)\\).\nPodaj przykład dwóch zależnych zmiennych losowych \\(X\\) \\(Y\\),\ndla których \\(X^2\\) \\(Y^2\\) są niezależne.\n\nOdpowiedź\n\n\n\n\\(X=Y\\) takie, że \\(\\mathbb{P}[X=1=\\mathbb{P}[X=-1]=1/2\\).\n\nPodaj przykład dwóch zależnych zmiennych losowych \\(X\\) \\(Y\\),\ndla których \\(X^2\\) \\(Y^2\\) są niezależne.\n\nOdpowiedź\n\n\\(X=Y\\) takie, że \\(\\mathbb{P}[X=1=\\mathbb{P}[X=-1]=1/2\\).\nZałóżmy, że \\(\\xi_1, \\ldots, \\xi_n\\) są niezależnymi identycznie rozłożonymi zmiennymi losowymi,\ndla których:\n\\[\n\\mathbb{P}\\{\\xi_j = 1\\} = p, \\quad \\mathbb{P}\\{\\xi_j = 0\\} = 1 - p,\n\\]\ndla pewnego \\(0 < p < 1\\). Niech \\(S_k = \\xi_1 + \\ldots + \\xi_k\\), gdzie \\(k \\leq n\\).\nUdowodnij, że dla \\(1 \\leq m \\leq n\\) zachodzi:\n\\[\n\\mathbb{P}(S_m = k \\mid S_n = l) = \\frac{\\binom{m}{k} \\binom{n - m}{l - k}}{\\binom{n}{l}}.\n\\]\n\nOdpowiedź\n\n\n\nZauważmy, że \\(S_n-S_m = \\xi_{m+1}+\\ldots + \\xi_n\\). Wobec tego zmienne\n\\(S_m\\) oraz \\(S_n-S_m\\) są niezależne mają rozkłady odpowiednio \\(\\mathrm{Bin}(m,p)\\) oraz \\(\\mathrm{Bin}(n-m,p)\\). Mamy więc\n\\[\\begin{multline*}\n\\mathbb{P}[S_m=k, \\: S_n=l]\n= \\mathbb{P}[S_m=k, \\: S_n-S_m=l-k] \\\\\n= {m \\choose k}p^k(1-p)^{m-k}{n-m \\choose l-k} p^{l-k}(1-p)^{n-m-l+k}\n\\end{multline*}\\]\nPodstawiając powyższe wyliczenie wzoru na prawdopodobieństwo całkowite otrzymujemy tezę.\n\nZałóżmy, że \\(\\xi_1, \\ldots, \\xi_n\\) są niezależnymi identycznie rozłożonymi zmiennymi losowymi,\ndla których:\n\\[\n\\mathbb{P}\\{\\xi_j = 1\\} = p, \\quad \\mathbb{P}\\{\\xi_j = 0\\} = 1 - p,\n\\]\ndla pewnego \\(0 < p < 1\\). Niech \\(S_k = \\xi_1 + \\ldots + \\xi_k\\), gdzie \\(k \\leq n\\).\nUdowodnij, że dla \\(1 \\leq m \\leq n\\) zachodzi:\n\\[\n\\mathbb{P}(S_m = k \\mid S_n = l) = \\frac{\\binom{m}{k} \\binom{n - m}{l - k}}{\\binom{n}{l}}.\n\\]\n\nOdpowiedź\n\nZauważmy, że \\(S_n-S_m = \\xi_{m+1}+\\ldots + \\xi_n\\). Wobec tego zmienne\n\\(S_m\\) oraz \\(S_n-S_m\\) są niezależne mają rozkłady odpowiednio \\(\\mathrm{Bin}(m,p)\\) oraz \\(\\mathrm{Bin}(n-m,p)\\). Mamy więc\n\\[\\begin{multline*}\n\\mathbb{P}[S_m=k, \\: S_n=l]\n= \\mathbb{P}[S_m=k, \\: S_n-S_m=l-k] \\\\\n= {m \\choose k}p^k(1-p)^{m-k}{n-m \\choose l-k} p^{l-k}(1-p)^{n-m-l+k}\n\\end{multline*}\\]\nPodstawiając powyższe wyliczenie wzoru na prawdopodobieństwo całkowite otrzymujemy tezę.\n","code":""},{"path":"lista-9-niezależne-zmienne.html","id":"zadania-na-ćwiczenia-7","chapter":"Lista 9: Niezależne zmienne","heading":"Zadania na ćwiczenia","text":"Niech \\(X\\) będzie zmienną losową posiadającą wartość oczekiwaną. Pokaż, że dla zdarzenia\n\\(\\) o dodatnim prawdopodobieństwie\n\\[\\begin{equation*}\n\\mathbb{E}[X|] = \\frac{1}{\\mathbb{P}[]} \\mathbb{E}\\left[ X \\mathbf{1}_A \\right].\n  \\end{equation*}\\]Niech \\(X\\) będzie zmienną losową posiadającą wartość oczekiwaną. Pokaż, że dla zdarzenia\n\\(\\) o dodatnim prawdopodobieństwie\n\\[\\begin{equation*}\n\\mathbb{E}[X|] = \\frac{1}{\\mathbb{P}[]} \\mathbb{E}\\left[ X \\mathbf{1}_A \\right].\n  \\end{equation*}\\]Niech \\(\\vec{X}=(X_1, \\ldots , X_n)\\), gdzie zmienne \\(X_1, \\ldots , X_n\\) są niezależne z gęstościami odpowiednio\n\\(f_1, \\ldots , f_n\\). Pokaż, że \\(X_1, \\ldots, X_n\\) są niezależne wtedy tylko wtedy, gdy wektor losowy\n\\(\\vec{X}\\) ma rozkład o gęstości\n\\[\\begin{equation*}\nf_{\\vec{X}}(x_1,x_2 \\ldots , x_n) = f_1(x_1) \\cdot f_2(x_2) \\cdots f_n(x_n).\n\\end{equation*}\\]Niech \\(\\vec{X}=(X_1, \\ldots , X_n)\\), gdzie zmienne \\(X_1, \\ldots , X_n\\) są niezależne z gęstościami odpowiednio\n\\(f_1, \\ldots , f_n\\). Pokaż, że \\(X_1, \\ldots, X_n\\) są niezależne wtedy tylko wtedy, gdy wektor losowy\n\\(\\vec{X}\\) ma rozkład o gęstości\n\\[\\begin{equation*}\nf_{\\vec{X}}(x_1,x_2 \\ldots , x_n) = f_1(x_1) \\cdot f_2(x_2) \\cdots f_n(x_n).\n\\end{equation*}\\]Niech \\(X_1, \\ldots, X_n\\) będą niezależnymi zmiennymi losowymi o rozkładzie wykładniczym z parametrem \\(1\\). Znajdź rozkład \\(Y = \\min_{1 \\leq \\leq n} X_i\\). Czy \\(X_n\\) \\(Y\\) są niezależne?Niech \\(X_1, \\ldots, X_n\\) będą niezależnymi zmiennymi losowymi o rozkładzie wykładniczym z parametrem \\(1\\). Znajdź rozkład \\(Y = \\min_{1 \\leq \\leq n} X_i\\). Czy \\(X_n\\) \\(Y\\) są niezależne?Niech \\(X\\) \\(Y\\) będą niezależnymi zmiennymi losowymi o wartościach całkowitych. Pokaż, że dla każdej wartości \\(k\\) zachodzi\n\\[\n\\mathbb{P}[X + Y = k] = \\sum_{j = -\\infty}^{+\\infty} \\mathbb{P}[X = k - j] \\mathbb{P}[Y = j].\n\\]Niech \\(X\\) \\(Y\\) będą niezależnymi zmiennymi losowymi o wartościach całkowitych. Pokaż, że dla każdej wartości \\(k\\) zachodzi\n\\[\n\\mathbb{P}[X + Y = k] = \\sum_{j = -\\infty}^{+\\infty} \\mathbb{P}[X = k - j] \\mathbb{P}[Y = j].\n\\]Zmienne losowe \\(X_1, \\ldots, X_n\\) są niezależne mają rozkłady Poissona z parametrami \\(\\lambda_i\\). Pokaż, że \\(X_1 + \\ldots + X_n\\) ma rozkład Poissona z parametrem \\(\\lambda_1 + \\ldots + \\lambda_n\\).Zmienne losowe \\(X_1, \\ldots, X_n\\) są niezależne mają rozkłady Poissona z parametrami \\(\\lambda_i\\). Pokaż, że \\(X_1 + \\ldots + X_n\\) ma rozkład Poissona z parametrem \\(\\lambda_1 + \\ldots + \\lambda_n\\).Załóżmy, że \\(X_1\\) \\(X_2\\) są niezależnymi zmiennymi losowymi o rozkładach odpowiednio\n\\(\\mathcal{N}(m_1, \\sigma_1)\\) \\(\\mathcal{N}(m_2, \\sigma_2)\\).\nZnajdź rozkład zmiennej losowej \\(X_1 + X_2\\).Załóżmy, że \\(X_1\\) \\(X_2\\) są niezależnymi zmiennymi losowymi o rozkładach odpowiednio\n\\(\\mathcal{N}(m_1, \\sigma_1)\\) \\(\\mathcal{N}(m_2, \\sigma_2)\\).\nZnajdź rozkład zmiennej losowej \\(X_1 + X_2\\).Niech \\(E_1\\) \\(E_2\\) będą niezaleznymi zmiennymi losowymi o rozkładach Exp(\\(\\lambda\\)).\nZnajdź rozkład \\(E_1\\) pod warunkiem \\(\\{ E_1 \\leq t < E_1+E_2\\}\\) dla \\(t>0\\).Niech \\(E_1\\) \\(E_2\\) będą niezaleznymi zmiennymi losowymi o rozkładach Exp(\\(\\lambda\\)).\nZnajdź rozkład \\(E_1\\) pod warunkiem \\(\\{ E_1 \\leq t < E_1+E_2\\}\\) dla \\(t>0\\).Niech \\(\\Omega\\subseteq \\mathbb{R}\\) będzie kołem jednostkowym. Rozważmy \\(\\mathcal{F}=\\mathcal{B}(\\Omega)\\)\noraz \\(\\mathbb{P}\\) jako unormowaną dwuwymiarową miarę Lebesgue’. Rozważmy zmienne losowe\n\\(X(\\omega) = \\omega_1/\\sqrt{\\omega_1^2+\\omega_2^2}\\), \\(Y(\\omega)=\\omega_2/\\sqrt{\\omega_1^2+\\omega_2^2}\\) oraz\n\\(Z(\\omega) = \\omega_1^2+\\omega_2^2\\) dla \\(\\omega=(\\omega_1, \\omega_2) \\\\Omega\\).\nCzy zmienne \\(X\\) \\(Y\\) są niezależne?\nCzy zmienne \\(X\\) \\(Z\\) są niezależne?\nNiech \\(\\Omega\\subseteq \\mathbb{R}\\) będzie kołem jednostkowym. Rozważmy \\(\\mathcal{F}=\\mathcal{B}(\\Omega)\\)\noraz \\(\\mathbb{P}\\) jako unormowaną dwuwymiarową miarę Lebesgue’. Rozważmy zmienne losowe\n\\(X(\\omega) = \\omega_1/\\sqrt{\\omega_1^2+\\omega_2^2}\\), \\(Y(\\omega)=\\omega_2/\\sqrt{\\omega_1^2+\\omega_2^2}\\) oraz\n\\(Z(\\omega) = \\omega_1^2+\\omega_2^2\\) dla \\(\\omega=(\\omega_1, \\omega_2) \\\\Omega\\).Czy zmienne \\(X\\) \\(Y\\) są niezależne?Czy zmienne \\(X\\) \\(Z\\) są niezależne?Niech \\(Z\\) będzie zmienną losową o rozkładzie \\(\\mathrm{Exp}(1)\\).\nNiech \\(\\{Z\\}\\) oznacza część ułamkową zmiennej \\(Z\\), \\([Z]\\) — jej część całkowitą.\nUdowodnij, że \\(\\{Z\\}\\) \\([Z]\\) są niezależne oraz wyznacz ich rozkłady jawnie.\nRozważmy dodatnią zmienną losową \\(X\\), której rozkład jest absolutnie ciągły z\ngęstością \\(\\varphi\\)\ntaką, że \\(\\{X\\}\\) \\([X]\\) są niezależne \\(\\{X\\}\\) ma rozkład jednostajny na \\([0, 1]\\).\nZnajdź \\(\\varphi\\).\nNiech \\(Z\\) będzie zmienną losową o rozkładzie \\(\\mathrm{Exp}(1)\\).\nNiech \\(\\{Z\\}\\) oznacza część ułamkową zmiennej \\(Z\\), \\([Z]\\) — jej część całkowitą.Udowodnij, że \\(\\{Z\\}\\) \\([Z]\\) są niezależne oraz wyznacz ich rozkłady jawnie.Rozważmy dodatnią zmienną losową \\(X\\), której rozkład jest absolutnie ciągły z\ngęstością \\(\\varphi\\)\ntaką, że \\(\\{X\\}\\) \\([X]\\) są niezależne \\(\\{X\\}\\) ma rozkład jednostajny na \\([0, 1]\\).\nZnajdź \\(\\varphi\\).","code":""},{"path":"lista-9-niezależne-zmienne.html","id":"zadania-dodatkowe-6","chapter":"Lista 9: Niezależne zmienne","heading":"Zadania dodatkowe","text":"Z odcinka \\([0,1]\\) losujemy niezależnie w sposób jednostajny liczby \\(X_1, X_2, \\ldots\\).\nUzasadnij, że z prawdopodobieństwem \\(1\\), ciąg \\(\\{X_n\\}\\) jest gęsty w odcinku \\([0,1]\\).Z odcinka \\([0,1]\\) losujemy niezależnie w sposób jednostajny liczby \\(X_1, X_2, \\ldots\\).\nUzasadnij, że z prawdopodobieństwem \\(1\\), ciąg \\(\\{X_n\\}\\) jest gęsty w odcinku \\([0,1]\\).Scharakteryzuj gęstości doodatnich zmiennych losowych \\(X\\) dla których \\([X]\\) oraz \\(\\{X\\}\\) są\nniezależne.Scharakteryzuj gęstości doodatnich zmiennych losowych \\(X\\) dla których \\([X]\\) oraz \\(\\{X\\}\\) są\nniezależne.Niech \\(\\Gamma \\\\mathcal{F}\\) niech \\(\\mathcal{G} \\subseteq \\mathcal{F}\\) będzie \\(\\sigma\\)-ciałem.\nUdowodnij, że następujące warunki są równoważne:\n\\(\\Gamma\\) jest niezależny od \\(\\mathcal{G}\\) względem \\(\\mathbb{P}\\),\ndla każdego prawdopodobieństwa \\(\\mathbb{Q}\\) na \\((\\Omega, \\mathcal{F})\\),\nrównoważnego z \\(\\mathbb{P}\\), takiego że \\(\\left(\\frac{d\\mathbb{Q}}{d\\mathbb{P}}\\right)\\)\njest \\(\\mathcal{G}\\)-mierzalna, zachodzi \\(\\mathbb{Q}(\\Gamma) = \\mathbb{P}(\\Gamma)\\).\nNiech \\(\\Gamma \\\\mathcal{F}\\) niech \\(\\mathcal{G} \\subseteq \\mathcal{F}\\) będzie \\(\\sigma\\)-ciałem.\nUdowodnij, że następujące warunki są równoważne:\\(\\Gamma\\) jest niezależny od \\(\\mathcal{G}\\) względem \\(\\mathbb{P}\\),dla każdego prawdopodobieństwa \\(\\mathbb{Q}\\) na \\((\\Omega, \\mathcal{F})\\),\nrównoważnego z \\(\\mathbb{P}\\), takiego że \\(\\left(\\frac{d\\mathbb{Q}}{d\\mathbb{P}}\\right)\\)\njest \\(\\mathcal{G}\\)-mierzalna, zachodzi \\(\\mathbb{Q}(\\Gamma) = \\mathbb{P}(\\Gamma)\\).","code":""},{"path":"lista-10-wariancja.html","id":"lista-10-wariancja","chapter":"Lista 10: Wariancja","heading":"Lista 10: Wariancja","text":"Zadania na ćwiczenia: 2025-05-12Lista zadań w formacie PDF","code":""},{"path":"lista-10-wariancja.html","id":"zadania-do-samodzielnego-rozwiązania-9","chapter":"Lista 10: Wariancja","heading":"Zadania do samodzielnego rozwiązania","text":"Zmienne losowe \\(X, Y\\) spełniają warunki: \\(\\mathbb{V}ar[X] = 3\\), \\(\\mathrm{Cov}(X,Y) = -1\\), \\(\\mathbb{V}ar[Y] = 2\\).\nOblicz \\(\\mathbb{V}ar[4X - 3Y]\\) oraz \\(\\mathrm{Cov}(2X - Y, 2X + Y)\\).\n\nOdpowiedź\n\n\n\n\\(\\mathbb{V}ar[4X - 3Y]=90\\) oraz \\(\\mathrm{Cov}(2X - Y, 2X + Y)=10\\).\n\nZmienne losowe \\(X, Y\\) spełniają warunki: \\(\\mathbb{V}ar[X] = 3\\), \\(\\mathrm{Cov}(X,Y) = -1\\), \\(\\mathbb{V}ar[Y] = 2\\).\nOblicz \\(\\mathbb{V}ar[4X - 3Y]\\) oraz \\(\\mathrm{Cov}(2X - Y, 2X + Y)\\).\n\nOdpowiedź\n\n\\(\\mathbb{V}ar[4X - 3Y]=90\\) oraz \\(\\mathrm{Cov}(2X - Y, 2X + Y)=10\\).\nNiech \\(\\vec{X}= (X, Y)\\) będzie jednostajnie wylosowanym punktem kwadratu jednostkowego \\([0, 1]^2\\).\nZnajdź \\(\\mathbb{V}ar(X)\\), \\(\\mathbb{V}ar(Y)\\), \\(\\mathrm{Cov}(X, Y)\\).\n\nOdpowiedź\n\n\n\n\\(\\mathrm{Var}(X)= 1/12\\), \\(\\mathrm{Var}(Y)=1/12\\), \\(\\mathrm{Cov}(X, Y)=0\\).\n\nNiech \\(\\vec{X}= (X, Y)\\) będzie jednostajnie wylosowanym punktem kwadratu jednostkowego \\([0, 1]^2\\).\nZnajdź \\(\\mathbb{V}ar(X)\\), \\(\\mathbb{V}ar(Y)\\), \\(\\mathrm{Cov}(X, Y)\\).\n\nOdpowiedź\n\n\\(\\mathrm{Var}(X)= 1/12\\), \\(\\mathrm{Var}(Y)=1/12\\), \\(\\mathrm{Cov}(X, Y)=0\\).\nZnajdź wariancję dla zmiennej losowej \\(X\\) o rozkładzie\n\\(\\mathrm{Pois}(\\lambda)\\), \\(\\lambda>0\\).\n\\(\\mathcal{U}[,b]\\), \\(<b\\)\n\\(\\mathrm{Exp}(\\lambda)\\), \\(\\lambda>0\\).\nZnajdź wariancję dla zmiennej losowej \\(X\\) o rozkładzie\\(\\mathrm{Pois}(\\lambda)\\), \\(\\lambda>0\\).\\(\\mathcal{U}[,b]\\), \\(<b\\)\\(\\mathrm{Exp}(\\lambda)\\), \\(\\lambda>0\\).\n\\(\\lambda\\), b \\((b-)^2/12\\), c \\(1/\\lambda^2\\)\nWykaż, że jeżeli \\(X\\) jest zmienną losową całkowalną z kwadratem, :\n\\(\\mathbb{V}ar[cX] = c^2 \\mathbb{V}ar[X]\\) dla każdego \\(c \\\\mathbb{R}\\);\n\\(\\mathbb{V}ar[X + ] = \\mathbb{V}ar[X]\\) dla każdego \\(\\\\mathbb{R}\\);\n\\(\\mathbb{V}ar[X] = 0\\) wtedy tylko wtedy, gdy zmienna losowa \\(X\\) jest stała z prawdopodobieństwem 1.\nWykaż, że jeżeli \\(X\\) jest zmienną losową całkowalną z kwadratem, :\\(\\mathbb{V}ar[cX] = c^2 \\mathbb{V}ar[X]\\) dla każdego \\(c \\\\mathbb{R}\\);\\(\\mathbb{V}ar[X + ] = \\mathbb{V}ar[X]\\) dla każdego \\(\\\\mathbb{R}\\);\\(\\mathbb{V}ar[X] = 0\\) wtedy tylko wtedy, gdy zmienna losowa \\(X\\) jest stała z prawdopodobieństwem 1.\n\n\\[\\begin{align*}\n\\mathbb{V}ar(cX) & = \\mathbb{E}[(cX)^2] - (\\mathbb{E}[cX])^2 \\\\\n& = c^2 \\mathbb{E}[X^2] - c^2 (\\mathbb{E}[X])^2 \\\\\n& = c^2 (\\mathbb{E}[X^2] - (\\mathbb{E}[X])^2) \\\\\n&= c^2 \\mathbb{V}ar(X)\n\\end{align*}\\]\nb\n\\[\\begin{align*}\n\\mathbb{V}ar(X + ) & = \\mathbb{E}[(X + )^2] - (\\mathbb{E}[X + ])^2 \\\\\n&= \\mathbb{E}[X^2 + 2aX + ^2] - (\\mathbb{E}[X] + )^2\\\\\n&= \\mathbb{E}[X^2] + 2a\\mathbb{E}[X] + ^2 - (\\mathbb{E}[X]^2 + 2a\\mathbb{E}[X] + ^2)\\\\\n&= \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 = \\mathbb{V}ar(X)\n\\end{align*}\\]\nc (\\(\\Rightarrow\\)) Jeśli \\(\\text{Var}(X) = 0\\), całka z funkcji nieujemnej \\((X-\\mathbb{E}[X])^2\\) jest równa zero.\nOznacza , że owa funkcja jest równa zero p.w. Czyli \\(X =\\mathbb{E}[X]\\) p.w.\n(\\(\\Leftarrow\\)) Jeśli \\(X = c\\) z prawdopodobieństwem 1, :\n\\[\n\\text{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 = c^2 - c^2 = 0\n\\]\nNiech \\(X\\), \\(Y\\) \\(Z\\) będą zmiennymi losowymi całkowalnymi z kwadratem. Pokaż, że\n\\(\\mathrm{Cov}(X, Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y].\\)\n\\(\\mathrm{Cov}(X, X) = \\mathrm{Var}[X].\\)\n\\(\\mathrm{Cov}(X, Y) = \\mathrm{Cov}(Y, X).\\)\nKowariancja jest operatorem dwuliniowym:\n\\[\n\\mathrm{Cov}(aX + , Z) = \\,\\mathrm{Cov}(X, Z) + b\\,\\mathrm{Cov}(Y, Z).\n\\]\nNiech \\(X\\), \\(Y\\) \\(Z\\) będą zmiennymi losowymi całkowalnymi z kwadratem. Pokaż, że\\(\\mathrm{Cov}(X, Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y].\\)\\(\\mathrm{Cov}(X, X) = \\mathrm{Var}[X].\\)\\(\\mathrm{Cov}(X, Y) = \\mathrm{Cov}(Y, X).\\)Kowariancja jest operatorem dwuliniowym:\n\\[\n\\mathrm{Cov}(aX + , Z) = \\,\\mathrm{Cov}(X, Z) + b\\,\\mathrm{Cov}(Y, Z).\n\\]\nZ definicji kowariancji:\n\\[\\begin{align*}\n\\text{Cov}(X, Y) &  = \\mathbb{E}[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])]\\\\\n& = \\mathbb{E}[XY - X\\mathbb{E}[Y] - \\mathbb{E}[X]Y + \\mathbb{E}[X]\\mathbb{E}[Y]]\\\\\n& = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y] - \\mathbb{E}[X]\\mathbb{E}[Y] + \\mathbb{E}[X]\\mathbb{E}[Y] \\\\\n& = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]\n\\end{align*}\\]\nb\n\\[\\begin{align*}\n\\text{Cov}(X, X) & = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 = \\text{Var}(X)\n\\end{align*}\\]\nc Z definicji:\n\\[\\begin{align*}\n\\text{Cov}(X, Y) & = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]\\\\\n\\text{Cov}(Y, X) & = \\mathbb{E}[YX] - \\mathbb{E}[Y]\\mathbb{E}[X] \\\\\n& = \\text{Cov}(X, Y)\n\\end{align*}\\]\nd Niech \\(, b \\\\mathbb{R}\\). Udowodnijmy:\n\\[\\begin{align*}\n\\text{Cov}(aX + , Z) & = \\text{Cov}(X, Z) + b \\text{Cov}(Y, Z)\\\\\n& = \\mathbb{E}[(aX + )Z] - \\mathbb{E}[aX + ] \\mathbb{E}[Z]\\\\\n& = \\mathbb{E}[XZ] + b\\mathbb{E}[YZ] - (\\mathbb{E}[X] + b\\mathbb{E}[Y])\\mathbb{E}[Z]\\\\\n& = (\\mathbb{E}[XZ] - \\mathbb{E}[X]\\mathbb{E}[Z]) + b(\\mathbb{E}[YZ] - \\mathbb{E}[Y]\\mathbb{E}[Z])\\\\\n& = \\text{Cov}(X, Z) + b \\text{Cov}(Y, Z)\n\\end{align*}\\]\nLosujemy jednostajnie punkt \\(\\vec{X}=(X,Y)\\) z koła\n\\[ \\left\\{(x,y) \\\\mathbb{R}^2 \\: : \\: x^2+y^2-2x-4y\\leq -4  \\right\\}.\\]\nZnajdź \\(\\mathrm{Cov}(X,Y)\\).\n\\(0\\)\n","code":""},{"path":"lista-10-wariancja.html","id":"zadania-na-ćwiczenia-8","chapter":"Lista 10: Wariancja","heading":"Zadania na ćwiczenia","text":"Załóżmy, że wektor losowy \\(\\vec{X}= (X,Y)\\) ma dwuwymiarowy rozkład normalny.\nPokaż, że \\(X\\) \\(Y\\) są niezależne wtedy tylko wtedy, gdy \\(\\mathrm{Cov}(X,Y)=0\\).Załóżmy, że wektor losowy \\(\\vec{X}= (X,Y)\\) ma dwuwymiarowy rozkład normalny.\nPokaż, że \\(X\\) \\(Y\\) są niezależne wtedy tylko wtedy, gdy \\(\\mathrm{Cov}(X,Y)=0\\).Podaj przykład zmiennych losowych \\(X\\) \\(Y\\) takich, że \\(X\\) \\(Y\\) mają standardowy jednowymiarowy rozkład normalny,\n\\(\\mathrm{Cov}(X,Y)=0\\), ale \\(X\\) \\(Y\\) nie są niezależne.Podaj przykład zmiennych losowych \\(X\\) \\(Y\\) takich, że \\(X\\) \\(Y\\) mają standardowy jednowymiarowy rozkład normalny,\n\\(\\mathrm{Cov}(X,Y)=0\\), ale \\(X\\) \\(Y\\) nie są niezależne.Niech \\(X_1\\) \\(X_2\\) będą niezależnymi zmiennymi losowymi o rozkładzie \\(\\mathcal{N}(0,1)\\).\nWykaż, że zmienne losowe \\(\\frac{X_1 + X_2}{\\sqrt{2}}\\) \\(\\frac{X_1 - X_2}{\\sqrt{2}}\\)\nsą niezależne obie mają rozkład \\(\\mathcal{N}(0,1)\\).Niech \\(X_1\\) \\(X_2\\) będą niezależnymi zmiennymi losowymi o rozkładzie \\(\\mathcal{N}(0,1)\\).\nWykaż, że zmienne losowe \\(\\frac{X_1 + X_2}{\\sqrt{2}}\\) \\(\\frac{X_1 - X_2}{\\sqrt{2}}\\)\nsą niezależne obie mają rozkład \\(\\mathcal{N}(0,1)\\).Na płaszczyźnie zaznaczono punkty \\(p_1, \\ldots , p_n\\) w taki sposób, że żadne trzy nie są współliniowe.\nKażda para punktów została połączona odcinkiem z prawdopodobieństwem \\(p\\(0,1)\\). Niech \\(X\\)\noznacza liczbę powstałych trójkątów o wierzchołkach w punktach \\(p_1, \\ldots , p_n\\). Oblicz \\(\\mathbb{V}ar[X]\\).Na płaszczyźnie zaznaczono punkty \\(p_1, \\ldots , p_n\\) w taki sposób, że żadne trzy nie są współliniowe.\nKażda para punktów została połączona odcinkiem z prawdopodobieństwem \\(p\\(0,1)\\). Niech \\(X\\)\noznacza liczbę powstałych trójkątów o wierzchołkach w punktach \\(p_1, \\ldots , p_n\\). Oblicz \\(\\mathbb{V}ar[X]\\).Niech \\(X\\) będzie zmienną losową o rozkładzie \\(\\mathcal{N}(\\mu, \\sigma^2)\\). Pokaż, że dla każdej \\(\\theta \\\\mathbb{R}\\),\n\\[\\begin{equation*}\n\\mathbb{E}\\left[ e^{\\theta X} \\right] = e^{\\theta \\mu +\\theta^2\\sigma^2/2}.\n\\end{equation*}\\]Niech \\(X\\) będzie zmienną losową o rozkładzie \\(\\mathcal{N}(\\mu, \\sigma^2)\\). Pokaż, że dla każdej \\(\\theta \\\\mathbb{R}\\),\n\\[\\begin{equation*}\n\\mathbb{E}\\left[ e^{\\theta X} \\right] = e^{\\theta \\mu +\\theta^2\\sigma^2/2}.\n\\end{equation*}\\]Niech \\(\\vec{X}=(X,Y)\\) będzie wektorem losowym o dwuwymiarowym rozkładzie normalnym z parametrami \\(\\vec{m}=(0,0)\\) \n\\[\\begin{equation*}\n\\Sigma = \\left(\\begin{array}{cc} & \\rho \\\\ \\rho & b \\end{array} \\right)\n\\end{equation*}\\]\ndla \\(\\rho < \\sqrt{ab}\\). Znajdź rozkład zmiennej \\(X\\) wywnioskuj, że \\(\\mathbb{V}ar[X]=\\).Niech \\(\\vec{X}=(X,Y)\\) będzie wektorem losowym o dwuwymiarowym rozkładzie normalnym z parametrami \\(\\vec{m}=(0,0)\\) \n\\[\\begin{equation*}\n\\Sigma = \\left(\\begin{array}{cc} & \\rho \\\\ \\rho & b \\end{array} \\right)\n\\end{equation*}\\]\ndla \\(\\rho < \\sqrt{ab}\\). Znajdź rozkład zmiennej \\(X\\) wywnioskuj, że \\(\\mathbb{V}ar[X]=\\).Dla zmiennej losowej \\(X\\) całkowalnej z kwadratem zdarzenia \\(\\) o dodatnim prawdopodobieństwie definiujemy\nwariancję \\(X\\) pod warunkiem \\(\\) wzorem\n\\[\\begin{equation*}\n\\mathbb{V}ar[X|] = \\mathbb{E} \\left[ \\left. (X - \\mathbb{E}[X|])^2 \\right|\\right].\n\\end{equation*}\\]\nPokaż, że dla rozbicia \\(\\{A_j\\}_{j \\\\mathbb{N}}\\) zbioru \\(\\Omega\\) na zdarzenia o dodatnim prawdopodobieństwie zachodzi\n\\[\n\\mathbb{E}[X] = \\sum_i \\mathbb{P}[A_i] \\mathbb{E}[X|A_i]\n\\]\noraz\n\\[\n\\mathbb{V}ar(X) = \\sum_i \\mathbb{P}(A_i) \\cdot \\mathbb{V}ar(X \\mid A_i) + \\sum_i \\mathbb{P}(A_i) \\cdot\n\\left( \\mathbb{E}[X \\mid A_i] - \\mathbb{E}[X] \\right)^2.\n  \\]Dla zmiennej losowej \\(X\\) całkowalnej z kwadratem zdarzenia \\(\\) o dodatnim prawdopodobieństwie definiujemy\nwariancję \\(X\\) pod warunkiem \\(\\) wzorem\n\\[\\begin{equation*}\n\\mathbb{V}ar[X|] = \\mathbb{E} \\left[ \\left. (X - \\mathbb{E}[X|])^2 \\right|\\right].\n\\end{equation*}\\]\nPokaż, że dla rozbicia \\(\\{A_j\\}_{j \\\\mathbb{N}}\\) zbioru \\(\\Omega\\) na zdarzenia o dodatnim prawdopodobieństwie zachodzi\n\\[\n\\mathbb{E}[X] = \\sum_i \\mathbb{P}[A_i] \\mathbb{E}[X|A_i]\n\\]\noraz\n\\[\n\\mathbb{V}ar(X) = \\sum_i \\mathbb{P}(A_i) \\cdot \\mathbb{V}ar(X \\mid A_i) + \\sum_i \\mathbb{P}(A_i) \\cdot\n\\left( \\mathbb{E}[X \\mid A_i] - \\mathbb{E}[X] \\right)^2.\n  \\]Rzucamy kostką aż momentu otrzymania pierwszej jedynki. Niech \\(X\\) będzie sumą wyrzuconych oczek. Znajdź \\(\\mathbb{E}[X]\\)\noraz \\(\\mathbb{V}ar[X]\\).Rzucamy kostką aż momentu otrzymania pierwszej jedynki. Niech \\(X\\) będzie sumą wyrzuconych oczek. Znajdź \\(\\mathbb{E}[X]\\)\noraz \\(\\mathbb{V}ar[X]\\).","code":""},{"path":"lista-10-wariancja.html","id":"zadania-dodatkowe-7","chapter":"Lista 10: Wariancja","heading":"Zadania dodatkowe","text":"Niech \\(\\vec{X}\\) \\(\\vec{Y}\\) będą niezależnymi \\(d\\)-wymiarowymi wektorami losowymi z\nrozkładem normalnym o parametrach \\((0, \\ldots, 0)\\) macierzy kowariancji \\(I_d\\) (identyczność).Udowodnij, że dla dowolnych \\(f, g \\\\mathcal{C}_b^2(\\mathbb{R}^d)\\) zachodzi\n\\[\\begin{equation*}\n    \\mathrm{Cov}(f(X), g(X)) =\n    \\int_0^1 \\mathbb{E} \\left[\n    \\left\\langle \\nabla f(X), \\nabla g(\\alpha X + \\sqrt{1 - \\alpha^2} Y)\n    \\right\\rangle \\right] \\mathrm{d}\\alpha,\n    \\end{equation*}\\]\ngdzie \\(\\nabla f(x) = \\left( \\frac{\\partial f}{\\partial x_i}(x) \\right)\\).\nNajpierw sprawdź wzór dla \\(f(x) = e^{\\langle t, x \\rangle}\\) oraz \\(g(x) = e^{\\langle s, x \\rangle}\\),\ngdzie \\(s, t, x \\\\mathbb{R}^d\\).Udowodnij, że dla dowolnych \\(f, g \\\\mathcal{C}_b^2(\\mathbb{R}^d)\\) zachodzi\n\\[\\begin{equation*}\n    \\mathrm{Cov}(f(X), g(X)) =\n    \\int_0^1 \\mathbb{E} \\left[\n    \\left\\langle \\nabla f(X), \\nabla g(\\alpha X + \\sqrt{1 - \\alpha^2} Y)\n    \\right\\rangle \\right] \\mathrm{d}\\alpha,\n    \\end{equation*}\\]\ngdzie \\(\\nabla f(x) = \\left( \\frac{\\partial f}{\\partial x_i}(x) \\right)\\).\nNajpierw sprawdź wzór dla \\(f(x) = e^{\\langle t, x \\rangle}\\) oraz \\(g(x) = e^{\\langle s, x \\rangle}\\),\ngdzie \\(s, t, x \\\\mathbb{R}^d\\).Niech \\(\\mu_\\alpha\\) będzie miarą probabilistyczną na \\(\\mathbb{R}^{2d}\\),\nktóra jest rozkładem wektora\n\\[\n    \\left(X, \\alpha X + \\sqrt{1 - \\alpha^2} Y\\right),\n    \\]\nniech \\(\\mu\\) oznacza miarą probabilistyczną daną przez\n\\[\n    \\int_0^1 \\mu_\\alpha \\, \\mathrm{d}\\alpha.\n    \\]\nNiech \\(Z\\) będzie wektorem losowym w \\(\\mathbb{R}^d\\) takim, że wektor \\((X, Z)\\) w \\(\\mathbb{R}^{2d}\\) ma rozkład \\(\\mu\\).\nUdowodnij, że dla każdej funkcji Lipschitza \\(f\\), takiej że \\(\\|f\\|_{\\text{Lip}} \\leq 1\\) oraz\n\\(\\mathbb{E}[f(X)] = 0\\), zachodzi nierówność\n\\[\n    \\mathbb{E}\\left[f(X)e^{t f(X)}\\right] \\leq t \\mathbb{E}\\left[e^{t f(Z)}\\right],\n    \\]\ndla wszystkich \\(t \\geq 0\\). Wywnioskuj, że\n\\[\n    \\mathbb{E}\\left[e^{t f(X)}\\right] \\leq e^{\\frac{t^2}{2}}.\n    \\]Niech \\(\\mu_\\alpha\\) będzie miarą probabilistyczną na \\(\\mathbb{R}^{2d}\\),\nktóra jest rozkładem wektora\n\\[\n    \\left(X, \\alpha X + \\sqrt{1 - \\alpha^2} Y\\right),\n    \\]\nniech \\(\\mu\\) oznacza miarą probabilistyczną daną przez\n\\[\n    \\int_0^1 \\mu_\\alpha \\, \\mathrm{d}\\alpha.\n    \\]\nNiech \\(Z\\) będzie wektorem losowym w \\(\\mathbb{R}^d\\) takim, że wektor \\((X, Z)\\) w \\(\\mathbb{R}^{2d}\\) ma rozkład \\(\\mu\\).\nUdowodnij, że dla każdej funkcji Lipschitza \\(f\\), takiej że \\(\\|f\\|_{\\text{Lip}} \\leq 1\\) oraz\n\\(\\mathbb{E}[f(X)] = 0\\), zachodzi nierówność\n\\[\n    \\mathbb{E}\\left[f(X)e^{t f(X)}\\right] \\leq t \\mathbb{E}\\left[e^{t f(Z)}\\right],\n    \\]\ndla wszystkich \\(t \\geq 0\\). Wywnioskuj, że\n\\[\n    \\mathbb{E}\\left[e^{t f(X)}\\right] \\leq e^{\\frac{t^2}{2}}.\n    \\]Mówimy, że funkcja \\(f : \\mathbb{R}^n \\\\mathbb{R}\\) jest ograniczona wielomianowo,\njeśli istnieją liczby całkowite \\(k = (k_1, \\ldots, k_n)\\) oraz liczba rzeczywista \\(> 0\\) takie, że\n\\[\n|f(x)| \\leq |x_1|^{k_1} \\cdots |x_n|^{k_n}\n   \\]\ndla każdego \\(x = (x_1, \\ldots, x_n)\\) takiego, że \\(\\|x\\| \\geq \\).Udowodnij, że jeśli \\(G\\) jest zmienną losową o rozkładzie normalnym o średniej zero,\ndla każdej ograniczonej wielomianowo różniczkowalnej w sposób ciągły funkcji \\(\\Phi\\) zachodzi:\n\\[\n    \\mathbb{E}[G \\Phi(G)] = \\mathbb{E}\\left[G^2\\right] \\, \\mathbb{E}[\\Phi'(G)].\n    \\]Udowodnij, że jeśli \\(G\\) jest zmienną losową o rozkładzie normalnym o średniej zero,\ndla każdej ograniczonej wielomianowo różniczkowalnej w sposób ciągły funkcji \\(\\Phi\\) zachodzi:\n\\[\n    \\mathbb{E}[G \\Phi(G)] = \\mathbb{E}\\left[G^2\\right] \\, \\mathbb{E}[\\Phi'(G)].\n    \\]Udowodnij, że jeśli \\((G, G_1, G_2, \\ldots, G_n)\\) jest\n\\((n+1)\\)-wymiarowym wektorem losowym o rozkładzie normalnym, \\(\\mathbb{E}[G]=\\mathbb{E}[G_i]=0\\),\ndla każdej ograniczonej wielomianowo różniczkowalnej w sposób ciągły funkcji\n\\[\n    \\Phi : \\mathbb{R}^n \\\\mathbb{R},\n    \\]\nzachodzi\n\\[\n    \\mathbb{E}\\left[G \\Phi(G_1, \\ldots, G_n)\\right] =\n    \\sum_{l \\leq n} \\mathbb{E}[G G_l] \\, \\mathbb{E}\\left[\\frac{\\partial \\Phi}{\\partial x_l}(G_1, \\ldots, G_n)\\right].\n    \\]Udowodnij, że jeśli \\((G, G_1, G_2, \\ldots, G_n)\\) jest\n\\((n+1)\\)-wymiarowym wektorem losowym o rozkładzie normalnym, \\(\\mathbb{E}[G]=\\mathbb{E}[G_i]=0\\),\ndla każdej ograniczonej wielomianowo różniczkowalnej w sposób ciągły funkcji\n\\[\n    \\Phi : \\mathbb{R}^n \\\\mathbb{R},\n    \\]\nzachodzi\n\\[\n    \\mathbb{E}\\left[G \\Phi(G_1, \\ldots, G_n)\\right] =\n    \\sum_{l \\leq n} \\mathbb{E}[G G_l] \\, \\mathbb{E}\\left[\\frac{\\partial \\Phi}{\\partial x_l}(G_1, \\ldots, G_n)\\right].\n    \\]","code":""}]
