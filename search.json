[{"path":"index.html","id":"section","chapter":"","heading":"","text":"Notatki te powstały na potrzeby kursu o tej samej nazwie, prowadzonego w Instytucie Matematycznym Uniwersytetu\nWrocławskiego w semestrze letnim 2024/2025.\nPierwsza wersja tych notatek jak plan merytoryczny wykładu zostały przygotowane przez DB w latach 2020-2024.\nInteraktywna wersja notatek została przygotowana przez PD w semestrze letnim roku akademickiego 2024/2025.Interaktywny charakter notatek był zainspirowany książkami Complex Analysis autorstwa Juana Carlosa Ponce Campuzano oraz Collision Detection Jeffreya Thompsona.Notatki te wciąż znajdują się wstępnej fazie mogą zawierać błędy. Jeśli zauważysz jakiekolwiek nieścisłości, niedociągnięcia lub inne problemy, gorąco zachęcam ich zgłaszania na\nGitHubie.napisania książki wykorzystałem bibliotekę bookdown z pakietu R, symulacje zostały stworzone przy użyciu javascript (p5.js). Rysunki w przeważającej większości są generowane przy użyciu TikZ.Wrocław, luty 2025Piotr Dyszewski","code":""},{"path":"sylabus.html","id":"sylabus","chapter":"Sylabus","heading":"Sylabus","text":"","code":""},{"path":"sylabus.html","id":"dane-dotyczące-przedmiotu","chapter":"Sylabus","heading":"Dane dotyczące przedmiotu","text":"Nazwa przedmiotu: Rachunek Prawdopodobieństwa 1RJednostka oferująca przedmiot: Instytut MatematycznyZałożenia: Analiza topologia R (28-MT-S-oAnTopR), (Kombinatoryka 28-MT-S-oKomb)Strona www: https://sites.google.com/site/piotrdyszewski/teaching/RPR1Forma zajęć: wykład + ćwiczeniaPunkty ECTS: 7Sprawdziany pisemne: 7.04 9.06","code":""},{"path":"sylabus.html","id":"skrócony-plan-wykładu","chapter":"Sylabus","heading":"Skrócony plan wykładu","text":"W trakcie wykładu poruszymy następujące zagadnienia:) Przestrzeń probabilistyczna: aksjomatyka rachunku prawdopodobieństwa, miara probabilistyczna,\nwłasności miary probabilistycznej, prawdopodobieństwo warunkowe,\nprawdopodobieństwo całkowite, wzór Bayesa, niezależność zdarzeń, lemat Borela-Cantellego;B) elementy losowe: zmienne losowe, wektory losowe, niezależne zmienne losowe;C) rozkłady prawdopodobieństwa:\nrozkłady zmiennych losowych, dystrybuanty, rozkłady dyskretne absolutnie ciągłe;D) parametry rozkładów: wartość oczekiwana, wariancja, kowariancja, funkcje charakterystyczne;E) twierdzenia graniczne: zbieżność zmiennych losowych, prawa wielkich liczb,\nprawo 0-1 Kołmogorowa mocne słabe prawo wielkich liczb, twierdzenie de Moivre’-Laplace’,\ncentralne twierdzenie graniczne.Podstawowa literatura wykładu:Durrett, R. (2019). Probability: theory examples (Vol. 49). Cambridge university press.Billingsley, P. (2017). Probability measure. John Wiley & Sons.Jakubowski, J., & Sztencel, R. (2001). Wstęp teorii prawdopodobieństwa. Script.","code":""},{"path":"sylabus.html","id":"szczegółowy-plan-wykładu","chapter":"Sylabus","heading":"Szczegółowy plan wykładu","text":"Wstępny plan tematów poruszanych na poszczególnych wykładach:Aksjomatyka rachunku prawdopodobieństwaPrawdopodobieństwo warunkowe, niezależność zdarzeńLemat Borela-CantellegoZmienne losoweRozkłady zmiennych losowychWektory losowe, niezależność zmiennych losowychParametry rozkładówNierówności związane z momentami, słabe prawo wielkich liczbZbieżność zmiennych losowych, prawo \\(0-1\\) KołmogorowaMocne prawo wielkich liczbTwierdzenie de Moivre’-Laplace’aZbieżność według rozkładuFunkcje charakterystyczneCentralne twierdzenie graniczneZastosowania CTG, rozkłady stabilne","code":""},{"path":"sylabus.html","id":"efekty-kształcenia","chapter":"Sylabus","heading":"Efekty kształcenia","text":"Po wykładzie student:Wymienia definiuje podstawowe obiekty teorii prawdopodobieństwa (, B, C, D );Podaje związki między podstawowymi obiektami teorii prawdopodobieństwa (, B, C, D );Wykorzystuje narzędzia teorii prawdopodobieństwa opisu zmiennych losowych w terminach ich rozkładu ich parametrów (B, C, D);Formułuje twierdzenia graniczne (D);Bada zmienne losowe pod kątem zależności (, B);Analizuje ciąg zmiennych losowych pod kątem różnych rodzajów zbieżności (B, E);Stosuje twierdzenia graniczne analizy ciągów zmiennych losowych (B, E);Stosuje nierówności lemat Borela-Cantelliego w analizie ciągów zmiennych losowych (, B, E);Stosuje metodę funkcji charakterystycznej dowodzenia twierdzeń granicznych (D, E).","code":""},{"path":"sylabus.html","id":"sposób-weryfikacji-efektów-kształcenia","chapter":"Sylabus","heading":"Sposób weryfikacji efektów kształcenia","text":"Na zaliczenie składać się będą:Aktywność na ćwiczeniach;Dwa sprawdziany pisemne (7.04 9.06).","code":""},{"path":"sylabus.html","id":"metody-i-kryteria-oceniania","chapter":"Sylabus","heading":"Metody i kryteria oceniania","text":"Zaliczenie ćwiczeń na sprawdzianów pisemnych.\nAktywności w czasie zajęć podnosi ocenę z ćwiczeń o \\(1\\) lub \\(0.5\\).\nOcena z egzaminu wystawiona jest na podstawie egzaminu pisemnego.","code":""},{"path":"sylabus.html","id":"warunkiem-zaliczenia-przedmiotu-jest","chapter":"Sylabus","heading":"Warunkiem zaliczenia przedmiotu jest:","text":"Uzyskanie 30% punktów za zadania stanowiące bieżącą weryfikację efektów kształcenia;Uzyskanie pozytywnej oceny z egzaminu stanowiącego końcową weryfikację efektów kształcenia.","code":""},{"path":"sylabus.html","id":"kryteria-ocen","chapter":"Sylabus","heading":"Kryteria ocen:","text":"(dst) student realizuje punkty 1-4 efektów kształcenia(db) student realizuje punkty 1-7 efektów kształcenia(bdb) student realizuje punkty 1-9 efektów kształceniaWrocław, luty 2025\nPiotr Dyszewski","code":""},{"path":"wprowadzenie.html","id":"wprowadzenie","chapter":"1 Wprowadzenie","heading":"1 Wprowadzenie","text":"Każdy wykład z rachunku prawdopodobieństwa zaczyna się ogólnym stwierdzeniem, że jest dział\nmatematyki zajmujący się badaniem zdarzeń losowych. Ważne jednak jest doprecyzowanie\no jaką analizę zdarzeń losowych chodzi jakiego rodzaju aparat matematyczny jest tego potrzebny.\nJeżeli ograniczymy się jedynie badania prawdopodobieństwa poszczególnych zdarzeń\ntakich jak jedenaście osób trafiło “szóstkę” w Lotto. Znajomość samej wartości liczbowej, na dodatek\nbardzo małej trudnej wyobrażenia, niezbyt wiele użytecznych informacji nie pozwoli udzielić\nodpowiedzi na wiele naturalnych pytań. Jak często dochodzi takich zdarzeń?\nJakie powinny być stawki za poszczególne zakłady aby Lotto było opłacalne dla Totalizatora\nSportowego?Okazuje się, że mimo iż nie mamy sposobu na przewidzenie wyniku zdarzenia czy eksperymentu\nlosowego, jesteśmy w stanie opisać pewne deterministyczne zależności między nimi.\nZależności te pozwalają między innymi dobrze zaplanować cennik Lotto.\nOmówimy teraz pokrótce kilka konkretnych przykładów aby dokładniej zobrazować zależności\no których będziemy w trakcie wykładu mówić.","code":""},{"path":"wprowadzenie.html","id":"krzywa-dzwonowa","chapter":"1 Wprowadzenie","heading":"1.1 Krzywa dzwonowa","text":"Pierwszy przykład pochodzi z Tokio, gdzie w październiku 2024 roku odbył\nsię Hakone Ekiden Yosenkai - półmaraton (bieg na 21.0975 km) kwalifikacyjny wyścigu\nHakone Ekiden (bieg sztafetowy). Oczywiście spodziewamy się, że znakomita większość\nzawodników uplasuje się w połowie stawki kilka najszybszych lub najwolniejszych osób będzie\nodstawało odpowiednio na jej początku końcu.\nOkazuje się jednak, że dokładny rozkład biegaczy jest bardzo regularny.\nZaobserwowana krzywa funkcja dzwonowa, która\njest równa\n\\[\\begin{equation*}\n    \\exp \\left(-x^2/(2\\sigma^2)\\right)/\\sqrt{2\\pi\\sigma^2}\n\\end{equation*}\\]\ndla pewnego parametru \\(\\sigma>0\\).\nW trakcie wykładu zobaczymy dokładnie skąd bierze się\nzaobserwowany kształt. Poniżej przedstawiamy\nwykres dla \\(\\sigma=1/2\\).","code":""},{"path":"wprowadzenie.html","id":"deska-galtona","chapter":"1 Wprowadzenie","heading":"1.2 Deska Galtona","text":"Istnieje wiele innych przykładów, w których obserwujemy krzywą dzwonową.\nAby zrozumieć jej pochodzenie warto przyjrzeć się najprostszym przykładom.\nJednym z nich jest deska Galtona zaprezentowana poniżej.\nKule spadają po kołkach za każdym razem odbijając się losowo w lewo bądź prawo.\nPoniżej widzimy histogram (znormalizowany wykres słupkowy)\nliczby kul, które opuściły deskę przez dane miejsce.Numery na histogramie mówią ile razy kula odbiła się od kołka w prawo.\nZauważmy, że po dłuższym czasie nasz histogram zaczyna przypominać krzywą dzwonową.\nPowyższe stanowi przykład Twierdzenia granicznego. Mimo, że nie jesteśmy w stanie przewidzieć wyniku\npojedynczego eksperymentu (nie wiemy gdzie dokładnie wyląduje ustalona kula),\nwiemy jak po uśrednieniu będzie zachowywał się cały system. W trakcie wykładu będziemy\nzajmować się opisem tego typu zjawisk.","code":""},{"path":"wprowadzenie.html","id":"jeszcze-jeden-przykład","chapter":"1 Wprowadzenie","heading":"1.3 Jeszcze jeden przykład","text":"Ideą twierdzeń granicznych jest wyodrębnienie deterministycznego\nstwierdzenia o losowym układzie.\nNie wszystkie twierdzenia graniczne muszą się wiązać z krzywą dzwonową.\nLosowe parkietaże, które teraz krótko omówimy, wiążą się z innym dobrze znanym kształtem.Dla naturalnego \\(n\\) rozważmy szachownicę wymiarów \\(2n\\times 2n\\) z której usunięto\ncztery rogi (równoramienne trójkąty prostokątne o ramieniu \\(n-1\\)).\nDiament dla \\(n=4\\) wygląda następująco.Dla naturalnego \\(n\\) chcemy pokryć diament kostkami domina o wymiarach\n\\(2\\times 1\\) oraz \\(1\\times 2\\). Poniżej znajduje się skrypt\ngenerujący losowy parkietaż diamentu rzędu \\(n\\).Widzimy, że dużych wartości \\(n\\) parkietaż wygląda bardzo regularnie. Rogi parkietażu są\nmonochromatyczne, co oznacza, że północny południowy róg są wyłożone kostkami poziomymi \nwschodni zachodni pionowymi. Widzimy też. że kolorowy fragment parkietażu\n(tam, gdzie widzimy zarówno kostki poziome jak kostki pionowe) zbiega okręgu.Dokładne opisanie powyższego zjawiska wykracza poza ramy tego wykładu. Zainteresowanym polecamy przeanalizowanie algorytmu generującego losowe permutacje (przycisk Auto). Algorytm ten tłumaczy dlaczego rogi\ndiamentu są monochromatyczne.","code":""},{"path":"wprowadzenie.html","id":"quiz","chapter":"1 Wprowadzenie","heading":"1.4 Quiz","text":"Niektóre sekcje w notatkach będą zakończone krótkim quizem aby czytelnik\nmógł sprawdzić swój poziom zrozumienia materiału.\nPrezentowane pytania będą zazwyczaj proste. Aby dokładnie zrozumieć materiał\nnależy przerobić listy zadań.","code":""},{"path":"aksjomatyka-rachunku-prawdopodobieństwa.html","id":"aksjomatyka-rachunku-prawdopodobieństwa","chapter":"2 Aksjomatyka rachunku prawdopodobieństwa","heading":"2 Aksjomatyka rachunku prawdopodobieństwa","text":"Omówimy podstawowe aksjomaty teorii prawdopodobieństwa. Zaczniemy jednak od przykładu\nilustrującego konieczność wprowadzenia matematycznego formalizmu.","code":""},{"path":"aksjomatyka-rachunku-prawdopodobieństwa.html","id":"paradoks-bertranda","chapter":"2 Aksjomatyka rachunku prawdopodobieństwa","heading":"2.1 Paradoks Bertranda","text":"W rachunku prawdopodobieństwa można wiele powiedzieć na poziomie intuicyjnym.\nOkazuje się jednak, że bez odpowiedniego formalizmu łatwo jest popaść w kłopoty.\nAby zilustrować rozważmy klasyczne zjawisko zwane paradoksem Bertranda.\nW okręgu o promieniu \\(1\\) wybrano losowo cięciwę \\(AB\\). Jakie jest prawdopodobieństwo,\nże będzie ona dłuższa niż bok trójkąta\nrównobocznego wpisanego w ten okrąg?Cięciwa wyznaczona jest przez dwa swoje końce. Skoro\nokrąg jest niezmienniczy na obroty, zawsze możemy umieścić\ninteresujący nas trójkąt równoboczny tak, aby jeden w końców\ncięciwy \\(\\) znajdował się dokładnie w wierzchołku trójkąta.\nWylosowanie cięciwy można więc utożsamić z wylosowaniem\ndrugiego punktu na okręgu.Cięciwa \\(AB\\) spełnia zadany warunek wtedy tylko wtedy, gdy\npunkt \\(B\\) trafia w łuk wyznaczony przez bok trójkąta\nprzeciwległy \\(\\). Skoro trójkąt dzieli okrąg na\ntrzy łuki równej długości, szukane prawdopodobieństwo wynosi \\(1/3\\).Istnieje inny sposób podejścia tego problemu. Cięciwa jest jednoznacznie wyznaczona przez\npołożenie swojego środka, więc wylosowanie jej jest równoważne z wylosowaniem jej środka.Cięciwa spełnia warunki zadania, gdy jej środek leży wnętrzu\nkoła o promieniu \\(1/2\\) tym samym środku. Prawdopodobieństwo, że\ncięciwa \\(AB\\) spełnia warunki zadania jest zatem\nrówne ilorazowi pół obu kół\n\\[\n    \\frac{|B(0,1/2)|}{|B(0,1)|}  = \\frac 14.\n\\]\nDokładne uzasadnienie słuszności powyższego wyrażenia wymaga wprowadzenia odpowiedniego\nformalizmu. Widzimy jednak, że o wiele większym problemem jest , że otrzymaliśmy\ninny wynik niż poprzednio.\nOkazuje się jednak, że można wprowadzić więcej chaosu.\nPodobnie jak w poprzednim przypadku chcemy wylosować środek cięciwy.\nZauważmy, że istotna jest jedynie jej długość, ta zależy tylko od odległości środka\ncięciwy od środka okręgu.\nPodobnie jak poprzednio wnioskujemy więc, że\n\\[\n    \\frac{|(0,1/2)|}{|(0,1)|}  = \\frac 12.\n\\]\nOtrzymaliśmy trzy różne odpowiedzi, co tłumaczy dlaczego zjawisko nazywane jest niekiedy\nparadoksem. Sprzeczność jest jednak pozorna. Jak się okazuje powyższe trzy metody losowania nie są\nrównoważne. Aby się o tym przekonać możemy powtórzyć powyższe eksperymenty wiele razy porównać rezultaty.\nJeżeli sto razy wylosujemy cięciwę przez wylosowanie jej końców na okręgu otrzymamy następujący rysunek.Widzimy, że w tym przypadku cięciwy umieszczone są równomiernie. Jeżeli natomiast wylosujemy sto\ncięciw poprzez wylosowanie ich środka otrzymamy więcej cięciw bliżej okręgu.Jeżeli wreszcie wylosujemy sto cięciw losując odległość od środka\nokręgu w sposób jednostajny, otrzymamy wiele cięciw blisko\nśrodka okręgu.Widzimy więc, że wspomniany paradoks jest pozorny, ponieważ przedstawione\nsposoby losowania cięciw nie są równoważne. Aby uniknąć tego typu niejasności\njak formalnie uzasadnić wyniki teoretyczne otrzymane przy trzech sposobach losowania,\nmusimy wprowadzić odpowiedni język opisu zdarzeń losowych.","code":""},{"path":"aksjomatyka-rachunku-prawdopodobieństwa.html","id":"przestrzeń-probabilistyczna","chapter":"2 Aksjomatyka rachunku prawdopodobieństwa","heading":"2.2 Przestrzeń probabilistyczna","text":"Za każdym razem kiedy rzucamy kością czy losujemy kartę z talii wykonujemy pewnego rodzaju\neksperyment losowy. W pierwszej części wykładu zajmiemy się ich badaniem.\nZaczniemy od sposobu matematycznej reprezentacji wyniku naszego eksperymentu.\nKażdy możliwy wynik eksperymentu nazywać będziemy zdarzeniem elementarnym.\nPrzykładowo dla rzutu kością sześcienną zdarzeniem elementarnym będzie\nliczba wyrzuconych oczek, czyli dowolna z liczb\n\\(1,2, \\ldots 6\\).\nZbiór wszystkich zdarzeń elementarnych oznaczać będziemy przez \\(\\Omega\\) \nbędziemy nazywać przestrzenią zdarzeń elementarnych, jego elementy,\nczyli zdarzenia elementarne będziemy oznaczać przez \\(\\omega\\).Przykład 2.1  Powiedzmy, że nasz eksperyment losowy polega na rzucie parą kości sześciennych.\nWynikiem naszego eksperymentu jest para elementów zbioru ze\n\\([6] = \\{1, 2, \\ldots , 6\\}\\).\nIstotne jest aby rozróżnić wyniki\notrzymane na obu kościach,\nwszak kości nie muszą być symetryczne. Zbiorem zdarzeń elementarnych jest\n\\[\n\\Omega = [6]^2=  \n\\{ (1,1), (1,2),\n\\ldots , (6,6) \\}.\n\\]\nW tym przypadku przestrzeń zdarzeń elementarnych jest skończona \nskłada się z \\(|\\Omega| = 36\\) elementów.Przykład 2.2  Tym razem rzucamy monetą momentu otrzymania orła.\nPrzestrzeń zdarzeń elementarnych \\(\\Omega\\) składa się ze skończonych ciągów \\(\\{O, R\\}\\)\nkończących się znakiem \\(O\\) jednego nieskończonego ciągu \\(R\\). Dokładniej\n\\[\\begin{align*}\n\\Omega & = \\{R\\}^{\\mathbb{N}} \\cup \\bigcup_{k=0}^\\infty\\{R \\}^k \\times \\{O\\}\n\\\\ & = \\{ RRRR \\ldots\\} \\cup \\{ O, RO, RRO, \\ldots  \\}.\n\\end{align*}\\]\nTutaj niekończony ciąg \\(RRR\\ldots\\) opisuje scenariusz, w którym nigdy nie wyrzucimy orła.Przykład 2.3  Losujemy liczbę z przedziału \\([0,1]\\). Wówczas wynikiem naszego losowania jest dokładnie\notrzymana liczba. Stąd \\(\\Omega=[0,1]\\).W większości eksperymentów losowych nie interesują nas poszczególne jego wyniki,\nlecz pewien ich wspólny aspekt. Dla przykładu w rzucie kością sześcienną istotna\ndla nas może być tylko parzystość uzyskanego wyniku.\nInnymi słowy bardziej od poszczególnych zdarzeń elementarnych interesować nas będę ich wyróżnione\nzbiory \\(\\subset \\Omega\\).\nTe interesujące nas zbiory nazywać będziemy zdarzeniami będziemy\noznaczać przez\n\\(,B,C\\) itd.\nDla określonego zbioru zdarzeń elementarnych \\(\\Omega\\)\nchcemy określić prawdopodobieństwo (miarę) \\(\\mathbb{P}\\).\nJeżeli \\(\\Omega\\) jest zbiorem skończonym lub przeliczalnym,\nzazwyczaj możemy przypisać prawdopodobieństwo dowolnemu podzbiorowi\n\\(\\Omega\\), więc dowolnemu elementowi\n\\(2^\\Omega\\), rodziny wszystkich podzbiorów \\(\\Omega\\).\nJeżeli \\(\\Omega\\) jest zbiorem nieprzeliczalnym, \\(2^\\Omega\\) jest zbyt duże\npojawiają się problemy ze zdefiniowaniem na \\(\\Omega\\) prawdopodobieństwa.\nJesteśmy zatem zmuszeni ograniczenia się pewnej\nszczególnej rodziny \\(\\mathcal{F} \\subseteq 2^\\Omega\\) zbiorów, dla których\njesteśmy w stanie określić prawdopodobieństwo.\nRodzina \\(\\mathcal{F}\\) powinna być zamknięta na wykonywanie podstawowych operacji na zbiorach.\nOkazuje się, że należy założyć, że \\(\\mathcal{F}\\) jest \\(\\sigma\\)-ciałem.Definicja 2.1  Rodzinę \\(\\mathcal{F}\\) podzbiorów \\(\\Omega\\) nazywamy \\(\\sigma\\)-ciałem, jeżeli\\(\\emptyset\\\\mathcal{F}\\);jeżeli \\(\\\\mathcal{F}\\), \\(^c\\\\mathcal{F}\\);jeżeli \\(A_1,A_2,\\ldots\\\\mathcal{F}\\), \\(\\bigcup_{=1}^\\infty A_i \\\\mathcal{F}\\).Parę \\((\\Omega,\\mathcal{F})\\) nazywamy przestrzenią mierzalną. Elementy \\(\\mathcal{F}\\)\nnazywamy zdarzeniami.Przykład 2.4  Wróćmy rzutu parą kości, gdzie przestrzenią zdarzeń elementarnych jest \\(\\Omega = [6]^2\\).\nWówczas zdarzeniem jest\n\\[\\begin{align*}\n& = \\{ \\mbox{suma oczek jest równa 7} \\} \\\\\n& = \\{ (1,6), (2,5),  \\ldots , (6,1)  \\} \\subseteq \\Omega.\n\\end{align*}\\]\nNatomiast zbiór\n\\[\n\\{ (1,1), (2,2) , \\ldots , (6,6)\\}\n\\]\njest zdarzeniem \\(\\{\\mbox{wypadł dublet}\\}\\).Przykład 2.5  Alex układa swoją dwudziestosześciotomową encyklopedię na półce.\nEksperyment polega na umieszczeniu poszczególnych tomów encyklopedii w losowej kolejności.\nWtedy zbiór zdarzeń elementarnych możemy utożsamić ze zbiorem permutacji. Dla \\(n \\\\mathbb{N}\\)\ndefiniujemy\n\\[\\begin{equation*}\n    S_{n} = \\left\\{ \\pi \\colon [n] \\[n]\\: : \\: \\mbox{$\\pi$ jest $1-1$ ''na''} \\right\\}.\n        \\end{equation*}\\]\nKażdy element \\(\\omega \\S_n\\) możemy utożsamić z macierzą\n\\[\\begin{equation*}\n\\omega = \\left( \\begin{array}{ccccc}\n1 & 2 & 3 & \\ldots & n\\\\\n\\omega(1) & \\omega(2) & \\omega(3) & \\ldots & \\omega(n) \\end{array} \\right),\n\\end{equation*}\\]\ngdzie \\(\\omega(1), \\omega(2), \\ldots , \\omega(n)\\) są parami różnymi liczbami ze zbioru\n\\([n]=\\{1,2,\\ldots n\\}\\).\nWówczas w naszym przykładzie \\(\\Omega=S_{26}\\).\nPrzykładowo ułożenie tomów w kolejności alfabetycznej odpowiada permutacji\nidentycznościowej \\({\\rm id} \\\\Omega\\) czyli takiej, że\\({\\rm id}()=\\) dla \\(\\[26]\\).\nInnymi słowy\n\\[\\begin{equation*}\n{\\rm id} =\n\\left( \\begin{array}{ccccc} 1 & 2 & 3 & \\ldots & n\\\\ 1 & 2 & 3 & \\ldots & n \\end{array} \\right).\n\\end{equation*}\\]Przykład 2.6  Andrzej układa sagę o Wiedźminie na półce.\nWówczas tomy możemy ponumerować liczbami \\(1,2,3,4,5\\) według daty wydania.\nZbiorem zdarzeń elementarnych jest zatem \\(\\Omega = S_5\\).\nRozważmy zdarzenie \\(\\) polegające na tym, że\npierwsze dwa tomy ułożone są chronologicznie.\nWówczas \\(\\) składa się z permutacji o reprezentacji\n\\[\\begin{equation*}\n\\left( \\begin{array}{ccccc} 1 & 2 & 3 & 4 & 5\\\\ 1 & 2 & * & * & * \\end{array} \\right).\n\\end{equation*}\\]Wprowadzone przez nas Definicja 2.1 zdarzenia jest niezwykle pojemna.\nWskutek czego jeżeli \\(A_1, A_2 \\ldots\\) jest ciągiem zdarzeń, czyli dla każdego \\(k \\\\mathbb{N}\\),\n\\(A_k\\) jest zdarzeniem, zdarzeniem jest również \\(\\bigcup_{k=1}^{\\infty} A_k\\)\noraz \\(\\bigcap_{k=1}^\\infty A_k\\).\nDodatkowo, jeżeli \\(\\) jest zdarzeniem, jest nim również\nzdarzenie przeciwne zdarzenia \\(\\), zadane wzorem\n\\[\n    ^c = \\Omega \\setminus .\n\\]\nZdarzenie \\(^c\\) polega na tym, że nie zachodzi zdarzenie \\(\\).\nJest jeszcze jedna relacja teoriomnogościowa, z której będziemy korzystać.\nPowiemy, że zdarzenie \\(B\\) pociąga zdarzenie \\(\\), jeżeli \\(B \\subseteq \\).Przykład 2.7  Powiedzmy, że rzucamy tylko jedną kością.\nWówczas \\(\\Omega = D = \\{ 1, 2, \\ldots , 6\\}\\). Niech\n\\[\\begin{align*}\n& = \\{\\mbox{wypadła liczba parzysta} \\} \\\\ & = \\{ 2, 4, 6\\}.\n\\end{align*}\\]\nZdarzenie przeciwne zdarzenia \\(\\) \n\\[\\begin{align*}\n^c & = \\{\\mbox{wypadła liczba nieparzysta} \\} \\\\ &= \\{ 1, 3, 5\\}.\n\\end{align*}\\]\nZ kolei zdarzenie\n\\[\nB = \\{\\mbox{wypadła szóstka} \\} = \\{ 6\\}\n\\]\npociąga zdarzenie \\(\\), czyli \\(B \\subseteq \\).Przykład 2.8  Losujemy punkt z przedziału \\([0,1]\\). Tak jak przyjęliśmy wcześniej \\(\\Omega=[0,1]\\).\nW przyszłości będziemy chcieli określić prawdopodobieństwo, że wylosowany punkt wpada \nustalonego przedziału. Przedziały postaci \\((,b)\\) będą więc dla nas zdarzeniami.\nZ kursu teorii miary wiemy, że wówczas będziemy w stanie zmierzyć każdy zbiór, który jest\nrezultatem przeliczalnych operacji mnogościowych na przedziałach otwartych. Oznacza , że\n\\(\\mathcal{F} =\\mathcal{B}([0,1])\\) jest \\(\\sigma\\)-ciałem zbiorów borelowskich na \\([0,1]\\),\nczyli najmniejszym \\(\\sigma\\)-ciałem zawierającym wszystkie zbiory otwarte.\nPamiętajmy, że \\(\\mathcal{B}([0,1]) \\neq 2^{[0,1]}\\). Istnieją zatem \\(\\subseteq \\Omega\\) takie, że\n\\(\\notin \\mathcal{F}\\). Są zbiory, których nie będziemy w stanie zmierzyć. Dla zdarzenia\n\\(\\\\mathcal{F}\\) mamy\n\\[\\begin{equation*}\n    = \\{ \\omega \\\\Omega \\: : \\: \\omega \\\\}.\n\\end{equation*}\\]\nJest więc zdarzenie polegające na tym, że wylosowana liczba \\(\\omega\\) pochodzi ze zbioru \\(\\).","code":""},{"path":"aksjomatyka-rachunku-prawdopodobieństwa.html","id":"miara-probabilistyczna","chapter":"2 Aksjomatyka rachunku prawdopodobieństwa","heading":"2.3 Miara probabilistyczna","text":"Jesteśmy przyzwyczajeni intuicyjnego myślenia o prawdopodobieństwie.\nDla przykładu w eksperymencie polegającym na rzucie dwoma kostkami od razu napiszemy, że\n\\[\\begin{equation*}\n    \\mathbb{P}\\left[ \\mbox{na obu kościach wypadnie $1$} \\right] =\n    \\frac 1{36}.\n\\end{equation*}\\]\nW pewnym sensie o liczbie \\(\\mathbb{P}[]\\) myślimy jak o prawdopodobieństwie zdarzenia \\(\\)\nopierając się na słownym opisie zdarzenia \\(\\). Nie ma w tym nic złego,\nlecz w wielu przypadkach wygodniej jest myśleć o zdarzeniu \\(\\) jak o podzbiorze \\(\\Omega\\) \no liczbie \\(\\mathbb{P}[]\\) jak o masie zbioru \\(\\).Definicja 2.2  Niech \\((\\Omega,\\mathcal{F})\\) będzie przestrzenią mierzalną.\nFunkcję \\(\\mathbb{P}:\\mathcal{F}\\[0,1]\\) nazywamy prawdopodobieństwem\n(miarą probabilistyczną) jeżeli\\(\\mathbb{P}[\\Omega] = 1\\);\\(\\mathbb{P}\\left[\\bigcup_{=1}^\\infty A_i\\right] = \\sum_{=1}^\\infty \\mathbb{P}[A_i]\\)\ndla dowolnych parami rozłącznych zdarzeń \\(A_1,A_2,\\ldots \\\\mathcal{F}\\).Trójkę \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) nazywamy przestrzenią probabilistyczną.Aksjomatyzacja rachunku prawdopodobieństwa była jednym z problemów Hilberta.\nPowyższa definicja została podana przez Kołmogorowa w 1933 roku zapoczątkowała\nwspółczesny rachunek prawdopodobieństwa.\nWcześniej badano głównie dyskretne przestrzenie probabilistyczne\n(np. w celu zrozumienia gier hazardowych).\nZ biegiem czasu, na przełomie dziewiętnastego \ndwudziestego wieku, zaczęto badać jednak znacznie bardziej skomplikowane modele\n(np. pochodzące z fizyki statystycznej) brakowało matematycznych narzędzi\nich precyzyjnego opisu. Okazało się, że powyższa definicja jest właściwym punktem wyjścia.\nNależy jednak mieć świadomość, że np. pokazanie istnienia miary probabilistycznej\njest nietrywialne (odpowiednia konstrukcja pokazywana jest podczas kursu z teorii miary bazuje na tw. Caratheodory’ego).Przykład 2.9  (Prawdopodobieństwo klasyczne) Niech \\(\\Omega\\)\nbędzie skończonym zbiorem, \\(\\mathcal{F} = 2^\\Omega\\).\nWówczas wystarczy zdefiniować prawdopodobieństwo na zdarzeniach elementarnych\n(zbiorach jednoelementowych). Zazwyczaj zakłada się, że są one jednakowo prawdopodobne,\nco prowadzi \n\\[\n    \\mathbb{P}[\\{\\omega\\}] = \\frac 1{|\\Omega|},\n\\]\ngdzie \\(|\\Omega|\\) oznacza liczbę elementów zbioru \\(\\Omega\\).\nWówczas dla dowolnego \\(\\subseteq \\Omega\\):\n\\[\\begin{equation}\n\\mathbb{P}[] = \\frac{||}{|\\Omega|}.\n\\tag{2.1}\n\\end{equation}\\]Przykład 2.10  Rzucamy trzykrotnie symetryczną monetą. Interesuje nas prawdopodobieństwo, że\ndokładnie \\(2\\) razy wypadła reszka.\nMamy\n\\[\\begin{multline*}\n  \\Omega=\\left\\{ (O,O,O), (O,O,R), (O,R,O), (R,O,O),\\right. \\\\\n  \\left. (O,R,R), (R,O,R), (R,R,O), (R,R,R)   \\right\\}\n\\end{multline*}\\]\noraz \\(\\mathcal{F}= 2^{\\Omega}\\).\nSkoro moneta jest symetryczna, każdy wynik jest jednakowo prawdopodobny.\nZa miarę probabilistyczną \\(\\mathbb{P}\\)\nprzyjmujemy zatem prawdopodobieństwo klasyczne dane wzorem (2.1).\nInteresuje nas prawdopodobieństwo zdarzenia\n\\[\\begin{equation*}\n  = \\left\\{  (O,R,R), (R,O,R), (R,R,O)   \\right\\}.\n\\end{equation*}\\]\nSkoro \\(\\) jest trójelementowy, \\(\\mathbb{P}() = 3/8\\).Przykład 2.11  Jeśli rzucamy kością sześcienną, zbiorem zdarzeń elementarnych jest\n\\(\\Omega =[6]= \\{ 1, 2, \\ldots , 6\\}\\). Jeżeli kość jest dobrze wyważona, \nopisu tego doświadczenia użyjemy prawdopodobieństwa klasycznego \\(\\mathbb{P}_1\\) takiego, że\n\\[\n\\mathbb{P}_0[\\{ 1 \\}] = \\mathbb{P}_0[\\{ 2 \\}] = \\ldots = \\mathbb{P}_0[\\{ 6\\}] = 1/6.\n\\]\nMożemy też zakładać, że jedna kość jest niewyważona, przykładowo\n\\[\\begin{align*}\n\\mathbb{P}_1[\\{2\\}] & = \\mathbb{P}_1[\\{6\\}]= 1/4 \\\\\n\\mathbb{P}_1[\\{ 1\\}] & = \\mathbb{P}_1[\\{ 3\\}] = \\mathbb{P}_1[\\{4\\}] = \\mathbb{P}_1[\\{ 5 \\}] = 1/8.\n\\end{align*}\\]\nMiary \\(\\mathbb{P}_0\\) \\(\\mathbb{P}_1\\) są różne te zdarzenia będą miały względem nich\nróżne prawdopodobieństwa. Dla przykładu dla zdarzenia\n\\[\n= \\{\\mbox{wypadła liczba parzysta}\\} = \\{2, 4, 6\\}\n\\]\notrzymujemy \\(\\mathbb{P}_0[] = 1/2\\) oraz \\(\\mathbb{P}_1[]=5/8\\).Przykład 2.12  Załóżmy, że \\(\\Omega = \\{\\omega_1,\\omega_2,\\ldots,\\}\\)\njest zbiorem przeliczalnym niech \\(p_1,p_2,\\ldots\\) będą\nliczbami nieujemnymi o sumie \\(1\\).\nWtedy wybór \\(\\mathcal{F}=2^\\Omega\\) oraz \\(\\mathbb{P}[\\{\\omega_i\\}] = p_i\\) dla \\(\\\\mathbb{N}\\)\njednoznacznie definiuje miarę probabilistyczną. Mianowicie dla każdego\n\\(\\\\mathcal{F}\\) mamy\n\\[\\begin{equation}\n  \\mathbb{P}[] = \\sum_{: \\omega_i\\} p_i = \\sum_i p_i {\\bf 1}_A(\\omega_i).\n  \\tag{2.2}\n  \\end{equation}\\]\nTutaj \\({\\bf 1}_A\\) jest indykatorem zbioru \\(\\) zadanym przez\n\\[\\begin{equation*}\n{\\bf 1}_A(\\omega) = \\left\\{ \\begin{array}{cc} 1 & \\omega \\\\\\ 0 & \\omega\\notin \\end{array}\n\\right. .\n\\end{equation*}\\]\nNa tym przykładzie widać, że o prawdopodobieństwie \\(\\mathbb{P}\\) można (czasami nawet trzeba)\nmyśleć jak o masie. Każdy z punktów \\(\\Omega\\) ma pewną masę (punkt \\(\\omega_k\\) ma masę \\(p_k\\)).\nWówczas (2.2) jest po prostu zliczeniem masy wszystkich punktów znajdujących się\nw zbiorze \\(\\).Omówione tej pory przykłady dotyczą skończonych bądź przeliczalnych przestrzeni probabilistycznych.\nZdefiniowanie na nich prawdopodobieństwa jest stosunkowo łatwe, ponieważ można zrobić\ndla każdego punktu z osobna. Wprowadzona przez nas Definicja 2.2\njest na tyle pojemna, aby dopuścić przypadki większych przestrzeni. Z takimi\nprzestrzeniami będziemy się spotykali niejednokrotnie w trakcie wykładu. Najprostszą z\nnich jest odcinek jednostkowy.Przykład 2.13  (Wybór losowego punktu z odcinka [0,1]) Powiedzmy, że chcemy zdefiniować przestrzeń probabilistyczną \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\)\nopisującą losowanie punktu z odcinka \\([0,1]\\). Wówczas możemy przyjąć\n\\(\\Omega=[0,1]\\) oraz \\(\\mathcal{F}=\\mathcal{B}[0,1]\\) rodzinę zbiorów borelowskich \\([0,1]\\).\nJest najmniejsze \\(\\sigma\\)-ciało podzbiorów \\([0,1]\\) zawierające wszystkie otwarte podzbiory \\([0,1]\\).\nChcemy na \\((\\Omega, \\mathcal{F})\\) wprowadzić takie prawdopodobieństwo \\(\\mathbb{P}\\) aby (luźno mówiąc)\nwylosowanie każdego punktu było jednakowo prawdopodobne. Formalnie nie można\ntego zrobić wprost. Dokładniej nie jest możliwe zdefiniowanie prawdopodobieństwa tak,\naby punkty należące odcinka \\([0,1]\\) miały takie samo, niezerowe prawdopodobieństwo.\nWłaściwe jest pytanie o wylosowanie zbioru (borelowskiego).\nSzukamy prawdopodobieństwa takiego aby prawdopodobieństwo wylosowania (miara zbioru) postaci\n\\([, +\\epsilon)\\) nie zależała od \\(\\[0,1-\\epsilon)\\) (lub równoważnie prawdopodobieństwo było\nniezmiennicze na przesunięcia).\nJedynym wyborem jest \\(\\mathbb{P}\\) będące miarą Lebesgue’na \\([0,1]\\).\nJak widzimy na symulacji poniżej, rozrzut punktów w takim modelu\nrzeczywiście jest jednostajny na całym odcinku.Powyżej widzimy symulacje \\(100\\) punktów wylosowanych z \\([0,1]\\)\nw sposób jednostajny.\nZauważmy, że wówczas dla każdego \\(x\\[0,1]\\), \\(\\mathbb{P}[\\{x\\}] = 0\\).Przykład 2.14  Możemy powtórzyć poprzedni przykład dla równoramiennego trójkąta prostokątnego\n\\(T \\subseteq \\mathbb{R}^2\\). Wówczas \\(\\Omega=T\\), \\(\\mathcal{F}=\\mathcal{B}(T)\\).\nChcąc ponownie losować w sposób jednostajny za \\(\\mathbb{P}\\) wybierzemy\nodpowiednio unormowaną dwuwymiarową miarę Lebesgue’(tak aby \\(\\mathbb{P}[\\Omega]=1\\)).Przykład 2.15  Wróćmy jeszcze na chwilę, losowania liczb z przedziału \\([0,1]\\). Przypuśćmy jednak, że\n\\[\\begin{equation*}\n\\mathbb{P}[] = 2 \\int_A s \\: \\mathrm{d}s\n\\end{equation*}\\]\ndla dowolnego \\(\\\\mathcal{F}\\).Przy takim wyborze prawdopodobieństwa faworyzujemy\npunkty bliżej prawego końca odcinka. Dla \\(x, \\epsilon \\(0,1)\\) takich, że \\(x+\\epsilon<1\\)\nmamy bowiem\n\\[\\begin{equation*}\n    \\mathbb{P}[(x, x+\\epsilon)] = (x+\\epsilon)^2-x^2 = 2x\\epsilon +\\epsilon\n\\end{equation*}\\]\nco jest rosnącą funkcją \\(x\\).\nWidzimy też na poniższej symulacji.Twierdzenie 2.1  (Podstawowe własności prawdopodobieństwa) Niech \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) będzie przestrzenią\nprobabilistyczną oraz \\(,B,A_1,A_2,\\ldots \\\\mathcal{F}\\). Wtedy\\(\\mathbb{P}[\\emptyset] = 0\\).\\(\\mathbb{P}[^c] = 1-\\mathbb{P}[]\\).Jeżeli \\(\\subset B\\), \\(\\mathbb{P}[B\\setminus ] = \\mathbb{P}[B] - \\mathbb{P}[]\\),\nw szczególności \\(\\mathbb{P}[]\\le \\mathbb{P}[B]\\).\\(\\mathbb{P}[\\cup B] = \\mathbb{P}[] + \\mathbb{P}[B] - \\mathbb{P}[\\cap B]\\).\\(\\mathbb{P}[\\bigcup_{=1}^\\infty A_i] \\le \\sum_{=1}^\\infty \\mathbb{P}[A_i]\\).Proof. Pozostawiamy jako zadaniePrzykład 2.16  Romeo Julia umówili się na spotkanie między \\(12\\) \\(1\\) w nocy. Nie ustalili jednak\ndokładnej godziny spotkania. Osoba, która przyjdzie jako pierwsza może czekać co najwyżej\n\\(15\\) minut na drugą. Jakie jest prawdopodobieństwo, że dojdzie spotkania?\nKładziemy \\(\\Omega = [0,1]^2\\). Wówczas \\(\\omega = (x,y) \\\\Omega\\) reprezentuje czas\nprzyjścia pierwszej osoby przez \\(x\\) drugiej przez \\(y\\). \\(\\mathcal{F} = \\mathcal{B}([0,1]^2)\\).\nZdarzenie, że dojdzie spotkania \n\\[\\begin{equation*}\n    = \\{ \\omega= (x,y) \\\\Omega \\: : \\: |x-y|\\leq 1/4\\}.\n\\end{equation*}\\]\nZ reprezentacji graficznejłatwo znajdujemy prawdopodobieństwo zdarzenia przeciwnego \\(\\mathbb{P}[^c] = 9/16\\).\nStąd \\(\\mathbb{P}[] = 1-\\mathbb{P}[^c]=7/16\\).Przykład 2.17  Losujemy jedną z liczb \\(1, 2, \\ldots 1000\\) w taki sposób, że wylosowanie każdej z\nnich jest jednakowo prawdopodobne. Wówczas przestrzenią zdarzeń elementarnych jest\n\\[\\begin{equation*}\n\\Omega = \\{ 1, 2, \\ldots , 1000\\}.\n\\end{equation*}\\]\nJakie jest prawdopodobieństwo, że wylosowana liczba dzieli się przez \\(3\\) lub \\(5\\)?\nDla liczby \\(p\\) oznaczmy zdarzenie\n\\[\\begin{equation*}\n    D_{p} = \\{ n \\[1000] \\: | \\: \\mbox{$n$ dzieli się przez $p$} \\} \\subseteq \\Omega.\n\\end{equation*}\\]\nSzukamy \\(\\mathbb{P}[D_3\\cup D_5]\\). Zauważmy, że\n\\[\\begin{equation*}\n    D_{3} = \\{ 3m \\: | \\: 1\\leq m \\leq 333 \\},\n\\end{equation*}\\]\nwięc \\(\\mathbb{P}[D_3] = 333/1000\\). Podobnie \\(\\mathbb{P}[D_5] =1/5\\).\nSkoro \\(D_3 \\cap D_5 = D_{15}\\), \\(\\mathbb{P}[D_3 \\cap D_5] = 33/500\\).\nOstatecznie\n\\[\\begin{align*}\n    \\mathbb{P}[D_3 \\cup D_5] & = \\mathbb{P}[D_3]+\\mathbb{P}[D_5]-\\mathbb{P}[D_3\\cap D_5] \\\\&=\n    467/1000.\n\\end{align*}\\]Punkt 4. powyższego twierdzenia szczególny przypadek zasady włączeń wyłączeń.\nTen punkt można uogólnić na dowolną skończoną liczbę zbiorów. Dla trzech zbiorów mamy:\n\\[\\begin{align*}\n\\mathbb{P}[\\cup B\\cup C] & = \\mathbb{P}[]  +\\mathbb{P}[B] +\\mathbb{P}[C] + \\\\\n& - \\mathbb{P}[\\cap B] - \\mathbb{P}[\\cap C] - \\mathbb{P}[B\\cap C]\\\\ & + \\mathbb{P}[\\cap B\\cap C].\n\\end{align*}\\]Twierdzenie 2.2  (Zasada włączeń wyłączeń) Niech \\(n \\\\mathbb{N}\\). Jeżeli \\(A_1,A_2,\\ldots, A_n\\\\mathcal{F}\\), \n\\[\\begin{multline*}\n  \\mathbb{P}[A_1\\cup A_2\\cup \\ldots \\cup A_n] = \\sum_{=1}^n\\mathbb{P}[A_i]+ \\\\\n- \\sum_{<j}\\mathbb{P}[A_i\\cap A_j] + \\sum_{<j<k}\\mathbb{P}[A_i\\cap A_j\\cap A_k]\n  \\\\ + \\ldots +(-1)^{n+1} \\mathbb{P}[A_1\\cap A_2\\cap \\ldots \\cap A_n]\n  \\end{multline*}\\]Proof. Pozostawiamy jako ćwiczenie.Twierdzenie 2.3  (Twierdzenie o ciągłości) Niech \\((\\Omega, \\mathcal{F}, \\mathbb{P},\\) będzie przestrzenią probabilistyczną.\nRozważmy nieskończony ciąg zdarzeń \\(A_1,A_2,\\ldots \\\\mathcal{F}\\).Jeżeli ciąg \\(\\{A_n\\}_{n \\\\mathbb{N}}\\) jest wstępujący (tzn. \\(A_1\\subset A_2 \\subset \\ldots\\))\n\\(=\\bigcup_{=1}^\\infty A_i\\), \n\\[\n\\mathbb{P}[] = \\lim_{n\\\\infty} \\mathbb{P}[A_n].\n\\]Jeżeli ciąg \\(\\{A_n\\}_{n \\\\mathbb{N}}\\) jest zstępujący\n(tzn. \\(A_1\\supset A_2 \\supset \\ldots\\)) \\(=\\bigcap_{=1}^\\infty A_i\\), \n\\[\n\\mathbb{P}[] = \\lim_{n\\\\infty} \\mathbb{P}[A_n].\n\\]Proof. Punkt 1. Niech \\(B_1 = A_1\\), \\(B_2 = A_2\\setminus A_1\\), …, \\(B_n = A_n\\setminus A_{n-1}\\),\nwtedy zdarzenia \\(B_n\\) są rozłączne,\n\\[\n\\bigcup_{=1}^n B_i = A_n \\quad \\mbox{oraz } \\quad \\bigcup_{=1}^\\infty B_i =  .\n\\]\nNa mocy punktu drugiego Definicji 2.2 otrzymujemy\n\\[\\begin{align*}\n\\mathbb{P}[] & = \\mathbb{P}\\left[\\bigcup_{=1}^\\infty B_i\\right] =\n\\sum_{=1}^\\infty \\mathbb{P} [ B_i ] \\\\&= \\lim_{n\\\\infty} \\sum_{=1}^n \\mathbb{P}[B_i]\n= \\lim_{n\\\\infty} \\mathbb{P}[A_n].\n\\end{align*}\\]\nPunkt 2. Niech \\(C_i = A_i^c\\). Wtedy rodzina zdarzeń ciąg \\(\\{C_n\\}_{n \\\\mathbb{N}}\\)\njest wstępujący. Z prawa De Morgana\n\\[\n    \\bigcup_{=1}^\\infty C_i = \\bigcup_{=1}^\\infty ^c_i\n    = \\left( \\bigcap_{=1}^\\infty A_i \\right)^c = ^c.\n\\]\nKorzystając zatem z udowodnionego już punktu 1 otrzymujemy\n\\[\\begin{multline*}\n\\mathbb{P}[] = 1 - \\mathbb{P}[^c] = 1 - \\mathbb{P}[C] = \\\\\n1 - \\lim_{n\\\\infty} \\mathbb{P}[C_n] = \\lim_{n\\\\infty}\\mathbb{P}[\\Omega\\setminus C_n]\n= \\lim_{n\\\\infty} \\mathbb{P}[A_n].\n\\end{multline*}\\]Przykład 2.18  urny wrzucamy nieskończenie wiele kul o numerach \\(1,2,\\ldots\\) w\nnastępujący sposób (w każdym kroku wybieramy ustaloną opcję ), b) lub c)):o godz. (\\(12.00 - 1\\) min.) wrzucamy kule \\(1,2,\\ldots, 10\\);o godz. (\\(12.00 - 1/2\\) min.) wyciągamy ) kulę 10; b) kulę 1; c) losową kulę,\nnastępnie wrzucamy kule \\(11,12,\\ldots, 20\\);o godz. (\\(12.00 - 1/4\\) min.) wyciągamy ) kulę 20; b) kulę 2; c) losową kulę,\n~następnie\nwrzucamy kule \\(21,22,\\ldots, 30\\);…Ile kul będzie w urnie o godz. 12?\nJasne jest, że jeżeli zastosujemy opcję ), o 12 w urnie będzie nieskończenie wiele kul.\nTroszeczkę bardziej niepokojący jest fakt, że jeżeli zastosujemy opcję b) urna będzie pusta.Rozpatrzmy przypadek c). Niech \\(A_n\\) będzie zdarzeniem, że po \\(n\\) krokach kula \\(1\\)\njest wciąż w urnie (oczywiście \\(A_{n+1}\\subset A_n\\)). Wówczas\n\\[\n\\mathbb{P}[A_{n+1}]\n= \\frac{9}{10}\\cdot\\frac{18}{19}\\cdot\\frac{27}{28}\\cdot \\ldots  \\cdot\\frac{9n}{9n+1}.\n\\]\nChcemy obliczyć \\(\\mathbb{P}[\\bigcap_{n=1}^{\\infty}A_n]\\), więc prawdopodobieństwo,\nże kula \\(1\\) pozostanie w urnie. Z Twierdzenia 2.3 o ciągłości\nwynika\n\\[\n\\mathbb{P}\\bigg[\\bigcap_{n=1}^{\\infty}A_n\\bigg]\n= \\lim_{n\\\\infty} \\mathbb{P}[A_n]\n= \\prod_{n=1}^\\infty \\frac{9n}{9n+1}.\n\\]\nPozostaje więc policzenie powyższego iloczynu.\nW tym celu piszemy, dla ustalonej liczby \\(N\\),\n\\[\\begin{align*}\n\\bigg(\\prod_{n=1}^\\infty \\frac{9n}{9n+1}\\bigg)^{-1}\n& = \\prod_{n=1}^\\infty \\bigg( 1 + \\frac 1{9n} \\bigg)\n\\\\ & > \\prod_{n=1}^N \\bigg( 1 + \\frac 1{9n} \\bigg) \\\\ & > \\frac 19 + \\frac 1{18} +\\ldots + \\frac 1{9N}\n\\\\ & = \\frac 19 \\sum_{=1}^N \\frac 1i.\n\\end{align*}\\]\nPowyższe wyrażenie zbiega \\(\\infty\\), gdy \\(N\\\\infty\\).\nZatem szukane prawdopodobieństwo wynosi \\(0\\).\nPodobne obliczenia można wykonać dla kuli o dowolnym numerze.\npokazuje, że z prawdopodobieństwem \\(1\\) urna o godz. 12 będzie pusta.","code":""},{"path":"prawdopodobieństwo-warunkowe.html","id":"prawdopodobieństwo-warunkowe","chapter":"3 Prawdopodobieństwo warunkowe","heading":"3 Prawdopodobieństwo warunkowe","text":"Rozważmy pewien górnolotny przykład. Powiedzmy, że w chwili \\(0\\) zakupiliśmy opcję kupna na wykupienie\nakcji spółki w chwili \\(1\\) po określonej kwocie (taka opcja nazywana jest opcją Europejską). Jesteśmy zatem w sytuacji,\nktórej interesuje nas faktyczna wartość akcji tej spółki.Załóżmy, że w chwili \\(0.8\\) chcemy ten kontrakt sprzedać. Wówczas wyceny takiego kontraktu\nużywać będziemy historii ceny akcji na przedziale \\([0,0.8]\\). Innymi słowy będziemy musieli odpowiedzieć na pytanie,\njak zachowanie ceny na przedziale \\([0,0.8]\\) wpływa na wartość akcji w chwili \\(1\\)?\nWykres ceny akcji na przedziałach \\([0,0.8]\\) \\([0.8,1]\\) możemy traktować jak dwa zależne od siebie eksperymenty\nlosowe. Jak więc informacja o pierwszym wpływa na prawdopodobieństwa poszczególnych wyników w drugim?Pierwszym krokiem w kierunku takich zaawansowanych zastosowań rachunku prawdopodobieństwa jest\nzrozumienie pojęcia prawdopodobieństwa warunkowego.","code":""},{"path":"prawdopodobieństwo-warunkowe.html","id":"podstawowe-definicje","chapter":"3 Prawdopodobieństwo warunkowe","heading":"Podstawowe definicje","text":"Rozważmy rzut dwiema kośćmi sześciennymi. Wiemy już, że odpowiadająca temu doświadczeniu przestrzeń zdarzeń elementarnych \\[\n\\Omega = [6]^2=  \\{ (1,1), (1, 2),  \\ldots , (6,6) \\}.\n\\]\nJeżeli rozważymy zdarzenie \\(=\\) suma oczek na obu kościach wynosi \\(6\\), \\(\\) jest zbiorem danym przez\n\\[\n=  \\{ (1,5), (2, 4), (3,3), (4,2), (5,1) \\}.\n\\]\nZakładać będziemy, że kości są dobrze wyważone.\nopisu tego eksperymentu posłużymy się prawdopodobieństwem \\(\\mathbb{P}\\), które każdemu zdarzeniu\nelementarnemu przypisuje takie samo prawdopodobieństwo.\nWówczas \\(\\mathbb{P}[] = 5/36\\).\nZałóżmy teraz, że posiadamy dodatkową informację, że na pierwszej kości wypadło jedno oczko.\nWówczas powinniśmy zmienić rozważaną przestrzeń zdarzeń elementarnych, mianowicie\n\\[\nB=\\{(1,1), (1,2), (1,3), (1,4), (1,5), (1,6)\\}\n\\]\nMusimy zmienić również sposób przypisywania prawdopodobieństwa na \\(\\mathbb{P}_1\\) przypisujące każdemu\nzdarzeniu elementarnemu z \\(B\\) takie samo prawdopodobieństwo.\nWówczas zdarzenie \\(A_1=\\) suma oczek jest równa \\(6\\) \n\\[\nA_1 = \\cap B =\\{ (1,5)\\}\n\\]\nco daje \\(\\mathbb{P}_1[A_1] = 1/6\\). Zauważmy, że\n\\[\\begin{align*}\n\\mathbb{P}_1[A_1] & = \\mathbb{P}_1[\\cap B ]= \\frac{|\\cap B|}{ |B| } \\\\ & =\n\\frac{|\\cap B| \\cdot |\\Omega|^{-1}}{ |B| \\cdot |\\Omega|^{-1} } = \\frac{\\mathbb{P}[\\cap B]}{\\mathbb{P}[B]}.  \n\\end{align*}\\]\nOkazuje się, że prawdopodobieństwo \\(\\mathbb{P}_1\\) w nowej przestrzeni probabilistycznej można wyrazić w\nterminach pierwotnie rozważanego prawdopodobieństwa \\(\\mathbb{P}\\).\nTa konstrukcja jest spotykana tak często, że wyrażenie występujące po prawej stronie ostatniego\nwzoru ma swoją specjalną nazwę.Definicja 3.1  Niech \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) będzie przestrzenią probabilistyczną. Rozważmy zdarzenie \\(B\\)\ntakie, że \\(\\mathbb{P}[B]>0\\). Prawdopodobieństwem\nwarunkowym (zajścia) zdarzenia \\(\\) pod warunkiem (zajścia) zdarzenia \\(B\\) nazywamy liczbę\n\\[\n\\mathbb{P}[|B] = \\frac{\\mathbb{P}[\\cap B]}{\\mathbb{P}[B]}.\n\\]Przy ustalonym zbiorze \\(B\\), \\(\\mathbb{P}[\\cdot|B]\\) jest miarą probabilistyczną na \\((\\Omega,\\mathcal{F})\\).Prawdopodobieństwo warunkowe jest jednym z ważniejszych pojęć teorii prawdopodobieństwa.\nRzadko zdarza się, aby doświadczenie było wykonywane w idealnych warunkach zazwyczaj jest\nono obarczone zewnętrznymi czynnikami, pewną dodatkową informacją.\nJest wykorzystywane np. przez firmy ubezpieczeniowe (polisa samochodu zależy np. od płci wieku kierowcy;\npolisa na życie od wieku, przebytych chorób, ryzyka zawodowego), graczy giełdowych.\nDla przykładu, wysokość emerytury powinna zależeć (oprócz zgromadzonych środków) od przewidywanej długości życia emeryta,\nco z kolei zależy od płci (mężczyźni 73,8; kobiety 81,7 - dane wg GUS za 2018r.),\nale też aktualnego wieku (przeciętny 60 latek, niezależnie od płci, będzie żył jeszcze średnio 260,7 miesięcy,\n65 latek, 217,1 miesiąca).Przykład 3.1  Wybieramy losową rodzinę z dwojgiem dzieci.\nInteresuje nas prawdopodobieństwo, że jest dwóch chłopców, jeżeli wiemy, żestarsze dziecko jest chłopcem;jedno z nich ma na imię Franek.W obu przypadkach\n\\[\n\\Omega=\\{(c,c), (c,d), (d,c), (d,d)\\}.\n\\]\nW przypadku :\n\\[\n\\mathbb{P}[\\{(c,c)\\} | \\{(c,c),(d,c)\\}] = 1/2,\n\\]\nw przypadku b:\n\\[\n\\mathbb{P}[\\{(c,c)\\} | \\{(c,c),(d,c), (c,d)\\}] = 1/3.\n\\]","code":""},{"path":"prawdopodobieństwo-warunkowe.html","id":"wzór-na-prawdopodobieństwo-całkowite","chapter":"3 Prawdopodobieństwo warunkowe","heading":"Wzór na prawdopodobieństwo całkowite","text":"Liczba \\(\\mathbb{P}[|B]\\) mówi jakie jest prawdopodobieństwo zajścia zdarzenia \\(\\) jeżeli wiemy, że zaszło\nzdarzenie \\(B\\). Prawdopodobieństwa względem wyjściowej miary \\(\\mathbb{P}[\\cdot]\\)\nmożna reprezentować w terminach względem miary warunkowanej \\(\\mathbb{P}[\\cdot| B]\\). W wielu przypadkach\nułatwia rachunki. Mamy bowiem\n\\[\n    \\mathbb{P}[\\cap B] = \\mathbb{P}[B] \\mathbb{P}[|B]\n\\]\noraz, skoro zdarzenia \\(\\cap B\\) oraz \\(\\cap B^c\\) wykluczają się wzajemnie,\n\\[\\begin{align*}\n\\mathbb{P}[] & = \\mathbb{P}[\\cap B] + \\mathbb{P}[\\cap B^c] \\\\&=\n\\mathbb{P}[B]\\mathbb{P}[|B] + \\mathbb{P}[B^c]\\mathbb{P}[|B^c].\n\\end{align*}\\]\nOstatni wzór jest szczególnie pomocny kiedy eksperymenty podzielone są na etapy.\nZanim zbadamy konkretny przykład uogólnijmy powyższy rachunek na dowolną liczbę zdarzeń.Definicja 3.2  Niech \\(\\subseteq \\mathbb{N}\\) będzie zbiorem indeksów.\nMówimy, że rodzina zdarzeń \\(\\{B_k\\}_{k\\}\\) (dopuszczamy \\(||=\\infty\\)) jest rozbiciem zbioru \\(\\Omega\\), jeżeli\n\\[\n\\Omega = \\bigcup_{k \\} B_k\n\\]\noraz zbiory \\(B_k\\) są parami rozłączne.Twierdzenie 3.1  (Wzór na prawdopodobieństwo całkowite) Jeżeli \\(\\{B_k\\}_{k \\}\\) jest rozbiciem \\(\\Omega\\) (skończonym lub przeliczalnym) takim, że\n\\(\\mathbb{P}[B_k]>0\\) dla każdego \\(k\\\\), dla dowolnego zdarzenia \\(\\\\mathcal{F}\\)\n\\[\n  \\mathbb{P}[] = \\sum_{k \\} \\mathbb{P}[|B_k]\\mathbb{P}[B_k]\n\\]Proof. Korzystając z definicji rozbicia oraz prawdopodobieństwa warunkowego piszemy\n\\[\\begin{align*}\n\\mathbb{P}[] & = \\mathbb{P}\\left[ \\cap \\bigcup_{k\\} B_k\\right]\n= \\mathbb{P}\\left[  \\bigcup_{k\\} (\\cap B_k)\\right]\n\\\\ & = \\sum_{k\\} \\mathbb{P}[ \\cap B_k] = \\sum_{k\\} \\mathbb{P}[|B_k]\\mathbb{P}[B_k].\n\\end{align*}\\]Przykład 3.2  W loterii fantowej szansa wylosowania losu wygrywającego jest równa \\(p\\), przegrywającego \\(q\\), z prawdopodobieństwem\n\\(r\\) (\\(p+q+r=1\\)) wyciągamy los ‘graj dalej’. Los ‘graj dalej’ wrzucany\njest urny pozwala na kolejne losowanie. Jakie jest prawdopodobieństwo wygranej?\nOznaczmy przez \\(\\), \\(B\\), \\(C\\) zdarzenie polegające na wyciągnięciu losu odpowiednio wygrywającego,\nprzegrywającego, ‘graj dalej’, przez \\(W\\) zdarzenie wygrania w loterii. Wówczas\n\\[\\begin{align*}\n\\mathbb{P}[W] =&  \\mathbb{P}[W|]\\mathbb{P}[]+\\mathbb{P}[W|B]\\mathbb{P}[B]\\\\ &+\\mathbb{P}[W|C]\\mathbb{P}[C]\n\\\\ =& 1\\cdot p + 0\\cdot q + \\mathbb{P}[W]\\cdot r.\n\\end{align*}\\]\nZatem\n\\[\n\\mathbb{P}[W] = \\frac{p}{1-r} = \\frac p{p+q}.\n\\]","code":""},{"path":"prawdopodobieństwo-warunkowe.html","id":"wzór-bayesa","chapter":"3 Prawdopodobieństwo warunkowe","heading":"Wzór Bayesa","text":"Przykład 3.3  Rozważmy następujący test na obecność pewnej choroby.\nWiadomo, że \\(1\\) osoba na \\(1000\\) jest chora.\nPonadto wiemy, że u chorych test wykrywa chorobę z prawdopodobieństwem \\(99\\%\\),\nu osób zdrowych działa poprawnie (tzn. nie wykrywa choroby) z prawdopodobieństwem \\(95\\%\\).\nJakie jest prawdopodobieństwo, że u losowo wybranej osoby wynik będzie pozytywny?\nOznaczmy\\(C\\) - badana osoba jest chora;\\(Z\\) - badana osoba jest zdrowa;\\(T\\) - test był pozytywny.\nMamy\n\\[\\begin{align*}\n\\mathbb{P}[T] & = \\mathbb{P}[T|Z]\\mathbb{P}[Z] + \\mathbb{P}[T|C]\\mathbb{P}[C]\n\\\\ & = \\frac{5}{100}\\cdot \\frac{999}{1000}  +  \\frac{99}{100}\\cdot \\frac{1}{1000}\n= \\frac{5094}{100000} \\\\& =0.05094\n\\end{align*}\\]Zauważmy, że w powyższym przykładzie jest naturalne, o wiele istotniejsze pytanie.\nJeżeli test wyszedł pozytywny, jakie jest prawdopodobieństwo, że pacjent jest\nrzeczywiście chory? Pytamy więc o przyczynę pozytywnego wyniku.\nZ jakim prawdopodobieństwem wynik jest spowodowany przez chorobę?\nZ jakim prawdopodobieństwem wynik jest fałszywie pozytywny?\nOdpowiedzi na powyższe pytanie możemy udzielić stosując wzór Bayesa.Twierdzenie 3.2  (Wzór Bayesa) Przy założeniach jw. jeżeli \\(\\mathbb{P}[]>0\\), dla każdego \\(k\\\\),\n\\[\n\\mathbb{P}[B_k| ] = \\frac{\\mathbb{P}[|B_k]\\mathbb{P}[B_k]}{\\sum_{\\} \\mathbb{P}[|B_i]\\mathbb{P}[B_i]}.\n\\]Proof. Ze wzoru na prawdopodobieństwo całkowite\n\\[\n\\frac{\\mathbb{P}[|B_k]\\mathbb{P}[B_k]}{\\sum_{\\} \\mathbb{P}[|B_i]\\mathbb{P}[B_i]}\n= \\frac{\\mathbb{P}[\\cap B_k]}{\\mathbb{P}[]} =   \\mathbb{P}B_k| ].\n\\]Remark. Ze względu na strukturę wzorów w dwóch ostatnich twierdzeniach korzysta się z nich w różnych kontekstach.Wzór na prawdopodobieństwo całkowite pozwala na obliczanie prawdopodobieństw zdarzeń,\nktóre mogą zajść w wyniku innych zdarzeń, np. przy doświadczeniach wieloetapowych.Wzoru Bayesa używamy, gdy pytamy o przebieg doświadczenia znając już jego wynik.Przykład 3.4  Mamy \\(100\\) monet, spośród których jedna jest fałszywa ma orła po obu stronach.\nWybieramy losową monetę rzucamy nią \\(10\\) razy.\nW wyniku otrzymaliśmy \\(10\\) orłów. Jakie jest prawdopodobieństwo, że wylosowana moneta była fałszywa?\nOznaczmy zdarzenia\\(B_1\\) - wylosowaliśmy prawidłową monetę;\\(B_2\\) - wylosowaliśmy fałszywą monetę z dwoma orłami;\\(\\) - wyrzucono \\(10\\) orłów.Ze wzoru Bayesa\n\\[\\begin{align*}\n    \\mathbb{P}[B_2|] & =\n    \\frac{\\mathbb{P}[|B_2]\\mathbb{P}[B_2]}{\\mathbb{P}[|B_1]\\mathbb{P}[B_1] +  \\mathbb{P}[|B_2]\\mathbb{P}[B_2]}\n    \\\\& =\n    \\frac{1\\cdot \\frac{1}{100}}{\\frac 1{2^{10}}\\cdot \\frac{99}{100} + 1\\cdot \\frac 1{100}}\n    = \\frac{1024}{1123}\\\\ &\\approx 0,91.\n\\end{align*}\\]Przykład 3.5  U pacjenta przeprowadzono test na obecność pewnej choroby.\nWiadomo, że \\(1\\) osoba na \\(1000\\) jest chora.\nPonadto wiemy, że u chorych test wykrywa chorobę z prawdopodobieństwem \\(99\\%\\),\nu osób zdrowych działa poprawnie (tzn. nie wykrywa choroby) z prawdopodobieństwem \\(95\\%\\).\nZałóżmy, że u pacjenta test był pozytywny. Jakie jest prawdopodobieństwo, że jest chory?\nOznaczmy\n- \\(C\\) - badana osoba jest chora;\n- \\(Z\\) - badana osoba jest zdrowa;\n- \\(T\\) - test był pozytywny.\nZe wzoru Bayesa\n\\[\\begin{align*}\n\\mathbb{P}[C|T] & = \\frac{\\mathbb{P}[T|C]\\mathbb{P}[C]}{\\mathbb{P}[T|Z]\\mathbb{P}[Z] + \\mathbb{P}[T|C]\\mathbb{P}[C]}\n\\\\ & =\\frac{ \\frac{99}{100}\\cdot \\frac{1}{1000} }{ \\frac{5}{100}\\cdot \\frac{999}{1000}  +  \\frac{99}{100}\\cdot \\frac{1}{1000}  }\n= \\frac{99}{5094} \\\\& \\approx 0,019.\n\\end{align*}\\]\nPowyższy wynik jest zaskakujący.\nZobaczmy jak wygląda na przykładowych liczbach.\nJeżeli populacja składa się ze \\(100000\\) osób, wśród nich jest\nok. \\(100\\) chorych \\(99900\\) zdrowych. Aby lepiej zrozumieć\ndysproporcję można posłużyć się poniższym obrazkiem, gdzie\nstosunek pola małego kwadratu w lewym dolnym rogu całości obrazka dokładnie\n1:1000.Spośród chorych u \\(99\\) osób test wyjdzie pozytywny, spośród zdrowych u \\(4995\\).\nOgraniczenie przestrzeni probabilistycznej osób, u których test wyszedł pozytywny,\npozostawia nas w przestrzeni składającej się niemal wyłącznie z osób zdrowych.\nZauważmy, że powtórzenie testu niewiele poprawia jego skuteczność, dlatego też\nważna jest informacja o innych czynnikach związanych z chorobą (np. informacja genetyczna).","code":""},{"path":"niezależność-zdarzeń.html","id":"niezależność-zdarzeń","chapter":"4 Niezależność zdarzeń","heading":"4 Niezależność zdarzeń","text":"Wróćmy przykładu polegającego na rzucie dwoma kośćmi.\nWówczas przestrzenią probabilistyczną jest \\(\\Omega = [6]^2\\) z miarą jednostajną probabilistyczną \\(\\mathbb{P}\\) na \\(\\Omega\\).\nRozważmy dwa zdarzenia\n\\[\n  = \\{ \\mbox{na pierwszej kości wypadła liczba podzielna przez $3$}\\}\n\\]\noraz\n\\[\nB = \\{ \\mbox{na drugiej kości wypadła liczba mniejsza niż $6$}\\}.\n\\]\nWtedy \\(\\mathbb{P}[]= 1/2\\) oraz \\(\\mathbb{P}[B]= 5/6\\). Z kolei prawdopodobieństwo \\(\\) pod warunkiem \\(B\\) wynosi\n\\[\n\\mathbb{P}[\\: | \\: B] = 1/2 = \\mathbb{P}[].\n\\]\nInnymi słowy zajście zdarzenia \\(B\\) nie wpływa na prawdopodobieństwo zdarzenia \\(\\).\nPowyższa równość może być zapisana jako\n\\[\n\\mathbb{P}[\\cap B] = \\mathbb{P}[] \\mathbb{P}[B].\n\\]\nZauważmy, że powyższa równość nie wymusza założenia \\(\\mathbb{P}[B]>0\\).\nWłasność ta okazuje się być bardzo użyteczna często spotykana.Definicja 4.1  Niech \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) będzie przestrzenią probabilistyczną.\nZdarzenia \\(,B\\\\mathcal{F}\\) nazywamy niezależnymi, gdy\n\\[\n  \\mathbb{P}[\\cap B] = \\mathbb{P}[]\\mathbb{P}[B].\n  \\]Zauważmy, że jeżeli \\(\\mathbb{P}[]=0\\) lub \\(\\mathbb{P}[]=1\\), dla każdego \\(B\\\\mathcal{F}\\)\nzdarzenia \\(\\) \\(B\\) są niezależne.Przykład 4.1  Rzucamy kostką sześcienną. Niech \\(\\) będzie zdarzeniem, że wypadnie liczba parzysta,\n\\(B\\) zdarzeniem, że wypadnie wypadnie liczba podzielna przez \\(3\\).\nWówczas oba zdarzenia są niezależne.\nDefiniujemy przestrzeń probabilistyczną: \\(\\Omega = [6] = \\{ 1,2, \\ldots 6\\}\\),\n\\(\\mathcal{F}\\) składa się ze wszystkich podzbiorów \\(\\Omega\\), \\(\\mathbb{P}[ \\{ k \\}]=1/6\\).\nWówczas \\(= \\{2,4,6\\}\\), \\(B = \\{3,6\\}\\), \\(\\cap B = \\{6\\}\\).\nMamy\n\\[\\begin{equation*}\n\\mathbb{P}[\\cap B] = \\frac 16 = \\frac 12 \\cdot \\frac 13 = \\mathbb{P}[]\\cdot \\mathbb{P}[B].\n\\end{equation*}\\]Często z samego opisu zdarzeń nie wynika ich niezależność.Przykład 4.2  Wybieramy losową rodzinę posiadającą \\(n\\) dzieci.\nNiech \\(\\) będzie zdarzeniem, że w wybranej rodzinie jest co najwyżej jedna dziewczynka, \n\\(B\\) będzie zdarzeniem, że w wybranej rodzinie są dziewczynki chłopcy.\nCzy zdarzenia \\(\\) \\(B\\) są niezależne?\n\\(\\Omega\\) jest zbiór ciągów o długości \\(n\\) wyrazach \\(c\\) \\(d\\) (płeć dzieci uporządkowanych wg wieku).\nWtedy \\(|\\Omega| = 2^n\\). Przyjmujemy, że każdy ciąg jest jednakowo prawdopodobny.\nMamy\n\\(|| = n+1\\), \\(|B| = 2^n-2\\), \\(|\\cap B| = n\\) (dokładnie jedna dziewczynka). Zauważmy, że\n\\[\n\\mathbb{P}[]\\mathbb{P}[B] = \\frac{n+1}{2^n} \\cdot \\frac{2^n-2}{2^n}\n\\]\noraz\n\\[\n\\mathbb{P}[\\cap B] = \\frac{n}{2^n}.\n\\]\nZdarzenia \\(\\) \\(B\\) będą niezależne wtedy tylko wtedy, gdy\n\\[\n1+n = 2^{n-1}.\n\\]\nco ma miejsce jedynie dla \\(n=3\\).\nW tym przypadku powody niezależności są czysto algebraiczne.Definicja 4.2  Zdarzenia \\(A_1,\\ldots,A_n\\\\mathcal{F}\\) nazywamy niezależnymi, gdy\n\\[\n  \\mathbb{P}[A_{i_1} \\cap A_{i_2}\\cap \\ldots \\cap A_{i_k}] = \\mathbb{P}[A_{i_1}]\\cdot \\ldots \\cdot \\mathbb{P}[A_{i_k}]\n  \\]\ndla każdego ciągu \\(1\\le i_1 < i_2 <\\cdots < i_k \\le n\\), \\(k=2,3,\\ldots, n\\).Definicja 4.3  Zdarzenia \\(A_1,\\ldots,A_n\\\\mathcal{F}\\) nazywamy niezależnymi parami, gdy dla dowolnych \\(\\=j\\)\nzdarzenia \\(A_i\\) \\(A_j\\) są niezależneJeżeli zdarzenia \\(A_1,\\ldots, A_n\\) są niezależne, są niezależne parami. Odwrotna implikacja nie jest jednak prawdziwa.Przykład 4.3  Rzucamy 2 razy kostką. Oznaczmy\\(\\) - za pierwszym razem wypadła parzysta liczba oczek;\\(B\\) - za drugim razem wypadła parzysta liczba oczek;\\(C\\) - suma oczek jest parzysta;Wówczas zdarzania \\(,B,C\\) są niezależne parami, ale nie są niezależne. Rzeczywiście, mamy\n\\(\\cap B = \\cap C = B \\cap C\\) \\(\\cap B \\cap C = \\cap B\\). Stąd\\(\\mathbb{P}[] = 1/2\\)\\(\\mathbb{P}[B] = 1/2\\)\\(\\mathbb{P}[C] = 1/2\\)\\(\\mathbb{P}[\\cap B] = \\mathbb{P}[\\cap C]\\) \\(= \\mathbb{P}[C\\cap B]\\) \\(=\\mathbb{P}[\\cap B\\cap C]\\) \\(=1/4\\)Zbadanie niezależności \\(n\\) zdarzeń wymaga\nsprawdzenia \\({n\\choose 2}+ {n\\choose 3} +\\ldots + {n\\choose n} = 2^n-n-1\\) równań.\nChcemy badać ciągi niezależnych doświadczeń (może być ich nieskończenie wiele).","code":""},{"path":"niezależność-zdarzeń.html","id":"niezależne-sigma-ciała","chapter":"4 Niezależność zdarzeń","heading":"Niezależne \\(\\sigma\\)-ciała","text":"Definicja 4.4  Niech \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) będzie przestrzenią probabilistyczną\noraz niech \\(\\mathcal{F}_1\\), \\(\\mathcal{F}_2\\), …, \\(\\mathcal{F}_n\\)\nbędą \\(\\sigma\\)-ciałami zawartymi w \\(\\mathcal{F}\\).\nMówimy, że \\(\\sigma\\)-ciała te\nsą niezależne jeżeli dla dowolnych \\(A_1\\\\mathcal{F}_1\\), …, \\(A_n\\\\mathcal{F}_n\\)\nzachodzi warunek\n\\[\n\\mathbb{P}[A_1\\cap A_2\\cap \\ldots \\cap A_n] = \\mathbb{P}[A_1]\\cdot \\ldots \\cdot \\mathbb{P}[A_n].\n\\]Równoważnie (zadanie) \\(\\sigma\\)-ciała \\(\\mathcal{F}_1\\), \\(\\mathcal{F}_2\\), …, \\(\\mathcal{F}_n\\)\nsą niezależne gdy dowolne zdarzenia \\(A_1\\\\mathcal{F}_1\\), …, \\(A_n\\\\mathcal{F}_n\\) są niezależne.Definicja 4.5  Dowolna rodzina \\(\\sigma\\)-ciał \\(\\{\\mathcal{F}_i\\}_{\\}\\) jest niezależna,\njeżeli każdy skończony jej podzbiór jest niezależny.Przykład 4.4  Rzucamy \\(2\\) razy kostką. Pokażemy, że przy jednostajnym prawdopodobieństwie, rzuty te są niezależne.\nNiech \\(\\Omega = \\left\\{ (,j): ,j \\[6]\\right\\}\\), \\(\\mathcal{F}\\) składa się ze wszystkich podzbiorów\n\\(\\Omega\\), \\(\\mathbb{P}\\)\njest wspomnianą już jednostajną miarą probabilistyczną.\nRozważmy dwa \\(\\sigma\\)-ciała:\n\\[\\begin{align*}\n    \\mathcal{F}_1 & = \\left\\{ \\times [6]:\\ \\subset  [6]  \\right\\},\\\\\n    \\mathcal{F}_2 & = \\left\\{  [6]\\times B:\\ B\\subset  [6]  \\right\\}.\n\\end{align*}\\]\nWówczas zdarzenie \\(\\times [6] \\\\mathcal{F}_1\\) mówi, że wynik na pierwszej kości należy \\(\\) nie niesie\nżadnej informacji o wyniku drugiej kości.samo dotyczy zdarzeń postaci \\([6]\\times B \\\\mathcal{F}_2\\).Natomiast ich przekrój \\(\\times [6] \\cap [6]\\times B = \\times B\\)\noznacza zdarzenie, że wynik na pierwszej kości należy \\(\\) wynik na drugiej kości należy \\(B\\).Pokażemy, że \\(\\sigma\\)-ciała \\(\\mathcal{F}_1\\) \\(\\mathcal{F}_2\\) są niezależne.\nWeźmy dowolne zdarzenia \\(\\times [6] \\\\mathcal{F}_1\\) oraz \\([6]\\times B\\\\mathcal{F}_2\\).\nWówczas\n\\[\n\\mathbb{P}\\left[\\times [6] \\right] = \\frac{||\\cdot 6}{36} = \\frac{||}{6}.\n\\]\nPodobnie\n\\[\n\\mathbb{P}\\left[ [6] \\times B\\right] =\\frac{|B|}{6}.\n\\]\nDla przekroju\n\\[\n\\mathbb{P}\\left[ \\times [6]\\cap  [6] \\times B  \\right] = \\mathbb{P}[\\times B] = \\frac{|||B|}{36}.\n\\]Powyższy przykład pokazuje, że intuicyjne pojęcie niezależności dwóch rzutów kostką zgadza się z formalną definicją.\nŁatwo uogólnić go \\(n\\) rzutów.Lemma 4.1  Załóżmy, że zdarzenia \\(A_1,\\ldots, A_n\\) są niezależne.\nWtedy \\(\\sigma\\)-ciała generowane przez \\(A_i\\): \\(\\sigma(A_1),\\ldots, \\sigma(A_n)\\)\n(przypomnijmy \\(\\sigma() = \\{\\emptyset, , ^c, \\Omega\\}\\)) również są niezależne.Proof. Pozostawiamy jako zadanieWniosek 4.1  Jeżeli zdarzenia \\(A_1,\\ldots, A_n\\) są niezależne, \n\\[\n\\mathbb{P}\\left[ \\bigcup_{=1}^n A_i   \\right] =\n1 -\\mathbb{P}\\left[ \\bigcap_{=1}^n A_i^c   \\right] = 1 -\\prod_{=1}^n (1 - \\mathbb{P}[A_i]).\n\\]Rozważmy następujący problem.\nDany jest ciąg \\(n\\) doświadczeń, w którym wynik \\(\\)-tego doświadczenia jest\nopisany przestrzenią probabilistyczną \\((\\Omega_i,\\mathcal{F}_i, \\mathbb{P}_i)\\).\nJak zbudować przestrzeń probabilistyczną \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\)\nmodelującą przeprowadzenie tych \\(n\\) doświadczeń w sposób niezależny?\nZdefiniujmy\n\\[\n    \\Omega = \\Omega_1 \\times \\ldots \\times \\Omega_n\n\\]\noraz\n\\[\n\\mathcal{F}'_i = \\left\\{ \\Omega_1\\times \\ldots \\times \\Omega_{-1} \\times \\times \\Omega_{+1} \\times \\ldots \\times \\Omega_n :\n\\\\mathcal{F}_i \\right\\}.\n\\]\nWówczas \\(\\mathcal{F}'_i\\) jest kopią \\(\\mathcal{F}_i\\).\nWeźmy \\(\\mathcal{F} = \\sigma(\\mathcal{F}'_1,\\ldots,\\mathcal{F}'_n)\\) - \\(\\sigma\\)-ciało generowane przez \\(\\mathcal{F}'_i\\)\n(wówczas \\(\\mathcal{F} = \\mathcal{F}_1 \\otimes \\ldots \\otimes \\mathcal{F}_n\\) jest \\(\\sigma\\)-ciałem produktowym),\ntzn. elementami \\(\\mathcal{F}\\) są zbiory postaci \\(A_1\\times \\ldots \\times A_n\\), dla \\(A_i\\\\mathcal{F}_i\\).Chcemy, aby kolejne doświadczenia były niezależne, więc aby \\(\\sigma\\)-ciała \\(\\mathcal{F}'_1,\\ldots, \\mathcal{F}'_n\\)\nbyły niezależne. Poszukujemy więc prawdopodobieństwa \\(\\mathbb{P}\\) takiego, że dla dowolnych\n\\(A_i\\\\mathcal{F}_i\\) zachodzi\n\\[\\begin{multline*}\n\\mathbb{P}[A_1\\times\\ldots \\times A_n] = \\\\\n\\mathbb{P}\\left[(A_1\\times \\Omega_2 \\times \\ldots \\times \\Omega_n) \\cap \\ldots \\cap (\\Omega_1\\times \\ldots \\times A_n)\\right] \\\\\n=\\prod_{=1}^n \\mathbb{P}\\left[\\Omega_1 \\times \\ldots \\times A_i \\times \\ldots \\times \\Omega_n\\right].\n\\end{multline*}\\]\nSkoro chcemy, aby przestrzeń produktowa reprodukowała \\(\\)-ty eksperyment, musi również\nzachodzić\n\\[\n\\mathbb{P}[\\Omega_1 \\times \\ldots \\times A_i \\times \\ldots \\times \\Omega_n] = \\mathbb{P}_i[A_i].\n\\]\nUpraszczając zapis szukamy prawdopodobieństwa \\(\\mathbb{P}\\) na \\(\\Omega\\) takiego, że\n\\[\n\\mathbb{P}[A_1\\times\\ldots \\times A_n] =\\prod_{=1}^n \\mathbb{P}_i[A_i]\n\\]\ndla dowolnych zdarzeń \\(A_1, \\ldots, A_n\\).\nZ teorii miary wiemy, że istnieje dokładnie jedna taka miara probabilistyczna.\nJest miara produktowa:\n\\[\n\\mathbb{P} = \\mathbb{P}_1 \\otimes \\ldots \\otimes \\mathbb{P}_n.\n\\]Przykład 4.5  Losujemy niezależnie dwie liczby z przedziału \\([0,1]\\).\nW tym przypadku mamy \\(\\Omega_1=\\Omega_2=[0,1]\\),\n\\(\\mathcal{F}_1=\\mathcal{F}_2 = \\mathcal{B}([0,1])\\) oraz \\(\\mathbb{P}_1=\\mathbb{P}_2=\\lambda_1\\),\ngdzie \\(\\lambda_1\\) jest jednowymiarową miarą Lebesgue’.\nJak reprezentować wynik tego eksperymentu na jednej przestrzeni probabilistycznej?\nZ powyższej konstrukcji \\(\\Omega = \\Omega_1\\times \\Omega_2 = [0,1]^2\\),\n\\(\\mathcal{F} = \\mathcal{B}([0,1]) \\otimes \\mathcal{B}([0,1]) = \\mathcal{B}([0,1]^2)\\).\nOdpowiadające prawdopodobieństwo \n\\(\\mathbb{P}=\\mathbb{P}_1\\otimes \\mathbb{P}_2 = \\lambda_1\\otimes \\lambda_1\\).\nPrzypomnijmy, że jest jedyna miara na \\([0,1]^2\\) taka, że\n\\[\\begin{equation*}\n    \\lambda_1\\otimes \\lambda_1 (\\times B) = \\lambda_1 () \\lambda_1(B)\n\\end{equation*}\\]\ndla dowolnych \\(, B \\\\mathcal{B}([0,1])\\).\nJest dokładnie charakteryzacja dwuwymiarowej (płaskiej) miary Lebesgue’. Innymi słowy\n\\(\\mathbb{P} = \\lambda_1\\otimes \\lambda_1 = \\lambda_2\\).\nReasumując, wylosowanie niezależnie dwóch liczb z przedziału \\([0,1]\\) jest tożsame z wylosowaniem\npunktu z kwadratu jednostkowego \\([0,1]^2\\).Przykład 4.6  (Schemat Bernoulliego) Wykonano \\(n\\)-krotnie samo doświadczenie, w którym prawdopodobieństwo sukcesu wynosi \\(p\\).\nKolejne próby były niezależne.\nWówczas prawdopodobieństwo sukcesu w dokładnie \\(k\\) próbach wynosi\n\\({n\\choose k} p^k (1-p)^{n-k}\\).\nRzeczywiście, niech\\(\\Omega_i = \\{0,1\\}\\) (1 odpowiada sukcesowi w \\(\\)-tym doświadczeniu),\\(\\mathcal{F}_i = 2^{\\Omega_i}\\),\\(\\mathbb{P}_i[\\{1\\}] = p\\), równoważnie \\(\\mathbb{P}_i[\\{1\\}] = p^{\\omega_i}(1-p)^{1-\\omega_i}\\).Wówczas \\((\\Omega_i, \\mathcal{F}_i, \\mathbb{P}_i)\\)\nmodeluje wynik doświadczenia w \\(\\)-tym kroku. Definiując jak powyżej\\(\\Omega = \\Omega_1 \\times \\ldots \\times \\Omega_n = \\{0,1\\}^n\\),\\(\\mathcal{F} = \\mathcal{F}_1 \\otimes \\ldots \\otimes \\mathcal{F}_n\\),\\(\\mathbb{P}  = \\mathbb{P}_1 \\otimes \\ldots \\otimes \\mathbb{P}_n\\).Wtedy dla \\(\\omega = (\\omega_1,\\ldots, \\omega_n)\\\\Omega\\) mamy\n\\[\n\\mathbb{P}[\\{\\omega\\}] = p^{\\omega_1+\\ldots+\\omega_n}(1-p)^{n-(\\omega_1+\\ldots+\\omega_n)}.\n\\]\nNiech \\(A_k\\) oznacza zdarzenie osiągnięcia dokładnie \\(k\\) sukcesów, tj.\n\\[\nA_k = \\{\\omega\\\\Omega:\\; \\omega_1+\\ldots+\\omega_n = k\\},\n\\]\ninnymi słowy \\(A_k\\) zawiera \\(\\omega\\) zawierające dokładnie \\(k\\) jedynek.\nStąd \\(|A_k| = {n\\choose k}\\) oraz\n\\[\n\\mathbb{P}[A_k] = \\sum_{\\omega \\A_k} \\mathbb{P}[\\{\\omega\\}] =\n|A_k| p^k (1-p)^{n-k}= {n\\choose k} p^k (1-p)^{n-k}.\n\\]","code":""},{"path":"lemat-borela-cantellego.html","id":"lemat-borela-cantellego","chapter":"5 Lemat Borela-Cantellego","heading":"5 Lemat Borela-Cantellego","text":"","code":""},{"path":"lemat-borela-cantellego.html","id":"nieskończone-ciągi-eksperymentów","chapter":"5 Lemat Borela-Cantellego","heading":"Nieskończone ciągi eksperymentów","text":"W przyszłości chcemy analizować asymptotyczne zachowanie procesów losowych.\nW naturalny sposób potrzeba konstrukcji przestrzeni probabilistycznej, na której\nmożna wykonać nieskończenie wiele eksperymentów które mogą, ale nie muszą być niezależne.\nDla ustalenia uwagi skupimy się jednak na niezależnych eksperymentach.\nZakładać będziemy, że mamy ciąg przestrzeni probabilistycznych, w którym\n\\((\\Omega_n, \\mathcal{F}_n, \\mathbb{P}_n)\\) opisuje wynik \\(n\\)-tego eksperymentu. Zakładać będziemy, że\n\\[\\begin{equation}\n    \\Omega_n \\subseteq \\mathbb{R}, \\quad \\mathcal{F}_n \\subseteq \\mathcal{B}(\\mathbb{R}).\n    \\tag{5.1}\n\\end{equation}\\]\nChcemy udzielić odpowiedzi na pytanie jak skonstruować przestrzeń, na której\nwszystkie te eksperymenty wykonywane są niezależnie?Powiedzmy, że interesuje nas\nnieskończony ciąg rzutów monetą. Skoro ciąg \\(n\\) rzutów modelujemy przestrzenią skończonych\nciągów zer jedynek \\(\\{0,1\\}^n\\), nieskończony ciąg rzutów będziemy modelować\nprzestrzenią nieskończonych ciągów zer jedynek \\(\\{0,1\\}^\\mathbb{N}\\).\nNieskończony produkt kartezjański prowadzi pewnych trudności technicznych\nprzy sprawdzaniu warunku przeliczalnej addytywności, który nakładamy zawsze na rozważane\nprawdopodobieństwo \\(\\mathbb{P}\\). Trudności te są oczywiście przejścia.\njednak ograniczymy się gotowych rozwiązań.Przykład 5.1  Zamierzamy skonstruować przestrzeń probabilistyczną na której można\nzdefiniować ciąg niezależnych zdarzeń \\(\\{A_n\\}_{n=1}^{\\infty}\\)\ntakich, że \\(\\mathbb{P}[A_n]= 1/2\\).\nNiech \\((\\Omega, \\mathcal{F}, \\mathbb{P}) = ([0,1], \\mathcal{B}([0,1]), {\\rm Leb})\\).\nDla każdego \\(\\omega\\[0,1]\\) przyporządkowujemy jego rozwinięcie dwójkowe:\n\\[\n  \\omega =\\sum_{n=1}^\\infty \\frac{\\omega_n}{2^n} = 0,\\omega_1\\omega_2\\ldots\n\\]\nPowyższe przedstawienie nie jest jednoznaczne np.\n\\(1/2 = 0,10000\\ldots = 0,01111\\ldots\\), aby uzyskać jednoznaczność\nwybieramy rozwinięcie zawierające nieskończenie wiele \\(1\\).\nNiech\n\\[\nA_n = \\big\\{ \\omega \\[0,1]:\\; \\omega_n = 0  \\big\\},\n\\]\nczyli\n\\[\\begin{align*}\nA_1 &= [0,1/2),\\\\\nA_2 &= [0,1/4)\\cup [1/2,3/4),\\\\\nA_3 &= [0,1/8)\\cup [1/4, 3/8) \\cup [1/2,5/8)\\cup [3/4,7/8),\\\\\nA_4 & = \\cdots\n\\end{align*}\\]\nWtedy \\(\\mathbb{P}[A_n]=1/2\\).\nPonadto można pokazać, że zbiory \\(\\{A_n\\}\\) są niezależne (zadanie).Powyższy przykład pokazuje, że w przypadku rzutów monetą można wskazać bardzo konkretną przestrzeń,\nktórą możemy modelować nieskończony ciąg eksperymentów. Jeżeli bylibyśmy\nzainteresowani rzutami kostką, konstrukcję z powyższego przykładu można przeprowadzić\ndla rozwinięć w systemie szóstkowym.Co jeżeli nasz ciąg eksperymentów jest bardziej skomplikowany? Jeżeli w chwilach nieparzystych rzucamy\njedną kością, w chwilach parzystych rzucamy monetą z wyłączeniem chwil podzielnych przez 128,\nw których losujemy punkt z przedziału \\([0,42]\\)?Potrzebne nam jest ogólne narzędzie (maszynka) konstruowania przestrzeni probabilistycznych\ndla nieskończonych ciągów eksperymentów.Twierdzenie 5.1  (Kołmogorowa o istnieniu procesu) Załóżmy, że \\(\\{\\mathbb{P}_n'\\}_{n \\\\mathbb{N}}\\) jest ciągiem miar probabilistycznych takim, żedla każdego \\(n \\\\mathbb{N}\\), \\(\\mathbb{P}_n'\\) jest miarą na \\(\\mathbb{R}^n, \\mathcal{B}(\\mathbb{R}^n)\\).Spełniony jest warunek zgodności:\n\\[\n\\mathbb{P}_{n+1}'(A_1\\times A_2 \\times \\ldots \\times A_n\\times \\mathbb{R})\n=  \\mathbb{P}_{n}'(A_1\\times A_2 \\times \\ldots \\times A_n).\n\\]Wówczas istnieje jedyna miara probabilistyczna \\(\\mathbb{P}\\) na\n\\((\\mathbb{R}^\\mathbb{N}, \\mathcal{B}(\\mathbb{R}^\\mathbb{N}))\\) taka, że\n\\[\n\\mathbb{P}'(A_1\\times A_2 \\times \\ldots \\times A_n\\times \\mathbb{R}\\times \\ldots)\n=  \\mathbb{P}_{n}'(A_1\\times A_2 \\times \\ldots \\times A_n).\n\\]Proof. znalezienia tutaj, Theorem .3.1.\\(\\mathcal{B}(\\mathbb{R}^\\mathbb{N})\\) jest \\(\\sigma\\)-ciałem generowanym przez skończenie wymiarowe\nprostokąty\n\\[\\begin{equation*}\n    B_1\\times B_2 \\times \\cdots \\times B_n \\times \\mathbb{R} \\times \\mathbb{R} \\times \\cdots,\n\\end{equation*}\\]\ndla \\(n \\\\mathbb{N}\\), \\(B_1, \\ldots B_n \\\\mathcal{B}(\\mathbb{R})\\).Powyższe twierdzenie dostarcza ogólne narzędzie gwarantujące istnienie miary probabilistycznej.\nZauważmy też, że działa ono nie tylko dla niezależnych eksperymentów.\nJedynym warunkiem jest zgodność miar.\nW niektórych przypadkach można bezpośrednio skonstruować przestrzeń probabilistyczną.Wniosek 5.1  Istnieje przestrzeń probabilistyczna \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\)\nopisująca nieskończony ciąg niezależnych eksperymentów.Proof. Niech \\((\\Omega_n, \\mathcal{F}_n, \\mathbb{P}_n)\\) będzie ciągiem przestrzeni probabilistycznych\nspełniających (5.1).\nWówczas\n\\[\\begin{equation*}\n    \\mathbb{P}'_n = \\mathbb{P}_1\\otimes \\mathbb{P}_2\\otimes \\cdots \\otimes \\mathbb{P}_n\n\\end{equation*}\\]\nopisuje pierwsze \\(n\\) eksperymentów wykonanych niezależnie. Rodzina ta jest zgodna, bo\n\\[\\begin{multline*}\n\\mathbb{P}_{n+1}'[B_1\\times B_2\\times \\cdots B_{n} \\times \\mathbb{R}] \\\\\n= \\mathbb{P}_1[B_1]\\mathbb{P}_2[B_2] \\cdots \\mathbb{P}_n[B_n]\\mathbb{P}_{n+1}[\\mathbb{R}] \\\\\n\\mathbb{P}_{n}'[B_1\\times B_2\\times \\cdots B_{n}] \\\\\n\\end{multline*}\\]\nZ Twierdzenia 5.1 wynika, że istnieje \\(\\mathbb{P}\\) na \\(\\mathbb{R}^\\mathbb{N}\\) taka, że\n\\[\n\\mathbb{P}'(A_1\\times A_2 \\times \\ldots \\times A_n\\times \\mathbb{R}\\times \\ldots)\n=  \\mathbb{P}_{n}'(A_1\\times A_2 \\times \\ldots \\times A_n).\n\\]\nOkazuje się, że jest szukana przez nas miara probabilistyczna modelująca niezależne eksperymenty.\nNiech\n\\[\\begin{align*}\n    \\tilde{}_1 & = A_1\\times \\mathbb{R}\\times \\mathbb{R}\\times \\ldots \\\\\n    \\tilde{}_2 & = \\mathbb{R}\\times A_2 \\times \\mathbb{R} \\times \\ldots \\\\\n    \\tilde{}_3 & = \\mathbb{R} \\times \\mathbb{R} \\times A_3 \\times \\mathbb{R} \\ldots\n\\end{align*}\\]\nWówczas \\(\\tilde{}_j\\) jest kopią \\(A_j\\) w nieskończonym produkcie.\nWówczas dla każdego \\(j \\\\mathbb{N}\\),\n\\[\\begin{equation*}\n    \\mathbb{P}[\\tilde A_j] = \\mathbb{P}_j[A_j].\n\\end{equation*}\\]\nSkoro\n\\[\\begin{equation*}\n    A_1\\times A_2 \\times \\ldots \\times A_n \\times \\mathbb{R} \\ldots = \\tilde{}_1\\cap \\tilde{}_2\\cap \\ldots \\tilde{}_n,\n\\end{equation*}\\]\n\n\\[\\begin{equation*}\n    \\mathbb{P}[\\tilde{}_1\\cap \\tilde{}_2\\cap \\ldots \\tilde{}_n] = \\mathbb{P}[\\tilde A_1]\\mathbb{P}[\\tilde A_2] \\cdots \\mathbb{P}[\\tilde A_n].\n\\end{equation*}\\]\nCzyli miara \\(\\mathbb{P}\\) reprodukuje eksperymenty w sposób niezależny.Badając eksperymenty losowe polegające na nieskończonych powtórzeniach jednego eksperymentu, powiedzmy, że wykonujemy nieskończenie wiele rzutów monetą.Definicja 5.1  Dana jest przestrzeń probabilistyczna \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\)\noraz ciąg zdarzeń \\(\\{A_n\\}_{n\\\\mathbb{N}} \\subseteq \\mathcal{F}\\).\nGranicą górną ciągu zdarzeń\n\\(\\{A_n\\}_{n \\\\mathbb{N}}\\) nazywamy zdarzenie\n\\[\\begin{multline*}\n    \\limsup_{n\\\\mathbb{N}} A_n = \\bigcap_{m=1}^\\infty \\bigcup_{n=m}^\\infty A_n \\\\\n    = \\left( A_1\\cup A_2\\cup A_3\\cup\\ldots \\right)\\cap\n\\left( A_2\\cup A_3\\cup\\ldots \\right)\\cap\n\\left( A_3\\cup\\ldots \\right) \\cap \\ldots.\n\\end{multline*}\\]Przypomnijmy, że element należy przekroju zbiorów wtedy tylko wtedy, gdy należy\nkażdego z nich. Element należy sumy zbiorów wtedy tylko wtedy, gdy należy\njednego ze zbiorów. Mamy więc\n\\[\\begin{multline*}\n\\omega \\\\bigcap_{m=1}^\\infty\\bigcup_{n=m}^{\\infty }A_n \\iff\n\\forall m \\geq 1, \\: \\omega \\\\bigcup_{n=m}^{\\infty} A_m \\\\ \\iff\n\\forall m\\geq 1, \\: \\exists n\\geq m, \\: \\omega \\A_n.\n\\end{multline*}\\]\nReasumując, \\(\\omega\\\\limsup A_n\\) wtedy tylko wtedy, gdy \\(\\omega\\)\nnależy nieskończenie wielu zbiorów \\(A_i\\). Innymi słowy,\n\\(\\limsup A_n\\) zdarzenie polegające na tym, że\nzaszło nieskończenie wiele spośród zdarzeń \\(A_1, A_2, \\ldots\\).\nDla przykładu, rzucamy nieskończenie wiele razy kostką \\(A_n\\) oznacza zdarzenie,\nże w \\(n\\)-tym rzucie kostką wypadła \\(6\\), \\(\\limsup A_n\\) jest zdarzeniem,\nże wypadło nieskończenie wiele \\(6\\).Niezwykle przydatnym narzędziem będzie lemat wskazujący\nwarunki przy których zachodzi nieskończenie wiele zdarzeń.Lemma 5.1  (Borel-Cantelli) Załóżmy, że \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) jest przestrzenią probabilistyczną\noraz niech \\(\\{A_n\\}_{n \\\\mathbb{N}}\\subseteq \\mathcal{F}\\) będzie ciągiem zdarzeń.Jeżeli \\(\\sum_{n=1}^\\infty \\mathbb{P}[A_n] < \\infty\\), \\[\\mathbb{P}[\\limsup A_n] = 0,\\]\ntzn. z prawdopodobieństwem \\(1\\) zachodzi jedynie skończenie wiele spośród zdarzeń \\(A_n\\).Jeżeli \\(A_1,A_2,\\ldots\\) są niezależnymi zdarzeniami oraz\\(\\sum_{n=1}^\\infty \\mathbb{P}[A_n] = \\infty\\), \\[\\mathbb{P}[\\limsup A_n] =1,\\]\ntzn. z prawdopodobieństwem \\(1\\) zachodzi nieskończenie wiele zdarzeń \\(A_n\\).Proof. Korzystając z ciągłości miary, następnie z jej podaddytywności otrzymujemy\n\\[\\begin{equation*}\n\\mathbb{P}[\\limsup A_n] = \\mathbb{P}\\left[ \\bigcap_{m=1}^\\infty \\bigcup_{n=m}^\\infty A_n \\right]\n=\\lim_{m\\\\infty} \\mathbb{P}\\left[ \\bigcup_{n=m}^\\infty A_n \\right]\\le\n\\lim_{m\\\\infty} \\sum_{n=m}^\\infty \\mathbb{P}[A_n] = 0,\n\\end{equation*}\\]\ngdzie ostatnia równość wynika z sumowalności szeregu.\nAby wykazać punkt 2, wystarczy pokazać, że\n\\[\n  0 = \\mathbb{P}\\left[ (\\limsup A_n)^c\\right]=  \n  \\mathbb{P}\\left[\\left( \\bigcap_{m=1}^\\infty \\bigcup_{n=m}^\\infty A_n\\right)^c \\right]\n=   \\mathbb{P}\\left[ \\bigcup_{m=1}^\\infty \\bigcap_{n=m}^\\infty A_n^c \\right].\n\\]\nSkoro\n\\[\\begin{equation*}\n\\mathbb{P}\\left[ \\bigcup_{m=1}^\\infty \\bigcap_{n=m}^\\infty A_n^c \\right]\n\\leq \\sum_{m=1}^\\infty\\mathbb{P}\\left[ \\bigcap_{n=m}^\\infty A_n^c \\right],\n\\end{equation*}\\]\nwystarczy udowodnić, że dla każdego \\(m\\) mamy\n\\[\n\\mathbb{P}\\left[ \\bigcap_{n=m}^\\infty A_n^c \\right] = 0.\n\\]\nW tym celu piszemy (kolejno korzystamy z twierdzenia o ciągłości, niezależności zbiorów \\(A_n\\) oraz nierówności\n\\(1-x\\le e^{-x}\\))\n\\[\\begin{multline*}\n\\mathbb{P}\\left[ \\bigcap_{n=m}^\\infty A_n^c \\right]\n= \\lim_{k\\\\infty} \\mathbb{P}\\left[ \\bigcap_{n=m}^k A_n^c \\right]\n    = \\lim_{k\\\\infty} \\prod_{n=m}^k \\mathbb{P}\\left[ A_n^c \\right] \\\\\n    = \\lim_{k\\\\infty} \\prod_{n=m}^k \\left( 1 - \\mathbb{P}\\left[ A_n \\right]\\right)\n    \\le \\lim_{k\\\\infty} e^{-\\sum_{n=m}^k \\mathbb{P}[ A_n]} =  e^{-\\sum_{n=m}^\\infty \\mathbb{P}[ A_n]} = 0.\n  \\end{multline*}\\]Przykład 5.2  Uzasadnimy, że jeżeli będziemy rzucać odpowiednio długo kostką,\nz prawdopodobieństwem \\(1\\) wyrzucimy dowolną liczbę szóstek.\nNiech \\(\\Omega = \\{1,2,\\ldots,6\\}^\\mathbb{N}\\), \\(\\mathcal{F} = 2^\\Omega\\).\nPonadto niech \\(\\mathbb{P}\\) będzie odpowiednią miarą probabilistyczną.\nOznaczmy przez \\(A_n\\) zdarzenie, że w rzucie o numerze \\(n\\), wypadło \\(6\\).\nWówczas \\(\\mathbb{P}[A_n] = 1/6\\).\nZdarzania \\(A_i\\) są niezależne oraz\n\\[\n\\sum_{n=1}^\\infty \\mathbb{P}\\left[A_n\\right]  = \\sum_{n=1}^\\infty 1/6 = \\infty.\n\\]\nLemat Borela - Cantelliego pociąga\n\\[\n\\mathbb{P}\\left[\\limsup  A_n\\right] =1.\n\\]Przykład 5.3  Jeżeli będziemy rzucać odpowiednio długo kostką,\nz prawdopodobieństwem \\(1\\) wyrzucimy w kolejnych rzutach ciąg złożony z kolejnych \\(10\\) jedynek kolejnych \\(10\\) szóstek.\nNiech \\(\\Omega = \\{1,2,\\ldots,6\\}^\\mathbb{N}\\), \\(\\mathcal{F} = 2^\\Omega\\).\nPonadto niech \\(\\mathbb{P}\\) będzie odpowiednią miarą probabilistyczną.\nOznaczmy przez \\(A_n\\) zdarzenie, że w rzutach o numerach \\(n, n+1, \\ldots, n+19\\) wypadnie żądany ciąg.\nWówczas \\(\\mathbb{P}[A_n] = 1/6^{20}\\).Zdarzania \\(A_i\\) nie są niezależne, więc nie możemy dla nich użyć lematu Borela-Cantallego.\nZdefiniujmy jednak \\(\\widetilde A_n = A_{20n}\\).\nWówczas zbiory \\(\\widetilde A_n\\) są\nniezależne oraz\n\\[\n\\sum_{n=1}^\\infty \\mathbb{P}\\left[\\widetilde A_n\\right]  = \\sum_{n=1}^\\infty \\frac 1{6^{20}} = \\infty.\n\\]\nLemat Borela - Cantelliego pociąga\n\\[\n\\mathbb{P}\\left[\\limsup \\widetilde A_n\\right] =1,\n\\]\nwięc z prawdopodobieństwem \\(1\\) zdarzenia \\(\\widetilde A_n\\) (zatem również \\(A_n\\)) zachodzą nieskończenie wiele razy.\nW szczególności z prawdopodobieństwem \\(1\\) zajdzie przynajmniej jedno ze zdarzeń \\(A_n\\).Ostatni przykład można powtórzyć dla dowolnego ciągu wyników. W szczególności pociąga \nTwierdzenie o nieskończonej liczbie małp,\nktóre można sparafrazować w następujący sposób: jeżeli małpa będzie naciskać w sposób losowy klawisze maszyny pisania przez nieskończenie długi czas, z prawdopodobieństwem jeden napisze ona wszystkie dzieła Mickiewicza.Przykład 5.4  Rzucamy nieskończenie wiele razy niesymetryczną monetą (orzeł wypada na niej z prawdopodobieństwem \\(p\\=1/2\\)).\nNiech \\(A_n\\) oznacza zdarzenie, że w pierwszych \\(n\\) rzutach wypadło tyle samo orłów co reszek.\nPokażemy że z prawdopodobieństwem \\(1\\) zachodzi jedynie skończenie wiele zdarzeń \\(A_n\\).\nDla nieparzystej liczby rzutów mamy oczywiście \\(\\mathbb{P}[A_{2n+1}] = 0\\),\nnatomiast dla parzystej liczby\n\\[\n\\mathbb{P}[A_{2n}] = {2n \\choose n} p^n(1-p)^n \\sim \\frac{4^np^n(1-p)^n}{\\sqrt{n\\pi}},\n\\] gdzie ostatnia implikacja wynika ze wzoru Stirlinga\n\\[\\begin{equation}\nn! \\sim \\sqrt{2\\pi n} \\bigg(\\frac{n}{e}\\bigg)^n,\n\\end{equation}\\] zapis \\(a_n\\sim b_n\\) oznacza, że \\(\\lim_{n\\\\infty} \\frac{a_n}{b_n}=1\\).Funkcja \\(x\\4x(1-x)\\) na przedziale \\([0,1]\\) przyjmuje maksimum równe 1 dla \\(x=1/2\\), zatem dla \\(p\\= 1/2\\), \\(4p(1-p)\\),\nz kolei, na mocy kryterium Cauchy’ego oznacza zbieżność szeregu\n\\[\n\\sum_n \\mathbb{P}[A_n] <\\infty.\n\\] Z lematu Borella-Cantallego zdarzenia \\(A_n\\) z prawdopodobieństwem jeden zachodzą\njedynie skończenie wiele razy. (Zauważmy jednak, że liczba tych zdarzeń jest losowa).Jak zmienia się rysunek, gdy \\(p>1/2\\)? Jakie zachowanie obserwujemy dla małych wartości \\(p-1/2\\)?","code":""},{"path":"zmienne-losowe-i-ich-rozkłady.html","id":"zmienne-losowe-i-ich-rozkłady","chapter":"6 Zmienne losowe i ich rozkłady","heading":"6 Zmienne losowe i ich rozkłady","text":"Wróćmy na chwilę paradoksu Bertranda. Wyposażeni w aparat zdobyty tej pory jesteśmy w stanie\ndokładniej przeanalizować ten przykład. Zobaczymy też, w którym kierunku powinniśmy\nrozwijać naszą teorię.Jeżeli losujemy cięciwę pierwszym sposobem, tj. przez wylosowanie jej końców przestrzenią probabilistyczną,\nktóra reprezentuje jest \\(\\Omega_1 = [0,1]^2\\) z \\(\\sigma\\)-ciałem zbiorów borelowskich miarą Lebesgue’.Zdarzenie elementarne \\(\\omega = (\\theta, \\phi)\\) utożsamiamy z cięciwą o końcach\n\\((\\cos(2\\pi \\theta), \\sin(2\\pi \\theta))\\) oraz\n\\((\\cos(2\\pi \\phi), \\sin(2\\pi \\phi))\\). Wówczas długość takiej cięciwy \n\\[\\begin{equation*}\n    X_1(\\omega) = \\sqrt{(\\cos(2\\pi \\theta) -\\cos(2\\pi \\phi))^2 + (\\sin(2\\pi \\theta)-\\sin(2\\pi \\phi))^2}.\n\\end{equation*}\\]\nPostać \\(X_1(\\omega)\\) możemy oczywiście nieco uprościć. Wykorzystując niezmienniczość\nna obroty\n\\[\\begin{multline*}\n    X_1(\\omega) = X_1(\\theta,\\phi) = X_1(\\theta-\\phi,0) \\\\\n    = \\sqrt{(1-\\cos(2\\pi (\\theta -\\phi))^2 + (\\sin(2\\pi (\\theta - \\phi)))^2}\n    = 2 \\sin \\left( \\pi|\\theta-\\phi| \\right).\n\\end{multline*}\\]\nZauważmy, że interesująca nas wielkość jest funkcją rzeczywistą\nzdarzenia elementarnego, tj. \\(X_1 \\colon \\Omega \\\\mathbb{R}\\).Załóżmy teraz, że losujemy cięciwę poprzez wylosowanie jej środka.\nWówczas \\(\\Omega_2\\) jest kołem jednostkowym z \\(\\sigma\\)-ciałem zbiorów Borelowskich miarą Lebesgue’.Jeżeli \\(\\omega =(x,y) \\\\Omega_2\\) jest środkiem cięciwy, z twierdzenia Pitagorasa jej długość jest dana przez\n\\[\\begin{equation*}\n    X_2(\\omega) = 2 \\sqrt{1-x^2-y^2}.\n\\end{equation*}\\]\nWreszcie losując tylko odległość od środka koła wybieramy \\(\\Omega_3=[0,1]\\) z \\(\\sigma\\)-ciałem zbiorów Borelowskich\nmiarą Lebesgue’. Wówczas dla \\(\\omega \\\\Omega_3\\) długość wylosowanej cięciwy \n\\[\\begin{equation*}\n    X_3(\\omega) = 2\\sqrt{1-\\omega^2}.\n\\end{equation*}\\]\nwszystkich trzech przykładach otrzymaliśmy różne przestrzenie probabilistyczne.\nAby rzeczywiście stwierdzić, że powyższe trzy metody nie są równoważne musimy\nporównać funkcje \\(X_i\\colon \\Omega_i \\\\mathbb{R}\\) dla \\(=1,2,3\\) pod kątem probabilistycznym.\nPowyższe stanowi przykład sytuacji, w której bardziej od samych zdarzeń elementarnych\ninteresują nas wartości funkcji zdarzeń elementarnych. Pamiętajmy, że struktura przestrzeni \\(\\Omega\\) może być niemal dowolna.\nElementami przestrzeni zdarzeń elementarnych mogą być liczby,\nwyniki rzutów monetą, ciągi, zbiory (kart), wykresy (kursy akcji) itd.\nNas jednak zazwyczaj interesuje bardzo konkretna, często liczbowa, informacja.","code":""},{"path":"zmienne-losowe-i-ich-rozkłady.html","id":"funkcje-mierzalne","chapter":"6 Zmienne losowe i ich rozkłady","heading":"Funkcje mierzalne","text":"Pracując na ogólnej przestrzeni probabilistycznej \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\)\nbędziemy chcieli analizować funkcje \\(X \\colon \\Omega \\\\mathbb{R}\\) z punktu widzenia\nprawdopodobieństwa \\(\\mathbb{P}\\). Oznacza , że od funkcji \\(X\\) będziemy wymagali\nodpowiedniej regularności.Definicja 6.1  Niech \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\)\nbędzie przestrzenią probabilistyczną. Zmienna losowa jest dowolna mierzalna funkcja\n\\(X:(\\Omega, \\mathcal{F})\\(\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))\\).Przypomnijmy, że \\(X:(\\Omega, \\mathcal{F})\\(\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))\\) jest mierzalna gdy\ndla każdego \\(\\\\mathcal{B}(\\mathbb{R})\\),\n\\[\nX^{-1}()= \\{ \\omega \\\\Omega \\: : \\: X(\\omega) \\\\}\\\\mathcal{F}.\n\\]Przykład 6.1  Jeżeli rzucamy pięć razy kostką, \n\\(\\Omega = \\{(i_1,\\ldots, i_5), i_j \\\\{1,2,\\ldots, 6\\}\\}\\), \\(\\omega = (\\omega_1,\\ldots, \\omega_5)\\).\nJeśli chcemy obliczyć sumę wyników (nie interesują nas konkretne wyniki rzutów, ale suma oczek). Wówczas\nrozważamy \\(X(\\omega) = \\omega_1+\\ldots + \\omega_5\\). Wtedy \\(X \\colon \\Omega \\\\mathbb{R}\\) jest zmienną losową.Remark. Jeżeli \\(\\Omega\\) jest przeliczalny \\(\\mathcal{F} = 2^\\Omega\\), każde odwzorowanie\n\\(X:\\Omega\\mapsto \\mathbb{R}\\) jest zmienną losową (jeżeli \\(\\mathcal{F}\\= 2^\\Omega\\) nie musi być już prawdą).Remark. \\(X\\) jest zmienną losową jeżeli dla każdego \\(t\\\\mathbb{R}\\), \\(X^{-1}((-\\infty,t])\\\\mathcal{F}\\).Twierdzenie 6.1  Jeżeli \\(X_1,X_2,\\ldots\\) są zmiennymi losowymi, \\(X_1+X_2\\), \\(X_1-X_2\\), \\(X_1\\cdot X_2\\), \\(X_1/X_2\\) (\\(X_2\\=0\\)) są zmiennymi losowymi.Jeżeli \\(f:\\mathbb{R}^n\\mapsto \\mathbb{R}\\) jest mierzalne, \\(f(X_1,\\ldots, X_n)\\) jest zmienną losową.\\(\\inf_n X_n\\), \\(\\sup_n X_n\\), \\(\\limsup_n X_n\\), \\(\\liminf_n X_n\\) są zmiennymi losowymi.Proof. Pozostawiamy jako zadanie.","code":""},{"path":"zmienne-losowe-i-ich-rozkłady.html","id":"rozkłady-zmiennych-losowych","chapter":"6 Zmienne losowe i ich rozkłady","heading":"Rozkłady zmiennych losowych","text":"Chcąc porównać dwie zmienne losowe \\(X_1 \\colon \\Omega_1 \\\\mathbb{R}\\) oraz \\(X_2 \\colon \\Omega_2 \\\\mathbb{R}\\)\nzdefiniowane na dwóch różnych przestrzeniach probabilistycznych musimy znaleźć sposób\nna reprezentację ich na pewnej wspólnej przestrzeni. Tak się akurat składa, że obie\nfunkcje mają takie przeciwdziedziny. Jeżeli \\(X\\) jest zmienną losową, można jej użyć \nprzetransportowania miary \\(\\mathbb{P}\\) na \\((\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))\\).Definicja 6.2  Miarę \\(\\mu_X\\) na \\((\\mathbb{R},\\mathcal{B}(\\mathbb{R}))\\) zdefiniowaną wzorem\n\\[\n  \\mu_X(B) = \\mathbb{P}[X\\B] = \\mathbb{P}\\left[ \\{\\omega\\\\Omega:\\; X(\\omega)\\B\\} \\right] = \\mathbb{P}[X^{-1}(B)]\n  \\] dla każdego \\(B\\\\mathcal{B}(\\mathbb{R})\\), nazywamy rozkładem zmiennej losowej \\(X\\).W ten sposób zdefiniowaliśmy nową przestrzeń probabilistyczną \\((\\mathbb{R}, \\mathcal{B}(\\mathbb{R}),\\mu_X)\\).\nnie jest już abstrakcyjna przestrzeń \\(\\Omega\\), ale\nprzestrzeń o której sporo wiemy (dysponujemy narzędziami analitycznymi, na \\(\\mathbb{R}\\) zachodzi twierdzenie Kołmogorowa).Przykład 6.2  Wykonujemy \\(n\\) prób Bernoulliego z prawdopodobieństwem sukcesu \\(p\\) w pojedynczej próbie.\nNiech \\(S_n\\) będzie liczbą sukcesów. Wówczas\n\\[\\begin{equation*}\n    \\mathbb{P}[S_n=k]=p_k={n \\choose k}p^k(1-p)^{n-k}.\n\\end{equation*}\\]\nOznacza , że\n\\[\\begin{equation*}\n    \\mathbb{P}[S_n\\B] = \\sum_{k \\B} p_k = \\sum_{k=0}^n p_k\\delta_k(B).\n\\end{equation*}\\]\nInnymi słowy rozkład \\(\\mu_{S_n}\\) jest równy\n\\[\\begin{equation*}\n    \\mu_{S_n}(\\cdot) = \\sum_{k=0}^n p_k\\delta_k(\\cdot).\n\\end{equation*}\\]Powyższy przykład można bardzo łatwo uogólnić. Rzeczywiście, jeżeli zmienna losowa \\(X\\)\njest taka, że istnieje przeliczalny zbiór \\(\\{x_k\\}_{k \\\\mathbb{N}}\\) taki, że\n\\[\\begin{equation*}\n    \\sum_{k=1}^\\infty\\mathbb{P}[X-x_k] =1,\n\\end{equation*}\\]\njej rozkład zadany jest przez\n\\[\\begin{equation*}\n    \\mu_X(\\cdot)=\\sum_{k=1}^\\infty p_k\\delta_{x_k}(\\cdot),\n\\end{equation*}\\]\ngdzie\n\\[\\begin{equation*}\n    p_k = \\mathbb{P}[X = x_k], \\quad k \\\\mathbb{N}.\n\\end{equation*}\\]\nPowyższe stosuje się każdej zmiennej losowej określonej na zbiorze przeliczalnym.\nAby móc w podobny sposób analizować zmienne losowe określone na większych przestrzeniach\nmusimy wprowadzić dodatkowy aparat.Definicja 6.3  Dystrybuantą zmiennej losowej \\(X\\) nazywamy funkcję \\(F:\\; \\mathbb{R} \\[0,1]\\) zadaną wzorem\n\\[\n  F(t) = \\mathbb{P}[X\\le t] = \\mu_X\\left((-\\infty,t]\\right).\n  \\]Przykład 6.3  Rzucamy monetą. \\(\\Omega = \\{O,R\\}\\), \\(X(O)=1\\), \\(X(R) = -1\\). Wtedy\n\\[\n  F(t) = \\mathbb{P}[X\\le t] = \\left\\{\n  \\begin{array}{cc}\n    0 & \\mbox{ dla } t < -1 \\\\\n    1/2 & \\mbox{ dla } -1\\le t < 1 \\\\\n    1 &  \\mbox{ dla } t \\ge 1.\n  \\end{array}\n  \\right.\n  \\]Przykład 6.4  Losowa liczba z przedziału \\([0,1]\\): \\((\\Omega, \\mathcal{F}, \\mathbb{P})\n  = ([0,1], \\mathcal{B}([0,1]), \\lambda_1)\\), \\(X(\\omega) = \\omega\\).\n\\[\n  F(t) = \\mathbb{P}[X\\le t] = \\left\\{\n  \\begin{array}{cc}\n    0 & \\mbox{ dla } t < 0 \\\\\n    t & \\mbox{ dla } 0\\le t < 1 \\\\\n    1 &  \\mbox{ dla } t \\ge  1.\n  \\end{array}\n  \\right.\n  \\]Przykład 6.5  Rozważmy pierwszy sposób losowania w paradoksie Bertranda.\n\\[\n  F(t) = \\mathbb{P}[X_1\\le t] = \\left\\{\n  \\begin{array}{cc}\n    0 & \\mbox{ dla } t < 0 \\\\\n    \\frac{2}{\\pi} \\arcsin(t/2) & \\mbox{ dla } 0\\le t < 2 \\\\\n    1 &  \\mbox{ dla } t \\ge  2.\n  \\end{array}\n  \\right.\n\\]Twierdzenie 6.2  (Własności dystrybuanty) Niech \\(F\\) będzie dystrybuantą pewnej zmiennej losowej \\(X\\). Wówczas\\(F\\) jest niemalejąca.\\(\\lim_{t\\-\\infty} F(t) = 0\\), \\(\\lim_{t\\ \\infty} F(t) = 1\\).\\(F\\) jest prawostronnie ciągła.dla dowolnego \\(t\\\\mathbb{R}\\) istnieje lewostronna granica\n\\[F(t-) = \\lim_{s\\t^-}F(s) = \\mathbb{P}[X<t].\\]\\(F\\) jest nieciągła w punkcie \\(t_0\\) wtedy tylko wtedy, gdy \\(\\mathbb{P}[X = t_0] > 0\\). Wówczas\n\\(\\mathbb{P}[X=t_0] = F(t_0) - F(t_0-)\\). Punkt \\(t_0\\) nazywamy wówczas atomem rozkładu.Proof. Punkt 1. Jeżeli \\(t_1<t_2\\), zachodzi inkluzja \\((-\\infty,t_1] \\subset (-\\infty, t_2]\\). Wówczas:\n\\[\nF(t_1) = \\mu((-\\infty,t_1]) \\leq \\mu((-\\infty, t_2]) = F(t_2).\n\\]Punkt 2. Niech \\(\\{t_n\\}\\) będzie dowolnym ciągiem rosnącym \\(+\\infty\\). Wówczas rodzina zbiorów \\((-\\infty,t_n]\\) jest rosnąca, ponadto:\n\\[\n\\mathbb{R} = \\bigcup_n (-\\infty,t_n].\n\\]\nZ twierdzenia o ciągłości otrzymujemy:\n\\[\n\\lim_{n\\\\infty} F(t_n) = \\lim_{n\\\\infty} \\mu((-\\infty,t_n]) = \\mu(\\mathbb{R}) = 1.\n\\]\nAnalogicznie dowodzimy drugiej części.Punkt 3. Ustalmy \\(t \\\\mathbb{R}\\) niech \\(\\{t_n\\}\\) będzie ciągiem malejącym \\(t\\). Wówczas ciąg przedziałów \\((t,t_n]\\) jest malejący spełnia:\n\\[\n\\bigcap_n (t,t_n] = \\emptyset.\n\\]\nZ twierdzenia o ciągłości otrzymujemy:\n\\[\\begin{multline*}\n\\lim_{n\\\\infty} \\left( F(t_n) - F(t) \\right)\n= \\lim_{n\\\\infty} \\left( \\mu((-\\infty,t_n]) - \\mu((-\\infty, t]) \\right)\\\\\n= \\lim_{n\\\\infty} \\mu((t,t_n]) = \\mu(\\emptyset) = 0.\n\\end{multline*}\\]Punkt 4. Dowód przebiega analogicznie jak w punkcie 3.Punkt 5. Jest konsekwencją punktu 4.Twierdzenie 6.3  Jeżeli \\(F\\) jest funkcją na \\(\\mathbb{R}\\) spełniającą warunki 1,2 3 z poprzedniego twierdzenia:\\(F\\) jest niemalejąca;\\(\\lim_{t\\-\\infty} F(t) = 0\\), \\(\\lim_{t\\ \\infty} F(t)\\) = 1;\\(F\\) jest prawostronnie ciągła;\\(F\\) jest dystrybuantą pewnego rozkładu.Proof. Naszym celem jest skonstruowanie przestrzeni probabilistycznej\n\\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) oraz zmiennej losowej \\(X\\) na niej określonej\ntakiej, że\n\\(F\\) jest dystrybuantą \\(X\\), tzn.\n\\[\\begin{equation}\n    F(t) = \\mathbb{P}[X\\le t]\n\\tag{6.1}\n\\end{equation}\\]\nZdefiniujmy \\((\\Omega, \\mathcal{F}, \\mathbb{P}) = \\big( (0,1), \\mathcal{B}((0,1)), {\\rm Leb} \\big)\\).Załóżmy najpierw, że funkcja \\(F\\) jest odwracalna zdefiniujmy\n\\[\nX(\\omega) = F^{-1}(\\omega).\n\\]\nMusimy najpierw sprawdzić, że \\(X\\) jest mierzalna.\nZauważmy, że dla dowolnego \\(t\\\\mathbb{R}\\),\n\\[\nX^{-1}((-\\infty, t]) = \\{\\omega:\\; X(\\omega) \\le t \\} = \\{\\omega:\\; \\omega \\le F(t)\\} = [0,F(t)] \\\\mathcal{B}(0,1).\n\\]\nDla sprawdzenia (6.1) piszemy\n\\[\n\\mathbb{P}[X(\\omega) \\le t] = \\mathbb{R}[ \\omega \\le F(t)] = F(t).\n\\]\nRozważmy teraz ogólny przypadek.\nZdefiniujmy zmienną losową \\(X\\)\njako uogólnioną funkcję odwrotną \\(F\\):\n\\[\\begin{equation}\nX(\\omega) = F^{-1}(\\omega):= \\sup\\{y\\\\mathbb{R}:\\; F(y)< \\omega\\}.\n\\tag{6.2}  \n\\end{equation}\\]\nPokażemy, że \\(F\\) jest dystrybuantą \\(X\\), tzn. zachodzi (6.1).\nW tym celu wystarczy pokazać\n\\[\\begin{equation}\n\\{ \\omega:\\; X(\\omega) \\le t \\} =\\{\\omega:\\; \\omega \\le F(t)\\}\n\\tag{6.3}\n\\end{equation}\\]\ndla każdego \\(t\\).Oznaczmy przez \\(L\\) (\\(P\\)) zbiór po lewej (prawej) stronie formuły (6.3).\nPokażemy najpierw, że \\(L\\supset P\\). Istotnie, niech\n\\(\\omega\\P\\), tzn. \\(\\omega \\le F(t)\\).\nWtedy \\[t\\notin \\{ y\\\\mathbb{R}:\\; F(y)< \\omega\\}.\\]\nSkoro \\(F\\) jest monotoniczna, \n\\[\nt \\geq \\sup\\{ y \\\\mathbb{R} \\: : \\: F(y)<\\omega\\} = F^{-1}(\\omega)=X(\\omega)\n\\]\nzatem \\(X(\\omega) \\le t\\).dowodu odwrotnej implikacji, niech \\(\\omega\\notin P\\), czyli \\(\\omega >F(t)\\).\nKorzystając z prawostronnej ciągłości dystrybuanty \\(F\\),\nistnieje \\(\\varepsilon > 0\\) tż. \\(F(t+\\varepsilon) < \\omega\\).\nZatem \\[t+\\varepsilon \\\\{ y\\\\mathbb{R}:\\; F(y) < \\omega \\}.\\]\nStąd\n\\[\nt<t+\\varepsilon \\leq \\sup\\{ y \\\\mathbb{R} \\: : \\: F(y)<\\omega\\} = F^{-1}(\\omega) = X(\\omega).\n\\]\nSkoro\n\\(X(\\omega)>t\\), czyli \\(\\omega \\notin L\\).\nZ równości (6.3) wynika, że \\(X\\) jest zmienną losową.\nAby pokazać, że \\(X\\) ma zadaną dystrybuantę\n\\[\n\\mathbb{P}[X\\le t] = \\mathbb{P}\\left[ \\{ \\omega:\\; X(\\omega) \\le t \\}\\right]\n=\\mathbb{P}\\left[\\{\\omega:\\; \\omega \\le F(t)\\}\\right] = F(t),\n\\]\ngdzie ostatnia równość wynika z definicji miary probabilistycznej \\(\\mathbb{P}\\),\nktóra jest miarą Lebesgue’na \\([0,1]\\).Twierdzenie 6.4  (Twierdzenie o jednoznaczności) Dystrybuanta zmiennej losowej \\(X\\) wyznacza jednoznacznie jej rozkład.Powyższy wynik jest konsekwencją lematu o \\(\\pi-\\lambda\\) układach zwanym też twierdzeniem Dynkina.Definicja 6.4  Niepustą rodzinę \\(\\mathcal{K}\\) podzbiorów \\(\\Omega\\) nazywamy \\(\\pi\\)-układem,\njeżeli jest zamknięta na operację przekroju, tzn. \\(\\cap B\\\\mathcal{K}\\) dla wszystkich \\(,B\\\\mathcal{K}\\).Definicja 6.5  Niepustą rodzinę \\(\\mathcal{L}\\) podzbiorów \\(\\Omega\\) nazywamy \\(\\lambda\\)-układem,\njeżeli\\(\\Omega \\\\mathcal{L}\\).jeżeli \\(,B\\\\mathcal{L}\\) \\(\\subset B\\), \\(B\\setminus \\\\mathcal{L}\\).jeżeli \\(A_1,A_2,...\\) jest wstępującym ciągiem elementów \\(\\mathcal{L}\\),\n\\(\\bigcup_{n=1}^\\infty A_n \\\\mathcal{L}\\).Lemma 6.1  (Dynkin) Jeżeli \\(\\mathcal{L}\\) jest \\(\\lambda\\)-układem zawierającym \\(\\pi\\)-układ \\(\\mathcal{K}\\),\n\\(\\mathcal{L}\\) zawiera także \\(\\sigma(\\mathcal{K})\\),\n\\(\\sigma\\)-ciało generowane przez \\(\\mathcal{K}\\).Proof. Krok 1. Pokażemy najpierw, że jeżeli \\(\\mathcal{L}\\) jest jednocześnie \\(\\pi\\)-układem oraz \\(\\lambda\\)-układem, jest \\(\\sigma\\)-ciałem. Istotnie\n\\(\\mathcal{L}\\) jest zamknięte na operację sumy: jeżeli \\(,B\\\\mathcal{L}\\), \n\\[\n\\cup B = \\cup \\big(  B\\setminus \\cap B \\big) =\n\\big( ^c \\setminus (B\\setminus \\cap B)   \\big)^c \\\\mathcal{L}.\n\\] Przez indukcję pokazuje się, że \\(\\mathcal{L}\\) jest zamknięte na skończone sumy,\ntzn. jeżeli \\(A_1,\\ldots, A_n\\\\mathcal{L}\\), \n\\(A_1\\cup A_2\\cup\\ldots\\cup A_n\\\\mathcal{L}\\).\nZałóżmy teraz, że \\(\\{A_n\\}_{n\\\\mathbb{N}}\\) jest przeliczalnym ciągiem elementów \\(\\mathcal{L}\\),\nwówczas \\(\\bigcup_{k=1}^n A_k\\) jest ciągiem wstępującym elementów \\(\\mathcal{L}\\), stąd\n\\[\n\\bigcup_{n=1}^\\infty A_n = \\bigcup_{n=1}^\\infty \\bigcup_{k=1}^n A_k \\\\mathcal{L},\n\\] zatem \\(\\mathcal{L}\\) jest \\(\\sigma\\)-ciałem.Krok 2. Niech \\(\\mathcal{L}_0\\) będzie przekrojem wszystkich \\(\\lambda\\)-układów\nzawierających \\(\\mathcal{K}\\). Wystarczy pokazać, że \\(\\mathcal{L}_0\\) jest \\(\\pi\\)-układem,\nbo wówczas z kroku 1 jest \\(\\sigma\\)-ciałem \n\\[\n\\mathcal{K} \\subset \\sigma(\\mathcal{K}) \\subset \\mathcal{L}_0 \\subset \\mathcal{L}.\n\\]\nUstalmy \\(\\\\mathcal{K}\\) rozważmy rodzinę\n\\[\\mathcal{K}_1^= \\{    B\\subset \\Omega:\\; \\cap B\\\\mathcal{L}_0   \\}.\\]\nWówczas \\(\\mathcal{K}\\subset  \\mathcal{K}_1^\\) (bo \\(\\mathcal{K}\\) jest \\(\\pi\\)-układem),\nale ponadto \\(\\mathcal{K}_1^\\) jest \\(\\lambda\\)-układem:\\(\\Omega \\\\mathcal{K}_1^\\);jeżeli \\(B_1,B_2\\\\mathcal{K}_1^\\) oraz \\(B_1\\subset B_2\\), \n\\[\\cap (B_2\\setminus B_1) = (\\cap B_2) \\setminus (\\cap B_1) \\\\mathcal{L}_0;\\]jeżeli \\(B_n\\) jest wstępującą rodziną elementów \\(\\mathcal{K}_1^\\), \n\\[\n\\cap \\bigg(\\bigcup B_n \\bigg) = \\bigcup (\\cap B_n) \\\\mathcal{L}_0\n\\] zatem \\(\\bigcup B_n \\\\mathcal{K}_1^\\).Pokazaliśmy, że \\(\\mathcal{K}_1^\\) jest \\(\\lambda\\)-układem zawierającym \\(\\mathcal{K}\\)\nStąd wynika, że \\(\\mathcal{L}_0 \\subseteq \\mathcal{K}_1^.\\).\nCzyli jeżeli \\(\\\\mathcal{K}, B\\\\mathcal{L}_0\\), \\(\\cap B \\\\mathcal{L}_0\\).Następnie ustalmy dowolny zbiór\n\\(B\\\\mathcal{L}_0\\) zdefiniujmy \\[\\mathcal{K}_2^B = \\{ : \\cap B\\\\mathcal{L}_0  \\}.\\]\nsamo rozumowanie co powyżej uzasadnia, że \\(\\mathcal{K}_2^B\\) jest \\(\\lambda\\)-układem oraz\nzawiera rodzinę \\(\\mathcal{K}\\), więc \\(\\mathcal{L}_0 \\subseteq \\mathcal{K}_2^B\\).\nPodsumowując pokazaliśmy, że jeżeli\n\\(,B\\\\mathcal{L}_0\\), \\(\\cap B\\\\mathcal{L}_0\\), więc \\(\\mathcal{L}_0\\) jest \\(\\pi\\)-układem,\nco kończy dowód.Proof. Dowód twierdzenia o jednoznaczności (Twierdzenie 6.4)\nChcemy pokazać, że jeżeli \\(X\\) \\(Y\\) są dwoma zmiennymi losowymi o tej samej dystrybuancie \\(F\\), muszą mieć te rozkłady:\n\\[\\begin{equation}\n\\mu_X(B) = \\mu_Y(B),\n\\tag{6.4}\n\\end{equation}\\]\ndla wszystkich \\(B\\\\mathcal{B}(\\mathbb{R})\\). (Zauważmy, że \\(X\\) \\(Y\\) mogą być\nokreślone na różnych przestrzeniach probabilistycznych. Pokazujemy równość\nrozkładów, nie równość zmiennych losowych.)Z definicji dystrybuanty wynika, że (6.4) zachodzi dla wszystkich zbiorów\npostaci \\((-\\infty,t]\\). Oznaczmy te zbiory przez \\(\\mathcal{K}\\). Tworzą one \\(\\pi\\)-układ. Niech\n\\[\n\\mathcal{L} = \\big\\{ \\\\mathcal{B}(\\mathbb{R}):\\; \\mu_X() = \\mu_Y()  \\big\\}.\n\\] Rodzina \\(\\mathcal{L}\\) jest \\(\\lambda\\)-układem. Zatem z lematu o \\(\\pi-\\lambda\\) układach\n\\[\n\\mathcal{L}\\supset \\sigma(\\mathcal{K}) = \\mathcal{B}((\\mathbb{R}).\n\\] pokazuje, że (6.4) zachodzi dla wszystkich zbiorów borelowskich,\nwięc obie miary \\(\\mu_X\\) \\(\\mu_Y\\) są równe.","code":""},{"path":"wartość-oczekiwana-definicja-i-własności.html","id":"wartość-oczekiwana-definicja-i-własności","chapter":"7 Wartość oczekiwana: definicja i własności","heading":"7 Wartość oczekiwana: definicja i własności","text":"Definicja 7.1  Niech \\(X\\) będzie zmienną losową określoną na przestrzeni probabilistycznej\n\\((\\Omega, \\mathcal{F}, \\mathbb{P})\\). Mówimy, że \\(X\\) ma wartość oczekiwaną jeżeli\n\\[\n  \\int_\\Omega |X| \\mathrm{d} \\mathbb{P} = \\int_\\Omega |X(\\omega)|\\mathbb{P}(\\mathrm{d}\\omega) <\\infty.\n\\]\nWówczas wartością oczekiwaną zmiennej losowej \\(X\\) nazywamy liczbę\n\\[\n    \\mathbb{E}[ X ] =  \\int_\\Omega X \\mathrm{d}\\mathbb{P} =\n    \\int_{\\Omega}X(\\omega) \\mathbb{P}(\\mathrm{d}\\omega).\n\\]Przykład 7.1  Rzucamy kostką. Niech \\(X\\) oznacza liczbę wyrzuconych oczek.\nWówczas, dla \\(\\Omega = [6]=\\{1,2, \\ldots, 6\\}\\),\n\\[\\begin{equation*}\n    \\mathbb{E}[X] = \\sum_{k=1}^6 k \\mathbb{P}[\\{ k\\}] = 3,5.\n\\end{equation*}\\]Jak widzimy po powyższym przykładzie nazwy “wartość oczekiwana” nie należy interpretować\ndosłownie. W rzucie kostką nie będziemy oczekiwać wartości \\(3,5\\).Zauważmy, że jeżeli \\(\\Omega = \\{\\omega_j\\}_j\\) jest przestrzenią dyskretną\n\\(\\mathbb{P}[\\{\\omega_i\\}]=p_i\\), \n\\[\n\\mathbb{E} [X] =\\sum_{=1}^n X(\\omega_i)p_i.\n\\]\nWartość oczekiwaną należy rozumieć jako średnią ważoną \\(X\\).Istnieje jeszcze jedno spojrzenie na wartość oczekiwaną. Załóżmy, że rzucamy kostką \\(n\\) razy.\nNiech \\(X_j\\) oznacza wynik \\(j\\)tego rzutu. Wówczas średnia empiryczna (uzyskanych wyników) \n\\[\\begin{equation*}\n    \\frac 1n \\sum_{j=1}^n X_j.\n\\end{equation*}\\]\nMożemy ją napisać równoważnie jako\n\\[\\begin{multline*}\n    \\frac 1n \\sum_{j=1}^n X_j=\n    1 \\frac{\\# \\{ j \\leq n \\: : \\: X_j=1\\}}{n}+\n    2 \\frac{\\# \\{ j \\leq n \\: : \\: X_j=2\\}}{n}+ \\ldots\\\\    \n    +6 \\frac{\\# \\{ j \\leq n \\: : \\: X_j=6\\}}{n}.\n\\end{multline*}\\]\nJeżeli \\(n\\) jest bardzo duże, spodziewamy się, że\n\\[\\begin{equation*}\n\\frac{\\# \\{ j \\leq n \\: : \\: X_j=k\\}}{n} \\\\mathbb{P}[\\{k\\}]=\\frac 16.\n\\end{equation*}\\]\nDokładne uzasadnienie powyższego faktu zobaczymy w dalszej części wykładu.\nChcąc zaagitować za naturalnością przyjętej przez nas definicji wartości oczekiwanej\nprzyjmijmy go chwilowo za prawdziwy. Wówczas średnia empiryczna zbiega \n\\[\\begin{equation*}\n    \\frac 1n \\sum_{j=1}^n X_j \\\\sum_{k=1}^6 k \\mathbb{P}[\\{ k\\}]\n\\end{equation*}\\]\nczyli wartości oczekiwanej wyniku jednego rzutu. Powyższy heurystyczny argument nie\njest zależny od wyważenia czy też liczby ścian na kostce.Przykład 7.2  Dwaj gracze \\(\\) \\(B\\) grają w następującą grę.\nRzucają kostką. Niech \\(k\\) będzie wynikiem rzutu.\nJeżeli \\(k\\) jest nieparzyste, \\(\\) wygrywa \\(k\\) złotych, jeżeli \\(k\\) jest parzyste,\n\\(B\\) wygrywa \\(k\\) złotych. Czy gra jest uczciwa?Wartość oczekiwana jest najważniejszym parametrem decydującym o opłacalności np. gier\nhazardowych. MIT Students Won 8 Millions Massachusetts Lottery.Twierdzenie 7.1  Załóżmy, że \\(X\\) \\(Y\\) są zmiennymi losowymi takimi, że \\(\\mathbb{E} X\\) \\(\\mathbb{E} Y\\) istnieją.\nWtedyJeżeli \\(X\\ge 0\\), \\(\\mathbb{E} [X] \\ge 0\\).\\(|\\mathbb{E} [X]| \\le \\mathbb{E} [|X|]\\).Dla dowolnych \\(,b\\\\mathbb{R}\\)\n\\[\n\\mathbb{E}[aX+] = \\mathbb{E} [X] + b\\mathbb{E}[Y].\n\\]Proof. Wszystkie własności są konsekwencją definicji wartości oczekiwanej jako całki z funkcji\nmierzalnej \\(X\\).\nTrzeci punkt wynika z liniowości całki\n\\[\\begin{multline*}\n    \\mathbb{E}[AX+] = \\int_\\Omega aX+\\mathrm{d}\\mathbb{P} =\\\\\n    \\int_\\Omega X\\mathrm{d}\\mathbb{P} +\n    b\\int_\\Omega Y \\mathrm{d}\\mathbb{P}\n    =\\mathbb{E}[X]+b\\mathbb{E}[Y].\n\\end{multline*}\\]Jeżeli \\(X \\geq 0\\), \n\\[\\begin{equation*}\n    \\mathbb{E}[X]=\\int_\\Omega X \\mathrm{d}\\mathbb{P} \\geq 0.\n\\end{equation*}\\]\nPodobnie jeżeli \\(X \\geq Y\\), \n\\[\\begin{equation*}\n    \\int_\\Omega X \\mathrm{d}\\mathbb{P}\n\\geq \\int_\\Omega Y \\mathrm{d}\\mathbb{P}\n\\end{equation*}\\]\npoprzez chociażby zastosowanie poprzedniego pierwszego punktu zmiennej losowej \\(X'=X-Y\\).\nSkoro \\(-|X|\\leq X \\leq |X|\\), \n\\[\\begin{equation*}\n    -\\int_\\Omega |X| \\mathrm{d}\\mathbb{P}\n\\leq \\int_\\Omega X \\mathrm{d}\\mathbb{P}\n\\leq \\int_\\Omega |X| \\mathrm{d}\\mathbb{P}\n\\end{equation*}\\]\nczyli\n\\[\\begin{equation*}\n-\\mathbb{E}[|X|]\\leq \\mathbb{E}[X] \\leq \\mathbb{E}[|X|]\n\\end{equation*}\\]\nco pociąga drugi punkt.Przykład 7.3  Rzucamy \\(100\\) razy kostką. Niech \\(X\\) oznacza sumę wyrzuconych oczek. Jaka jest wartość oczekiwana \\(X\\)?\nBezpośrednie użycie definicji jest kłopotliwe, ponieważ przestrzeń zdarzeń elementarnych jest\nbardzo duża. Rozważymy inne podejście.\nNiech \\(X_i\\) oznacza liczbę oczek uzyskanych w \\(\\)-tym rzucie. Wtedy\n\\[\nX = \\sum_{=1}^{100} X_i.\n\\]\nPonadto \\(\\mathbb{E} X_i = 3,5\\).\nZatem z liniowości wartości oczekiwanej\n\\[\n\\mathbb{E} [X] = \\sum_{=1}^{100} \\mathbb{E} [X_i] = 350.\n\\]Przykład 7.4  Niech \\(\\sigma\\) będzie losową permutacją zbioru \\(\\{1,\\ldots,n\\}\\)\n(losujemy jednostajnie element grupy permutacji \\(S_n\\)).\nNiech \\(X\\) oznacza liczbę punktów stałych. Obliczymy \\(\\mathbb{E} X\\).\nTen przykład ma wiele alternatywnych sformułowań np. wkładamy losowo \\(n\\)\nlistów zaadresowanych kopert, ilu adresatów otrzyma właściwy list.\nOznaczmy\n\\[X_i = \\left\\{\\begin{array}{cc}\n                 1 & \\mbox{ jeżeli $$-ty punkt jest stały} \\\\\n                 0 & \\mbox{ jeżeli $$-ty punkt nie jest stały}\n               \\end{array}\n\\right.\n\\]\nJeżeli oznaczymy przez \\(A_i\\) zdarzenie, że \\(\\)-ty punkt jest stały, \\(X_i = \\mathbf{1}_{A_i}\\).\nWtedy\n\\[\n\\mathbb{E} [X_i] = \\int_\\Omega \\mathbf{1}_{A_i}(\\omega) \\mathbb{P}(\\mathrm{d}\\omega) = \\mathbb{P}[A_i] = 1/n\n\\]\noraz \\(X = \\sum_{=1}^n X_i\\), stąd\n\\[\\mathbb{E} [X]  = \\sum_{=1}^n \\mathbb{E} [X_i] = n\\cdot \\frac 1n = 1.\\]Twierdzenie 7.2  Załóżmy, że \\(\\{X_n\\}_{n \\\\mathbb{N}}\\) jest ciągiem zmiennych losowych takich,\nże \\(\\mathbb{E} [X_n]\\) istnieją. Wtedy(Lemat Fatou) Jeżeli \\(X\\ge 0\\), \n\\[\n\\mathbb{E} \\left[\\liminf_{n\\\\infty} X_n\\right] \\le \\liminf_{n\\\\infty} \\mathbb{E} \\left[X_n\\right]\n\\](tw. o zbieżności monotonicznej)\nJeżeli \\(X_n\\ge 0\\) oraz \\(\\{X_n\\}_{n \\\\mathbb{N}}\\) jest ciągiem monotonicznym, \n\\[\n            \\mathbb{E} \\left[\\lim_{n\\\\infty} X_n \\right]= \\lim_{n\\\\infty} \\mathbb{E} \\left[X_n\\right]\n        \\](tw. Lebesgue’o zbieżności zmajoryzowanej) Jeżeli \\(|X_n|\\le Y\\) dla pewnej\nzmiennej losowej \\(Y\\) takiej, że\n\\(\\mathbb{E} |Y|<\\infty\\) oraz istnieje granica \\(\\lim_{n\\\\infty} X_n(\\omega) = X(\\omega)\\)\ndla \\(\\mathbb{P}\\)p.k.\\(\\omega\\), \n\\[\n\\mathbb{E} [X] =  \\mathbb{E}\\left[ \\lim_{n\\\\infty} X_n\\right] = \\lim_{n\\\\infty} \\mathbb{E} \\left[X_n\\right].\n\\]","code":""},{"path":"wartość-oczekiwana-zastosowania.html","id":"wartość-oczekiwana-zastosowania","chapter":"8 Wartość oczekiwana: zastosowania","heading":"8 Wartość oczekiwana: zastosowania","text":"Twierdzenie 8.1  Niech \\(f:\\mathbb{R}\\\\mathbb{R}\\) będzie funkcją borelowską, \\(X\\) zmienną\nlosową o rozkładzie \\(\\mu_X\\). Wówczas\n\\[\n  \\mathbb{E}\\left[ f(X)\\right]  = \\int_{\\mathbb{R}} f(x)\\mu_X(\\mathrm{d}x),\n\\]\no ile jedna z tych całek istnieje.Przed dowodem zobaczmy proste zastosowanie tego faktu.Przykład 8.1  Ile średnio należy wykonać rzutów kostką, aby otrzymać pierwszą szóstkę?\nOgólniej wykonujemy doświadczenie o prawdopodobieństwie sukcesu \\(p\\).\nIle średnio razy należy je powtórzyć, aby otrzymać sukces.Niech \\(X\\) będzie liczbą prób, które należy wykonać, aby otrzymać pierwszy sukces. Wówczas\n\\[\n\\mathbb{P}[X=k] = p_k= (1-p)^{k-1}p.\n\\]\nZatem rozkład \\(X\\) zadaje się przez\n\\[\\begin{equation*}\n    \\mu_X(\\cdot) = \\sum_{k=1}^\\infty p_k\\delta_k(\\cdot).\n\\end{equation*}\\]\nWzór z Twierdzenia 8.1 zapisuje się jako\n\\[\\begin{equation*}\n    \\mathbb{E}[f(X)] = \\int_\\mathbb{R} f(x) \\mu_X(\\mathrm{d}x)\n    = \\sum_{k=1}^\\infty f(k) p_k.\n\\end{equation*}\\]\nBiorąc \\(f(x)=x\\) otrzymujemy interesująca nas wartość oczekiwaną \\(X\\),\n\\[\n\\mathbb{E}\\left[ X\\right]  =\n\\sum_{k=1}^{\\infty} kp(1-p)^{k-1} =\np \\sum_{k=1}^{\\infty} k (1-p)^{k-1} = \\frac 1p.\n\\]\nOstatnia równość wynika z różniczkowania szeregu\n\\[\\begin{equation*}\n    \\sum_{k=0}^\\infty q^k = \\frac {1}{1-q}\n\\end{equation*}\\]\nabsolutnie zbieżnego dla \\(q\\(0,1)\\). Po zróżniczkowaniu stronami otrzymujemy\n\\[\\begin{equation*}\n    \\sum_{k=1}^\\infty kq^{k-1} = \\frac{1}{(1-q)^2}.\n\\end{equation*}\\]Proof (Twierdzenia 8.1). Dowód używa standardowych metod z teorii miary.\nPrzybliża się funkcję \\(f\\) funkcjami prostymi.Krok 1.\nJeżeli \\(f = {\\bf 1}_A\\) dla \\(\\\\mathcal{B}(\\mathbb{R})\\), \\(f(X)\\) jest zmienną\nlosową przyjmującą 2 wartości: \\(0\\) \\(1\\).\nMożemy więc napisać\n\\[\\begin{multline*}\n  \\mathbb{E} f(X) = 1\\cdot \\mathbb{P}[X\\] + 0 \\cdot \\mathbb{P}[X\\notin ] = \\mu_X()\\\\ =\n  \\int_A  \\mu_X(\\mathrm{d}x) = \\int_{\\mathbb{R}}  {\\bf 1}_A(x) \\mu_X(\\mathrm{d}x)\n  = \\int_{\\mathbb{R}}  f(x) \\mu(\\mathrm{d}x).\n\\end{multline*}\\]Krok 2. Jeżeli \\(f = \\sum_{=1}^n a_i {\\bf 1}_{A_i}\\) jest funkcją prostą, teza\nwynika z kroku 1 oraz liniowości wartości oczekiwanej.Krok 3. Niech \\(f\\ge 0\\) niech \\(f_n\\) będzie rosnącym ciągiem funkcji\nprostych zbiegającym punktowo \\(f\\). Wtedy teza wynika z twierdzenia o zbieżności monotonicznej.Krok 4. Dla dowolnej funkcji \\(f\\) zapisujemy ją w postaci \\(f = f^+ - f^-\\) korzystamy z kroku 3.Przykład 8.2  Kupujemy \\(k\\) losów w loterii, w której \\(M\\) losów jest przegrywających,\n\\(N\\) wygrywających (zakładamy \\(k\\le M,N\\)).\nNiech \\(X\\) będzie liczbą losów wygrywających wśród\ntych, które kupiliśmy. Znajdziemy \\(\\mathbb{E} [X]\\).1 sposób Zdefiniujmy\n\\[X_i = \\left\\{\\begin{array}{cc}\n                 1 & \\mbox{ jeżeli $$-ty los wygrywa} \\\\\n                 0 & \\mbox{ jeżeli $$-ty los przegrywa}\n               \\end{array}\n\\right.\n\\]\nWtedy \\(X = \\sum_{=1}^k X_i\\) oraz \\(\\mathbb{E} X_i = \\frac{N}{N+M}\\)\n\\[ \\mathbb{E} X = \\sum_{=1}^k \\mathbb{E} X_i  =  \\frac{kN}{N+M}.\\]\n2 sposób Szkolna metoda:\n\\[\n\\mathbb{E} X = \\sum_{j=0}^k j \\mathbb{P}[X=j] = \\sum_{j=0}^k j\\cdot \\frac{{N\\choose j}{M\\choose k-j}}{{N+M \\choose k}}\n\\]\nObie metody muszą prowadzić tego samego wyniku, stąd\n\\[\n\\sum_{j=0}^k j\\cdot \\frac{{N\\choose j}{M\\choose k-j}}{{N+M \\choose k}} = \\frac{kN}{N+M}.\n\\]Przykład 8.3  Tasujemy talię 52 kart metodą Top random, tzn. kartę, która znajduje się na górze\nwkładamy w losowe miejsce w talii (są 52 możliwości), następnie powtarzamy czynność.\nIle należy wykonać tasowań, aby można było uznać talię za potasowaną.\nJaka jest średnia liczba tasowań?Kluczowa jest karta, która początkowo znajduje się na samym dole talii. Oznaczmy ją przez \\(\\).\nZauważmy, że w trakcie tasowania będzie się ona przemieszczać powoli góry talii.\nPrawdopodobieństwo, że w pierwszym ruchu karta z góry talii znajdzie się pod \\(\\) wynosi \\(1/52\\).\nJest ono małe, ale z prawdopodobieństwem 1 (Lemat Borela-Cantellego) nastąpi moment,\ngdy pod \\(\\) znajdzie się jakaś karta, oznaczmy ją przez \\(K_1\\).Po wykonaniu losowej liczby tasowań kolejna karta \\(K_2\\) zostanie wstawiona poniżej \\(\\).\nWzajemne ułożenie kart \\(K_1\\) \\(K_2\\) jest losowe (tzn. z prawdopodobieństwem \\(1/2\\) karta \\(K_1\\)\njest nad \\(K_2\\) na odwrót). Kontynuując rozumowanie w pewnej chwili pod \\(\\) będzie\nznajdować się \\(k\\) kart: \\(K_1, \\ldots K_k\\). Każde ułożenie tychże kart jest tak samo prawdopodobne\n(indukcja!). Zatem karty pod \\(\\) są dobrze potasowane.Karta \\(\\) dotrze w końcu na sam szczyt talii właśnie ona zostanie wstawiona.\nW tej chwili wszystkie karty będą dobrze potasowane (każde ich wzajemne ułożenie jest\njednakowo prawdopodobne).Obliczmy ile czasu potrzebuje karta \\(\\), aby dotrzeć na górę talii.\nNiech \\(X_i\\) oznacza czas jaki karta \\(\\) spędza na \\(\\)-tej pozycji od dołu. Prawdopodobieństwo,\nże karta wstawiona zostanie poniżej \\(\\) wynosi \\(1/52\\). Czas oczekiwania na sukces \\(X_1\\),\nzgodnie z poprzednim przykładem jest zmienną\nlosową o rozkładzie Geom(\\(1/52\\)), zatem \\(\\mathbb{E} X_1 = 52\\).\nPodobnie dla każdego \\(\\): \\(\\mathbb{E} X_i = 52/\\). Podsumowując\n\\[\\mathbb{E} X = \\mathbb{E} \\sum_{=1}^{52 } X_i = \\sum_{=1}^{52} \\mathbb{E} X_i = \\sum_{=1}^{52} \\frac{52}\n= 52 \\sum_{=1}^{52} \\frac 1i \\sim 52 (\\log 52 + \\gamma) \\sim 235, \\]\ngdzie \\(\\gamma\\) jest stałą Eulera–Mascheroniego znaną z wykładu z analizy (\\(\\gamma \\sim 0,577\\)).","code":""},{"path":"przegląd-ważniejszych-rozkładów.html","id":"przegląd-ważniejszych-rozkładów","chapter":"9 Przegląd ważniejszych rozkładów","heading":"9 Przegląd ważniejszych rozkładów","text":"Celem tego rozdziału jest systematyzacja wiedzy o najczęściej\nspotykanych wykorzystywanych rozkładach.","code":""},{"path":"przegląd-ważniejszych-rozkładów.html","id":"rozkłady-dyskretne","chapter":"9 Przegląd ważniejszych rozkładów","heading":"Rozkłady dyskretne","text":"Zaczniemy od rozkładów skupionych na przeliczalnych zbiorach.Definicja 9.1  Zmienna losowa \\(X\\) o rozkładzie \\(\\mu_X\\) ma rozkład dyskretny\njeżeli istnieje przeliczalny (skończony) zbiór \\(S\\) taki, że \\(\\mu_X(S)=1\\).Jeżeli \\(X\\) ma rozkład dyskretny \n\\[\\begin{equation*}\n    S = \\left\\{ x\\\\mathbb{R}:\\; \\mathbb{P}[X = x]>0 \\right\\}\n\\end{equation*}\\]\njest zbiorem atomów. Zbiór \\(S\\) można ustawić w ciąg \\(S = \\{s_k\\}_{k \\\\mathbb{N}}\\). Mamy wówczas\n\\[\\begin{equation*}\n    \\sum_{k=1}^\\infty \\mathbb{P}[X=s_k]=1.\n\\end{equation*}\\]Twierdzenie 8.1 dla zmiennych o rozkładzie dyskretnym zapisuje się w postaci sumy.Wniosek 9.1  Jeżeli zmienna losowa \\(X\\) jest dyskretna ze zbiorem atomów \\(S=\\{s_k\\}_{k}\\),\n\\(\\varphi \\colon \\mathbb{R} \\\\mathbb{R}\\) jest funkcją borelowską,\n\\(\\mathbb{E} \\left[\\phi(X)\\right]\\) istnieje wtedy tylko wtedy, gdy\n\\(\\sum_i |\\phi(x_i)|\\mathbb{P}[X=x_i]<\\infty\\). Wówczas\n\\[\n  \\mathbb{E} \\left[\\phi(X)\\right] = \\sum_i \\phi(s_i)\\mathbb{P}[X=s_i].\n\\]Poniżej dokonamy przeglądu najczęściej spotykanych rozkładów dyskretnych.","code":""},{"path":"przegląd-ważniejszych-rozkładów.html","id":"rozkład-skupiony-w-punkcie","chapter":"9 Przegląd ważniejszych rozkładów","heading":"Rozkład skupiony w punkcie","text":"Rozkład skupiony w punkcie \\(\\\\mathbb{R}\\) odpowiada zmiennej losowej, która z prawdopodobieństwem jeden\njest równa \\(\\). Czyli\n\\[\\begin{equation*}\n    \\mathbb{P}[X=]=1.\n\\end{equation*}\\]\nWówczas odpowiadający rozkład jest skupiony w punkcie \\(\\), czyli \\(\\mu_X=\\delta_a\\). W tym przypadku zbiór atomów jest\njednopunktowy \\(S = \\{\\}\\).","code":""},{"path":"przegląd-ważniejszych-rozkładów.html","id":"rozkład-dwumianowy-bernoulliego","chapter":"9 Przegląd ważniejszych rozkładów","heading":"Rozkład dwumianowy (Bernoulliego)","text":"","code":""},{"path":"przegląd-ważniejszych-rozkładów.html","id":"rozkład-geometryczny","chapter":"9 Przegląd ważniejszych rozkładów","heading":"Rozkład Geometryczny","text":"Niech \\(p \\(0,1)\\). Powiemy, że zmienna losowa \\(X\\) ma rozkład geometryczny\nz parametrem \\(p\\), jeżeli\n\\[\\begin{equation*}\n    \\mathbb{P}[X=j] = (1-p)^{j-1}p, \\qquad j \\geq 1.\n\\end{equation*}\\]\nPiszemy wtedy \\(X \\sim Geo(p)\\).\nWówczas \\(X\\) modeluje liczbę prób jaką należy wykonać aby uzyskać pierwszy sukces w ciągu prób,\nw którym w pojedynczej próbie sukces zachodzi z prawdopodobieństwem \\(p\\).Należy mieć na względzie, że \\(\\mathbb{P}[X=j]>0\\) dla każdego \\(j\\). Zera pojawiające się\nna powyższym histogramie wynikają wyłącznie z zaokrąglenia.","code":""},{"path":"przegląd-ważniejszych-rozkładów.html","id":"rozkład-poissona","chapter":"9 Przegląd ważniejszych rozkładów","heading":"Rozkład Poissona","text":"Pokażemy w zadaniu, że jeżeli wykonujemy \\(n\\) prób Bernoulliego z prawdopodobieństwem sukcesu\n\\(p=\\lambda/n\\) przy \\(n \\\\infty\\) liczba sukcesów będzie miała w przybliżeniu rozkład\nPoissona z parametrem \\(\\lambda\\). Oznacza , że rozkład Poissona używany jest opisu\nliczby rzadkich zdarzeń takich jak liczba wypadków samochodowych czy\nliczba chorych na rzadką chorobę.","code":""},{"path":"przegląd-ważniejszych-rozkładów.html","id":"rozkłady-absolutnie-ciągłe","chapter":"9 Przegląd ważniejszych rozkładów","heading":"Rozkłady absolutnie ciągłe","text":"Definicja 9.2  Zmienna losowa \\(X\\) o rozkładzie \\(\\mu_X\\) ma rozkład absolutnie ciągły\n(względem miary Lebesgue’)\njeżeli istnieje funkcja borelowska \\(f_X:\\mathbb{R}\\[0,\\infty)\\) taka,\nże dla dowolnego \\(B\\\\mathcal{B}(\\mathbb{R})\\)\n\\[\n\\mathbb{P}[X\\B] = \\mu_X(B) = \\int_B f_X(s)\\mathrm{d}s.\n\\]\nFunkcję \\(f_X\\) nazywamy gęstością rozkładu \\(X\\).Gęstość jest jednoznacznie wyznaczona z dokładnością zbiorów miary Lebesgue’zero.\nZauważmy, że\n\\[\\begin{equation*}\n    \\int_\\mathbb{R} f_X(s) \\mathrm{d}s=\\mu_X(\\mathbb{R}) =1.\n\\end{equation*}\\]\nKażda funkcja mierzalna nieujemna \\(f\\) taka, że \\(\\int f(x)\\mathrm{d}x = 1\\) jest gęstością\npewnej zmiennej losowej.\nJeżeli przez \\(F_X\\) oznaczymy dystrybuantę zmiennej \\(X\\), \n\\[\nF_X(t) = \\int_{(-\\infty,t]} f_X(s) \\mathrm{d}s.\n\\]\nJeżeli \\(f_X\\) jest funkcją ciągłą, powyższa całka jest całką Riemanna wówczas \\(F'_X = f_X\\).Wniosek 9.2  Jeżeli zmienna losowa \\(X\\) ma rozkład absolutnie ciągły z gęstością \\(f_X\\), \ndla każdej borelowskiej \\(\\varphi \\colon \\mathbb{R} \\\\mathbb{R}\\) mamy\n\\[\\begin{equation*}\n    \\mathbb{E}[\\varphi(X)] = \\int_\\mathbb{R} \\varphi(s)f_X(s) \\mathrm{d}s.\n\\end{equation*}\\]","code":""},{"path":"przegląd-ważniejszych-rozkładów.html","id":"rozkład-jednostajny","chapter":"9 Przegląd ważniejszych rozkładów","heading":"Rozkład jednostajny","text":"Niech \\(D\\\\mathcal{B}(\\mathbb{R})\\) będzie dowolnym zbiorem o niezerowej mierze Lebesgue’.\nPowiemy, że zmienna losowa \\(X\\) ma rozkład jednostajny na \\(D\\) jeżeli\n\\[\n  f_X(s) = \\frac{{\\bf 1}_D(s)}{\\lambda_1(D)}.\n\\]\nWtedy dla każdego \\(B\\\\mathcal{B}(\\mathbb{R})\\)\n\\[\n  \\mathbb{P}[X\\B] = \\int_B f_X(s)\\mathrm{d}s = \\frac{\\lambda_1(B\\cap D)}{\\lambda_1(D)}.\n\\]\nDla przykładu jeżeli \\(D=[,b]\\), \nodpowiadająca gęstość zadaje się wzorem\n\\[\\begin{equation*}\n    f_X(s) = \\frac{1}{b-} \\mathbf{1}_{[,b]}(s).\n\\end{equation*}\\]","code":""},{"path":"przegląd-ważniejszych-rozkładów.html","id":"rozkład-wykładniczy","chapter":"9 Przegląd ważniejszych rozkładów","heading":"Rozkład wykładniczy","text":"Niech \\(\\lambda>0\\). Powiemy, że zmienna losowa \\(X\\) ma\nrozkład wykładniczy z parametrem \\(\\lambda\\) jeżeli\njej rozkład jest absolutnie ciągły z gęstością\n\\[\n  f_X(x) = \\lambda e^{-\\lambda x} {\\bf 1}_{[0,\\infty)}(x).\n\\]\nPiszemy wtedy \\(X \\sim \\mathrm{Exp}(\\lambda)\\).\nWówczas dystrybuanta zadaje się wzorem\n\\[\\begin{equation*}\n\\quad F(x) = \\left\\{\n  \\begin{array}{cc}\n    0, & x<0  \\\\\n    1-e^{-\\lambda x}, & x\\ge 0.\n  \\end{array}\\right.\n\\end{equation*}\\]Rozkład wykładniczy określa moment zajścia zdarzenia losowego","code":""},{"path":"przegląd-ważniejszych-rozkładów.html","id":"rozkład-normalny","chapter":"9 Przegląd ważniejszych rozkładów","heading":"Rozkład normalny","text":"Powiemy, że zmienna losowa \\(X\\) ma standardowy rozkład normalny, jeżeli\njest rozkład jest absolutnie ciągły z gęstością\n\\[\\begin{equation*}\n  f(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2}.\n\\end{equation*}\\]\nPiszemy wtedy \\(X \\sim \\mathcal{N}(0,1)\\). Funkcja gęstości nie jest elementarna,\nwięc dystrybuanta \\(F_X\\) nie ma zwartej postaci.\n\\[\\begin{equation*}\n    F(x) = \\frac 1{\\sqrt {2\\pi}} \\int_{-\\infty}^x e^{-t^2/2}\\mathrm{d}t.\n\\end{equation*}\\]\nNiech \\(\\mu \\\\mathbb{R}\\) \\(\\sigma^2>0\\). Powiemy, że zmienna losowa \\(X\\) ma rozkład normalny\nz parametrami \\(\\mu\\) oraz \\(\\sigma^2\\), jeżeli ma gęstość\n\\[\\begin{equation*}\n  f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-(x-\\mu)^2/(2\\sigma^2)}.\n\\end{equation*}\\]\nPiszemy wtedy \\(\\mathcal{N}(\\mu, \\sigma^2)\\).","code":""},{"path":"przegląd-ważniejszych-rozkładów.html","id":"pozostałe-rozkłady","chapter":"9 Przegląd ważniejszych rozkładów","heading":"Pozostałe rozkłady","text":"Rozkłady dyskretne ciągłe nie wyczerpują wszystkich możliwości. Wiemy, że każdą miarę \\(\\mu\\) można jednoznacznie zapisać w postaci\n\\[\n\\mu =  \\mu_{{\\rm abs}} + \\mu_{{\\rm sing}},\n\\] gdzie \\(\\mu_{{\\rm abs}} \\ll{\\rm Leb}\\) (tzn. \\(\\mu_{{\\rm abs}}\\) jest absolutnie ciągła względem \\({\\rm Leb}\\),\n\\(\\mu_{{\\rm sing}} \\perp {\\rm Leb}\\) są wzajemnie singularne (rozkład Lebesgue’).","code":""},{"path":"wektory-losowe.html","id":"wektory-losowe","chapter":"10 Wektory losowe","heading":"10 Wektory losowe","text":"Definicja 10.1  Niech \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) będzie przestrzenią probabilistyczną.\nNiech \\(d \\\\mathbb{N}\\). \\(d\\)-wymiarowym\nWektorem losowym\nnazywamy dowolną funkcję mierzalną \\(\\vec{X}: (\\Omega, \\mathcal{F}) \\(\\mathbb{R}^d, \\mathcal{B}(\\mathbb{R}^d)\\).Jeżeli \\(d=1\\), powyższa definicja opisuje zmienną losową.\nJeżeli \\(\\vec{X}\\colon \\Omega \\\\mathbb{R}^d\\), \ndla \\(\\omega \\\\Omega\\),\n\\[\\begin{equation*}\n    \\vec{X}(\\omega) = (X_1(\\omega), X_2(\\omega), \\ldots , X_d(\\omega)).\n\\end{equation*}\\]\nWówczas \\(\\vec{X}\\) jest wektorem losowym wtedy tylko wtedy, gdy \\(X_k\\) jest zmienną losową dla\nkażdego \\(k\\leq d\\). Istotnie, wystarczy zauważyć, że dla dowolnych\n\\(A_1,A_2, \\ldots ,A_d \\\\mathcal{B}(\\mathbb{R})\\),\n\\[\\begin{equation*}\n    \\vec{X}^{-1}[A_1\\times A_2\\times \\cdots \\times A_d] =\n    X_1^{-1}(A_1)\\cap X_2^{-1}(A_2)\\cap \\ldots \\cap X_d^{-1}[A_d] \\\\mathcal{F}\n\\end{equation*}\\]\njako przekrój skończonej liczby zdarzeń.Przykład 10.1  Wylosowano 13 kart z 52.\nNiech \\(X_1\\) oznacza liczbę pików, \\(X_2\\) liczbę kierów.\nWówczas \\(\\vec{X}=(X_1,X_2)\\) jest 2-wymiarowym wektorem losowym.Przykład 10.2  Losujemy punkt z kwadratu \\([0,1]^2\\). Dla \\(\\omega \\[0,1]^2\\) niech \\(X_1\\) będzie odległością\nwylosowanego punktu od lewej krawędzi kwadratu. Wtedy \\(X_1(\\omega) = \\omega_1\\) dla \\(\\omega=(\\omega_1, \\omega_2)\\).\nNiech \\(X_2\\) będzie odległością od punktu \\((0,0)\\). Wówczas \\(X_2(\\omega) = \\sqrt{\\omega_1^2+\\omega_2^2}\\).\nWówczas \\(\\vec{X}=(X_1, X_2)\\) jest wektorem losowym.Podstawowe własności wektorów losowych są analogiczne zmiennych losowych.jeżeli \\(\\vec{X}, \\vec{Y}\\) są \\(d\\)-wymiarowymi wektorami losowymi, \\(\\vec{X}+\\vec{Y}\\),\n\\(\\vec{X}-\\vec{Y}\\) również;jeżeli \\(\\phi:\\mathbb{R}^d\\\\mathbb{R}^k\\) jest mierzalne, \\(\\phi(\\vec{X})\\) jest \\(k\\)-wymiarowym wektorem losowym.Definicja 10.2  Rozkładem \\(d\\)-wymiarowego wektora losowego \\(\\vec{X}=(X_1, \\ldots , X_d)\\)\nnazywamy miarę probabilistyczną\n\\(\\mu_{\\vec{X}}\\) na \\(\\mathbb{R}^d\\)\nzadaną wzorem\n\\[\n    \\mu_{\\vec{X}}(B) =\n    \\mathbb{P}\\left[\\vec{X}\\B\\right] =\n    \\mathbb{P}[(X_1, \\ldots, X_d) \\B] = \\mathbb{P}\\left[\\vec{X}^{-1}(B)\\right].\n  \\]\nRozkład \\(\\mu_X\\) nazywamy rozkładem łącznym zmiennych losowych \\(X_1\\), \\(X_2, \\ldots , X_d\\).Niech \\(\\mu\\) będzie dowolną miarą probabilistyczną na \\(\\mathbb{R}^d\\).\nWówczas \\((\\Omega, \\mathcal{F}, \\mathbb{P})\n  = (\\mathbb{R}^d, \\mathcal{B}(\\mathbb{R}^d),\\mu_X)\\) jest przestrzenią probabilistyczną.\nJeżeli zdefiniujemy \\(\\vec{X} \\colon \\Omega \\\\mathbb{R}^d\\) poprzez \\(\\vec{X}(\\omega) = \\omega\\),\nrozkład wektora \\(\\vec{X}\\) \n\\[\\begin{equation*}\n    \\mu_{\\vec{X}}() = \\mathbb{P}\\left[ \\vec{X}^{-1}() \\right] = \\mathbb{P}[] = \\mu().\n  \\end{equation*}\\]\nOznacza , że dowolna miara probabilistyczna \\(\\mu\\)\nna \\(\\mathbb{R}^d\\) jest rozkładem pewnego wektora losowego \\(\\vec{X}\\).Definicja 10.3  \\(d\\)-wymiarowy wektor losowy \\(X\\) ma rozkład dyskretny, jeżeli istnieje przeliczalny zbiór \\(S\\subseteq \\mathbb{R}^d\\)\ntaki, że \\(\\mu_X(S)=1\\).W takim przypadku zbiór atomów\n\\[\n    S = \\left\\{s \\\\mathbb{R}^d \\: : \\: \\mathbb{P}[X=s] >0 \\right\\}\n\\]\nmożna ustawić w ciąg \\(S = \\{s_j\\}_{j \\\\mathbb{N}}\\). Wówczas\n\\[\\begin{equation*}\n    \\sum_{j=1}^\\infty\\mathbb{P}[S=s_j]=1.\n\\end{equation*}\\]\nWektor losowy \\(X=(X_1, \\ldots, X_d)\\) ma rozkład dyskretny wtedy tylko wtedy, dla każdego \\(j\\leq d\\)\nzmienna losowa \\(X_j\\) ma rozkład dyskretny.Przykład 10.3  W urnie są 2 kule czerwone, 5 białych 3 zielone. Wybieramy losowo 3 kule (jednocześnie). Niech \\(X_1\\) oznacza liczbę kul białych, \\(X_2\\) liczbę kul czerwonych. Wówczas \\((X_1,X_2)\\) jest 2-wymiarową zmienną losową ma rozkład dyskretny:\n\\[\n  \\begin{array}{c|c|c|c|c}\n    X_1\\setminus X_2 & 0 & 1 & 2 & \\mathbb{P}[X_1 = x] \\\\[2mm] \\hline\n    0 & \\frac{1}{120} & \\frac{6}{120} & \\frac{3}{120} & \\frac{10}{120} \\\\[2mm] \\hline\n    1 & \\frac{15}{120} & \\frac{30}{120} & \\frac{5}{120} & \\frac{50}{120} \\\\[2mm] \\hline\n    2 & \\frac{30}{120} & \\frac{20}{120} & 0 & \\frac{50}{120} \\\\[2mm] \\hline\n    3 & \\frac{10}{120} & 0 & 0 & \\frac{10}{120} \\\\[2mm] \\hline\n    \\mathbb{P}[X_2 = y] & \\frac{56}{120} & \\frac{56}{120} & \\frac{8}{120} & 1\n  \\end{array}\n  \\]\nMając dany powyższy rozkład możemy obliczyć:\n\\[\\begin{align*}\n    \\mathbb{P}[X_1\\le X_2] & =\\frac{45}{120} = \\frac 38 \\\\\n    \\mathbb{P}[X_1=1|X_1 \\le X_2] & =\n    \\frac{\\mathbb{P}[X_1=1 \\mbox{ } X_1\\le X_2]}{\\mathbb{P}[X_1 \\le X_2]} = \\frac{35/120}{45/120} = \\frac{7}{9}.\n  \\end{align*}\\]Aby badać rozkłady wektorów losowych, które nie są dyskretne musimy wprowadzić dodatkowe pojęcie.Definicja 10.4  Dystrybuanta \\(d\\)-wymiarowego wektora losowego \\(X=(X_1,\\ldots,X_d)\\) jest funkcja\n\\(F:\\mathbb{R}^d\\mapsto [0,1]\\) zadana wzorem\n\\[\\begin{multline*}\n  F(t_1,t_2,\\ldots, t_d) = \\mu\\big(\n  (-\\infty,t_1]\\times (-\\infty,t_2]\\times \\ldots \\times (-\\infty,t_d]\n  \\big)\\\\\n  = \\mathbb{P}\\big[ X_1\\le t_1, X_2\\le t_2,\\ldots, X_d\\le t_d \\big].\n  \\end{multline*}\\]Twierdzenie 10.1  Niech \\(F\\) będzie dystrybuantą \\(d\\)-wymiarowej zmiennej losowej \\(\\vec{X}\\). Wówczasjeżeli \\(x_i\\-\\infty\\) dla pewnego \\(\\), \\(F(x_1,\\ldots, x_d)\\0\\);jeżeli \\(x_i\\+\\infty\\) dla każdego \\(\\), \\(F(x_1,\\ldots, x_d)\\1\\);dystrybuanta zmiennej losowej \\(X\\) jednoznacznie wyznacza jej rozkład.Proof. Pierwsze dwa punkty wynikają z ciągłości miary. Trzeci jest zastosowaniem Lematu o \\(\\pi\\) \\(\\lambda\\) układach.\nUstalmy \\(\\leq d\\). Zauważmy, że\n\\[\\begin{equation*}\n    \\bigcap_{n\\\\mathbb{N}}\n    \\mathbb{R}^{-1} \\times(-\\infty, -n] \\times \\mathbb{R}^{d-} =\\emptyset.\n\\end{equation*}\\]\nZ ciągłości miary\n\\[\\begin{equation*}\n    \\lim_{n \\\\infty} \\mu_{\\vec{X}} \\left[\\mathbb{R}^{-1}\\times (-\\infty, -n] \\mathbb{R}^{d-} \\right] =0.\n\\end{equation*}\\]\nNiech teraz \\(\\{x_i(m)\\}_{m\\\\mathbb{N}}\\) dla \\(\\leq d\\) będzie dowolną kolekcją ciągów.\nJeżeli \\(x_i(m)\\-\\infty\\),\ndla dostatecznie dużych \\(m\\), \\(x_i(m)\\leq -n\\). Wówczas\n\\[\\begin{equation*}\n    F_{\\vec{X}}(x_i, \\ldots, x_d) \\leq\n    \\mu_{\\vec{X}} \\left[\\mathbb{R}^{-1}\\times (-\\infty, -n] \\times\\mathbb{R}^{d-} \\right]\n\\end{equation*}\\]\ngdzie prawa strona, poprzez dobór \\(n\\) może być uczyniona dowolnie małą.\nAby uzasadnić punkt drugi stosujemy podobny argument w oparciu o\n\\[\\begin{equation*}\n    \\bigcap_{n \\\\mathbb{N}} (-\\infty,n]^d =\\mathbb{R}^d.\n\\end{equation*}\\]\nAby uzasadnić ostatni punkt zauważmy, że\n\\[\\begin{equation*}\n\\mathcal{K} = \\{ (-\\infty, t_1] \\times \\cdots (-\\infty, t_d] \\: : \\: t_1, \\ldots t_d \\\\mathbb{R} \\}\n\\end{equation*}\\]\njest \\(\\pi\\)-układem. Jeżeli dwa wektory losowe \\(\\vec{X}\\) oraz \\(\\vec{Y}\\) mają równe dystrybuanty,\n\n\\[\\begin{equation*}\n    \\mathcal{K} \\subseteq \\mathcal{L} = \\{B \\\\mathcal{B}(\\mathbb{R}^d)\n    \\: \\: \\mu_{\\vec{X}}() = \\mu_{\\vec{Y}}() \\}.\n\\end{equation*}\\]\nSprawdzaliśmy już, że zbiory \\(\\mathcal{L}\\) postaci jak wyżej są \\(\\lambda\\)-układami.\nZ lematu o \\(\\pi\\)-\\(\\lambda\\) układach\n\\(\\mu_{\\vec{X}}() = \\mu_{\\vec{Y}}()\\) dla każdego borelowskiego \\(\\).Okazuje się, że podobnie jak w przypadku jednowymiarowym rozkład wektora losowego\nmożna wyrazić w terminach jego dystrybuanty.\nPrzykładowo, dla \\(d=2\\) mamy\n\\[\\begin{equation*}\n\\mu_X((,b]\\times(c,d]) = F_X(b,d) -F(b,c)-F(,d)+F(,c).\n\\end{equation*}\\]\nW szczególności oznacza , że dla dwuwymiarowej dystrybuanty prawa strona jest nieujemna\ndla każdych\n\\(< b\\) oraz \\(c < d\\).Przykład 10.4  Z powyższego łatwo liczymy, że\n\\[\\begin{equation*}\n    F_{\\vec{X}}(t_1, t_2) = \\left\\{\\begin{array}{cc}\n    4t_1t_2 & \\sqrt{2}(t_1+t_2)\\leq 1 \\\\\n    1-(1-\\sqrt{2}t_1)_+^2 -(1-\\sqrt{2}t_2)_+^2 & \\sqrt{2}(t_1+t_2)>1\n\\end{array} \\right.\n\\end{equation*}\\]\nWykres naszej dwuwymiarowej dystrybuanty przedstawia się następującoDefinicja 10.5  \\(d\\)-wymiarowy wektor \\(\\vec{X}\\) ma rozkład absolutnie ciągły, jeżeli istnieje funkcja borelowska\n\\(f_\\vec{X}:\\mathbb{R}^d \\mapsto [0,\\infty)\\)\ntaka, że\n\\[\n    \\mathbb{P}\\left[\\vec{X}\\B\\right] = \\mu(B) = \\int_B f_\\vec{X}(s)\\mathrm{d}s\n    \\qquad B\\\\mathcal{B}(\\mathbb{R}^d).\n  \\]Wówczas\n\\[\n  F_\\vec{X}(t_1,\\ldots,t_d) = \\int_{-\\infty}^{t_1}\\ldots \\int_{-\\infty}^{t_d} f_\\vec{X}(x_1,\\ldots, x_d)dx_1\\ldots dx_d.\n  \\]\nPonadto, jeżeli \\(F\\C^d(\\mathbb{R}^d)\\), \n\\[\n  f_{\\vec{X}}(x_1,\\ldots,x_d) = \\frac{\\partial^d F_{\\vec{X}}}{\\partial x_1\\ldots \\partial x_d}(x_1,\\ldots,x_d).\n  \\]Zauważmy, że jeżeli \\(\\vec{X}\\) ma rozkład absolutnie ciągły, \nkażdy \\(X_k\\) dla \\(k \\leq d\\) ma rozkład absolutnie ciągły. Implikacja przeciwna nie jest\njednak prawdziwa.Przykład 10.5  Niech \\(\\Omega = [0,1]\\) z \\(\\sigma\\)-ciałem zbiorów borelowskich jednowymiarową miara Lebesgue’.\nRozważmy \\(X_1(\\omega)=\\omega\\) oraz \\(X_2(\\omega) = 1-\\omega\\).\nWówczas \\(\\vec{X} = (X_1, X_2)\\) ma rozkład skupiony na nieprzeliczalnym zbiorze\no płaskiej mierze Lebesgue’zero\n\\[\\begin{equation*}\n\\{(x,1-x) \\: : \\: x \\[0,1]\\} \\subseteq \\mathbb{R}^2.\n\\end{equation*}\\]Przykład 10.6  Niech \\(S\\) będzie dowolnym mierzalnym ograniczonym podzbiorem \\(\\mathbb{R}^2\\),\nwtedy jego losowy punkt \\(\\vec{X}=(X_1,X_2)\\) (wybrany jednostajnie\nwzględem miary Lebesgue’) jest 2-wymiarową zmienną losową. Mamy wówczas\n\\[\n  f_{\\vec{X}}(x_1,x_2) = \\frac{1}{|S|}\\; {\\bf 1}_S\n  \\]","code":""},{"path":"wektory-losowe.html","id":"wielowymiarowy-rozkład-normalny","chapter":"10 Wektory losowe","heading":"Wielowymiarowy rozkład normalny","text":"Przykład 10.7  Powiemy, że \\(d\\)-wymiarowy wektor \\(\\vec{X}\\) ma \\(d\\)-wymiarowy standardowy rozkład normalny, jeżeli\njego rozkład jest absolutnie ciągły z gęstością\n\\[\\begin{equation*}\n    f_{\\vec{X}}(s_1, \\ldots, s_d) =\n(2\\pi)^{-d/2} e^{-\\|s\\|^2/2}.   \n(2\\pi)^{-d/2} e^{-(s_1^2+s_2^2+\\ldots +s_d^2)/2},\n\\end{equation*}\\]\ngdzie \\(s = (s_1, s_2, \\ldots, s_d)\\).Sprawdźmy, że podana wyżej funkcja rzeczywiście jest gęstością na \\(\\mathbb{R}^d\\).\nBędziemy stosować oznaczenie \\(s = (s_1, \\ldots, s_d)\\). Mamy\n\\[\\begin{multline*}\n    \\int_{\\mathbb{R}^d} f_\\vec{X}(s) \\mathrm{d}s =\n    \\int_\\mathbb{R} \\ldots \\int_\\mathbb{R} (2\\pi)^{-d/2} e^{-s_1^2/2} \\cdots\n    e^{-s_d/2} \\mathrm{d}s_1 \\ldots \\mathrm{d}s_d \\\\\n    = \\left( \\sqrt{2\\pi} \\int_\\mathbb{R} e^{-x^2/2} \\mathrm{d}x \\right)^d=1.\n\\end{multline*}\\]\nJeżeli wygenerujemy \\(100\\) punktów ma płaszczyźnie zgodnie ze standardowym rozkładem normalnym,\nbędą się one układały symetrycznie wokół zera.Niech \\(\\vec{m}\\) będzie wektorem \\(d\\)-wymiarowym \\(\\) odwracalną macierzą wymiaru \\(d\\times d\\).\nRozważmy zmienną losową\n\\[\\begin{equation*}\n    \\vec{Y} = \\vec{X} +m.\n\\end{equation*}\\]\nJaki rozkład ma wektor \\(\\vec{Y}\\)?\nZauważmy, że dla \\(B \\\\mathcal{B}(\\mathbb{R}^d)\\),\n\\[\\begin{equation*}\n    \\mu_{\\vec{Y}} (B)=\\mathbb{P}\\left[\\vec{Y}\\B\\right] =\n    \\mathbb{P}\\left[ \\vec{X} \\^{-1}B - ^{-1}\\vec{m} \\right],\n\\end{equation*}\\]\ngdzie\n\\[\\begin{equation*}\n    ^{-1}B-^{-1}\\vec{m} = \\left\\{ ^{-1}\\vec{b} -^{-1}\\vec{m} \\: : \\: \\vec{b} \\B \\right\\}.    \n\\end{equation*}\\]\nCałkując przez podstawienie \\(s = ^{-1}y -^{-1}\\vec{m}\\),\n\\[\\begin{equation*}\n    \\mu_{\\vec{Y}}(B) = \\int_{^{-1}B-^{-1}\\vec{m}} f_\\vec{X}(s) \\mathrm{d}s =\n    \\int_B |\\mathrm{det}(^{-1})| f_\\vec{X}\\left(^{-1}(y-\\vec{m})\\right) \\mathrm{d}y.\n\\end{equation*}\\]\nSkoro gęstość jest wyznaczona jednoznacznie, wektor losowy \\(\\vec{Y}\\) ma rozkład o gęstości\n\\[\\begin{multline*}\nf_\\vec{Y}(y) =\n|\\mathrm{det}(^{-1})| f_\\vec{X}\\left(^{-1}(y-\\vec{m})\\right)=\\\\\n\\frac{|\\mathrm{det}(^{-1})|}{(2\\pi)^{d/2}}\\exp \\left\\{ -\\left\\| ^{-1}(y-\\vec{m}) \\right\\|^2/2  \n\\right\\}\n\\end{multline*}\\]\nZauważmy, że\n\\[\\begin{multline*}\n    \\left\\| ^{-1}(y-\\vec{m}) \\right\\|^2=\n    \\langle ^{-1}(y-\\vec{m}), ^{-1}(y-\\vec{m})  \\rangle=\\\\\n    \\langle (^{-1})^TA^{-1}(y-\\vec{m}), y-\\vec{m}  \\rangle.\n\\end{multline*}\\]\nOznaczmy \\(\\Sigma = AA^T\\). Wówczas macierz \\(\\Sigma\\) jest symetryczna nieujemnie określona.\nGęstość wektora losowego \\(\\vec{Y}\\) zapisuje się jako\n\\[\\begin{equation*}\n    f_\\vec{Y}(y) =\n    \\mathrm{det}(\\Sigma)^{-1/2}(2\\pi)^{-d/2}\n    \\exp \\left\\{ -\\langle \\Sigma^{-1}(y-\\vec{m}), y-\\vec{m} \\rangle /2  \\right\\}.\n\\end{equation*}\\]\nJeżeli \\(\\Sigma\\) jest symetryczną macierzą dodatnio określoną, \\(\\vec{m} \\\\mathbb{R}^d\\) wektor\nlosowy \\(\\vec{Y}\\) ma rozkład o gęstości jak wyżej, będziemy mówić, że\n\\(\\vec{Y}\\) ma wielowymiarowy rozkład normalny o parametrach \\(\\vec{m}\\) \\(\\Sigma\\). Piszemy wówczas\n\\(\\vec{Y} \\sim \\mathcal{N}(\\vec{m}, \\Sigma)\\). Niebawem dowiemy się, jak dobór macierzy \\(\\Sigma\\) wpływa\nna kształt gęstości \\(f_{\\vec{Y}}\\) oraz na własności probabilistyczne wektora \\(\\vec{Y}\\).\nZauważmy jedynie teraz, że \\(\\vec{Y}\\) powstał z \\(\\vec{X}\\) poprzez przekształcenie liniowe\nprzesunięcie. samo będzie tyczyło się związku między \\(f_\\vec{X}\\) oraz \\(f_\\vec{Y}\\).Przykład 10.8  Rozważmy poprzedni przykład dla macierzy \\(\\) zadanej przez\n\\[\\begin{equation*}\n    = \\left( \\begin{array}{cc} 1 & 1 \\\\ 0 & 1 \\end{array}\\right).\n\\end{equation*}\\]\nWówczas\n\\[\\begin{equation*}\n    \\Sigma = \\left( \\begin{array}{cc} 2 & 1 \\\\ 1 & 1 \\end{array}\\right), \\qquad\n    \\Sigma^{-1} = \\left( \\begin{array}{cc} 1 & -1 \\\\ -1 & 2 \\end{array}\\right).\n\\end{equation*}\\]\nGęstość wektora \\(\\vec{Y}\\) o dwuwymiarowym rozkładzie normalnym\nz parametrami \\(\\vec{m} =(0,0)\\) \\(\\Sigma\\) wynosi\n\\[\\begin{equation*}\n    f_{\\vec{Y}}(x,y) = \\frac{1}{2 \\pi} \\exp \\left\\{ -(x^2-2xy+2y^2)/2 \\right\\}.\n\\end{equation*}\\]","code":""},{"path":"wektory-losowe.html","id":"rozkłady-brzegowe","chapter":"10 Wektory losowe","heading":"Rozkłady brzegowe","text":"Definicja 10.6  Niech \\(X=(X_1,\\ldots, X_d)\\) będzie \\(d\\)-wymiarową zmienną losową.\nWówczas dla każdego \\(k\\le d\\), rozkład \\(\\mu_{X_k}\\) nazywamy rozkładem brzegowym \\(\\mu_X\\).Rozkłady brzegowe nie wyznaczają jednoznacznie rozkładu łącznego (poza pewnymi szczególnymi sytuacjami). znaczy, może się\nzdarzyć, że jeżeli dane są cztery zmienne losowe \\(X_1,X_2,Y_1, Y_2\\), takie, że \\(X_1\\) \\(Y_1\\) mają takie rozkłady oraz \\(X_2\\) \\(Y_2\\) mają takie rozkłady, ale \\((X_1,X_2)\\) \\((Y_1,Y_2)\\) mają różne rozkłady.Przykład 10.9  Losujemy punkt z odcinka \\([0,1]\\). Dla wylosowanej liczby \\(\\omega \\\\Omega = [0,1]\\) niech\n\\(X_1(\\omega) = \\omega\\) \\(X_2(\\omega)=1-\\omega\\). Wówczas wektor losowy \\(\\vec{X} = (X_1, X_2)\\)\nprzyjmuje wartości w zbiorze\n\\[\\begin{equation*}\n    L = \\{(x,1-x) \\: : \\: x\\[0,1]\\}.\n\\end{equation*}\\]\nZauważmy, że \\(\\lambda_2(L)=0\\). Rozkład wektora \\(\\vec{X}\\) nie jest zatem ani dyskretny ani ciągły.\nJakie są rozkłady brzegowe? Dla \\(X_1\\) mamy\n\\[\\begin{equation*}\n    \\mu_{X_1}(B)=\\mathbb{P}[X_1\\B] = \\lambda_1(B),\n\\end{equation*}\\]\ndla \\(B \\\\mathcal{B}([0,1])\\).\nJest zatem rozkład jednostajny na \\([0,1]\\). Dla \\(X_2\\) mamy\n\\[\\begin{equation*}\n\\mu_{X_2}(B)=\\mathbb{P}[X_1\\B] = \\lambda_1( \\{ \\omega \\[0,1] \\: : \\: 1-\\omega \\B \\} ).\n\\end{equation*}\\]\nSkoro symetria względem \\(x=1/2\\) jest izometrią\n\\[\\begin{equation*}\n\\lambda_1( \\{ \\omega \\[0,1] \\: : \\: 1-\\omega \\B \\} ) = \\lambda_1(B).\n\\end{equation*}\\]\nJest również rozkład jednostajny na \\([0,1]\\).Przykład 10.10  Losujemy punkt z kwadratu jednostkowego \\([0,1]^2\\). Dla \\(\\omega = (\\omega_1, \\omega_2) \\\\Omega = [0,1]^2\\)\nniech \\(X_1(\\omega) = \\omega_1\\), \\(X_2(\\omega) = \\omega_2\\). Wówczas rozkład wektora losowego\n\\(\\vec{X} = (X_1, X_2)\\) płaska miara Lebesgue’\\(\\lambda_2\\) ograniczona \\([0,1]^2\\).\nZauważmy, że z wagi na symetrię rozkłady brzegowe są takie .\nDla \\(B \\\\mathcal{B}([0,1])\\) mamy\n\\[\\begin{equation*}\n    \\mu_{X_2}(B) = \\mu_{X_1}(B) = \\mathbb{P}[X_1\\B] = \\lambda_2(B \\times [0,1])\n    = \\lambda_1(B).\n\\end{equation*}\\]\nJest kolejny raz rozkład jednostajny na \\([0,1]\\).Zauważmy, że w dwóch ostatnich przykładach różne rozkłady łączne miały takie rozkłady brzegowe.\nWynika z tego, że rozkłady brzegowe nie determinują rozkładu łącznego.","code":""},{"path":"rozkłady-warunkowe.html","id":"rozkłady-warunkowe","chapter":"11 Rozkłady warunkowe","heading":"11 Rozkłady warunkowe","text":"Zobaczymy teraz jak koncepcja prawdopodobieństwa warunkowego współgra z koncepcjami\nwartości oczekiwanej rozkładu zmiennej losowej.\nNiech \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) będzie przestrzenią probabilistyczną.\nRozważać będziemy zdarzenie \\(\\) o dodatnim prawdopodobieństwie.\nPrzypomnijmy, że definiujemy prawdopodobieństwo warunkowe pod warunkiem zajścia zdarzenia \\(\\) poprzez\n\\[\\begin{equation*}\n    \\mathbb{P}[B | ] = \\frac{\\mathbb{P}[\\cap B]}{\\mathbb{P}[]}, \\qquad B \\\\mathcal{F}.    \n\\end{equation*}\\]Definicja 11.1  Niech \\(\\vec{X}\\) będzie \\(d\\)-wymiarowym wektorem losowym.\nRozkładem warunkowym \\(\\vec{X}\\) pod warunkiem zajścia zdarzenia \\(\\)\nnazywamy miarę probabilistyczną \\(\\mu_{\\vec{X}|}\\) na \\(\\mathbb{R}^d\\)\nzadaną przez\n\\[\\begin{equation*}\n    \\mu_{\\vec{X}|}(B) = \\mathbb{P} \\left[\\left. \\vec{X} \\B \\right|  \\right],\n    \\qquad B \\\\mathcal{B}\\left(\\mathbb{R}^d\\right).\n\\end{equation*}\\]Zauważmy, że powyższa konstrukcja nie różni się znacząco od rozkładu \\(\\mu_{\\vec{X}}\\)\nwektora losowego \\(\\vec{X}\\). Można wręcz powiedzieć, że jest taka sama. Istotnie,\nzauważmy, że funkcja zbioru \\(\\mathbb{P}[\\cdot |]\\) jest miarą probabilistyczną na\n\\((\\Omega, \\mathcal{F})\\). Mamy w takim przypadku dwie miary probabilistyczne na \\(\\Omega\\).\nSą nimi wyjściowe prawdopodobieństwo \\(\\mathbb{P}[\\cdot]\\) oraz prawdopodobieństwo warunkowe\n\\(\\mathbb{P}[\\cdot|]\\). Wektor losowy \\(\\vec{X} \\colon \\Omega \\\\mathbb{R}^d\\) ma więc dwa\nopisy probabilistyczne (rozkłady) w zależności od tego która z przestrzeni probabilistycznych\n\\[\\begin{equation*}\n    (\\Omega, \\mathcal{F}, \\mathbb{P}), \\qquad (\\Omega, \\mathcal{F}, \\mathbb{P}[\\cdot | ])\n\\end{equation*}\\]\njest aktualnie rozważana. Oznacza , że wszystkie związki między\n\\(\\mathbb{P}\\) \\(\\mu_{\\vec{X}}\\) zachodzą też między \\(\\mathbb{P}[\\cdot|]\\) \\(\\mu_{\\vec{X}|}\\).\nJednym z najczęściej spotykanych przykładów jest związek między wartością oczekiwaną\ncałką względem rozkładu.Definicja 11.2  Niech \\(X\\) będzie zmienną losową posiadającą wartość oczekiwaną.\nWarunkową wartością oczekiwaną pod warunkiem zajścia zdarzenia \\(\\)\nnazywamy\n\\[\\begin{equation*}\n    \\mathbb{E}[X | ] = \\int_\\Omega X(\\omega) \\mathbb{P}[\\mathrm{d}\\omega |].\n\\end{equation*}\\]Według powyższej definicji \\(\\mathbb{E}[X|]\\) jest po prostu całką z funkcji \\(X\\) względem\nmiary \\(\\mathbb{P}[\\cdot |]\\). Oznacza , że dla dowolnej\nfunkcji borelowskiej \\(\\varphi \\colon \\mathbb{R} \\\\mathbb{R}\\) zachodzi\n\\[\\begin{equation*}\n    \\mathbb{E} [\\varphi(X) | ] = \\int_\\mathbb{R} \\varphi(s) \\mu_{X|}(\\mathrm{d}s).\n\\end{equation*}\\]\nPowyższa dyskusja pokazuje, że pod kątem teoretycznym rozkłady warunkowe mają\ntaką samą naturę jak zwykłe rozkłady poznane wcześniej.\nW praktyce jednak operacja warunkowania istotnie zmienia własności probabilistyczne zmiennych losowych.Przykład 11.1  Rzucamy nieskończenie wiele razy symetryczną monetą.\nNiech \\(X\\) będzie liczbą rzutów potrzebnych otrzymania pierwszego orła.\nInteresuje nas rozkład \\(X\\) pod warunkiem zdarzenia \\(\\), że w pierwszych \\(n\\) rzutach pojawił\nsię (co najmniej jeden) orzeł.\nZauważmy najpierw, że \\(= \\{ X \\leq n\\}\\). Chcąc wyznaczyć rozkład \\(X\\) wystarczy\nwyznaczyć prawdopodobieństwa zdarzeń \\(\\{X=k\\}\\) względem prawdopodobieństwa warunkowego.\nZauważmy, że\n\\[\\begin{equation*}\n    \\mathbb{P}[X \\leq n] = 1-\\mathbb{P}[X > n] = 1-2^{-n}.\n\\end{equation*}\\]\nDla \\(k \\leq n\\) mamy\n\\[\\begin{equation*}\n    \\mathbb{P}[X =k | ] = \\frac{2^{-k}}{1-2^{-n}}\n\\end{equation*}\\]\noraz \\(\\mathbb{P}[X =k | ] = 0\\) dla \\(k>n\\).\nDla porównania \\(\\mathbb{P}[X=k] = 2^{-k}\\). Warunkowanie zmienia tylko\nprawdopodobieństwo o stałą na pierwszych \\(n\\) liczbach naturalnych.Przykład 11.2  Ponownie rzucamy symetryczną monetą. Niech \\(X\\) będzie liczbą rzutów potrzebną otrzymania\npierwszego orła. Tym razem niech \\(\\) będzie zdarzeniem, że w pierwszych \\(n\\) rzutach pojawił się\ndokładnie jeden orzeł.\nWówczas\n\\[\\begin{equation*}\n    \\mathbb{P}[] = n 2^{-n}.\n\\end{equation*}\\]\nDla \\(k\\leq n\\),\n\\[\\begin{equation*}\n    \\mathbb{P}[X = k|] = \\frac{2^{-n}}{n2^{-n}} = \\frac 1n.\n\\end{equation*}\\]\nTym razem warunkowanie całkowicie zmieniło charakter rozkładu. Rzeczywiście, pod warunkiem \\(\\)\nzmienna \\(X\\) ma rozkład jednostajny na \\([n] = \\{1, 2, \\ldots , n\\}\\).","code":""},{"path":"niezależne-zmienne-losowe.html","id":"niezależne-zmienne-losowe","chapter":"12 Niezależne zmienne losowe","heading":"12 Niezależne zmienne losowe","text":"Niech \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) będzie przestrzenią probabilistyczną \nniech \\(X\\) będzie zmienną losową.Definicja 12.1  \\(\\sigma\\)-ciało generowane przez \\(X\\) \\(\\sigma\\)-ciało podzbiorów \\(\\Omega\\) zadane przez\n\\[\\begin{equation*}\n    \\sigma(X) = \\{ X^{-1}(B) \\: : \\: B \\\\mathcal{B}(\\mathbb{R})\\}.\n\\end{equation*}\\]Z własności przeciwobrazu łatwo pokazujemy, że \\(\\sigma(X)\\) jest rzeczywiście\n\\(\\sigma\\)-ciałem. Definicja zmiennej losowej wymaga, aby \\(X^{-1}(B) \\\\mathcal{F}\\) dla\n\\(B \\\\mathcal{B}(\\mathbb{R})\\). Oznacza , że \\(\\sigma(X) \\subseteq \\mathcal{F}\\).\nZauważmy też, że \\(\\sigma(X)\\) jest najmniejsze \\(\\sigma\\)-ciało takie, że\n\\[\\begin{equation*}\n    X \\colon (\\Omega, \\sigma(X)) \\(\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))\n\\end{equation*}\\]\njest mierzalne. \\(\\sigma(X)\\) zawiera informacje o eksperymencie losowym, które możemy\nwywnioskować wyłącznie na podstawie wartości \\(X\\).Przykład 12.1  Losujemy liczbę z odcinka \\([0,1)\\). Niech \\(X(\\omega)\\) będzie pierwszą cyfrą\npo przecinku \\(\\omega \\[0,1)\\) w zapisie dziesiętnym. Jak wygląda \\(\\sigma(X)\\)?\nZauważmy, że \\(X(\\omega) = \\lfloor 10\\omega\\rfloor\\) oraz\n\\[\\begin{equation*}\n    \\{ X = k\\} = \\left[ k/10, (k+1)/10 \\right), \\qquad k=0,1, \\ldots 9.\n\\end{equation*}\\]\nStąd\n\\[\\begin{equation*}\n    \\sigma(X) = \\sigma \\left( [k/10, (k+1)/10) \\: :\\: k=0,1, \\ldots 9 \\right).\n\\end{equation*}\\]Zauważmy, że każde zdarzenie \\(\\\\sigma(X)\\) ma następującą własność. Jeżeli dla \\(\\omega_1, \\omega_2 \\\\Omega\\), \\(X(\\omega_1) = X(\\omega_2)\\), \n\\[\\begin{equation*}\n    \\omega_1 \\\\iff \\omega_2 \\.\n\\end{equation*}\\]Definicja 12.2  Niech \\(\\{X_i\\}_{\\}\\) będzie rodziną zmiennych losowych określonych na \\(\\Omega\\).\nZmienne te są niezależne, jeżeli \\(\\sigma(X_i)\\) (\\(\\sigma\\)-ciała generowane przez \\(X_i\\))\nsą niezależne. Innymi słowy \\(\\{X_i\\}_{\\}\\) są niezależne,\ngdy dla dowolnych parami różnych \\(i_1,i_2,\\ldots, i_n\\\\) oraz dowolnych\n\\(B_1,\\ldots, B_n\\\\mathcal{B}(\\mathbb{R})\\) zachodzi\n\\[\n\\mathbb{P}\\left[ X_{i_1}\\B_1,\\ldots, X_{i_n}\\B_n \\right] = \\mathbb{P}[X_{i_1}\\B_1]\\cdots \\mathbb{P}[X_{i_n}\\B_n].\n\\]Przykład 12.2  Losujemy jednostajnie punkt z prostokąta \\([0,3]\\times[0,1]\\).\nWówczas \\(\\mathbb{P}[\\cdot] = \\lambda_2(\\cdot)/3\\).\nNiech \\(X_1(\\omega) = \\omega_1\\) \\(X_2(\\omega) = \\omega_2\\) dla\n\\(\\omega=(\\omega_1, \\omega_2) \\\\Omega = [0,3]\\times [0,1]\\).\nZbiory z \\(\\sigma(X_1)\\) są postaci\n\\[\\begin{equation*}\n    \\{X_1 \\B\\} = \\{\\omega = (\\omega_1, \\omega_2) \\: : \\: \\omega_1 \\B\\} = B \\times [0,1]\n\\end{equation*}\\]\ndla \\(B \\\\mathcal{B}([0,3])\\). Podobnie\n\\[\\begin{equation*}\n    \\{X_2 \\\\} = [0,1] \\times \n\\end{equation*}\\]\ndla \\(\\\\mathcal{B}([0,1])\\).\nCzyli\n\\[\\begin{multline*}\n\\mathbb{P}[X_1 \\B, \\: X_2 \\] = \\mathbb{P}[B \\times ] = \\lambda_2(B \\times )/3\n\\\\= \\frac{\\lambda_1(B)}{3} \\cdot \\lambda_1() = \\mathbb{P}[X_1 \\B] \\mathbb{P}[X_2\\].\n\\end{multline*}\\]\nZmienne losowe \\(X_1\\) \\(X_2\\) są zatem niezależne.Przykład 12.3  Rozważmy schemat Bernoulliego na przestrzeni \\(\\Omega = \\{0,1\\}^n\\) z \\(\\mathcal{F} = 2^\\Omega\\)\noraz miarą produktową jako probabilistyczną. Zdefiniujmy\n\\[\nX_i(\\omega) = X_i(\\omega_1,\\ldots,\\omega_n) = \\left\\{\n\\begin{array}{cc}\n     1, & \\mbox{jeżeli w $$-tej próbie był sukces} \\\\\n     0, &  \\mbox{jeżeli w $$-tej próbie była porażka}\n   \\end{array}\n   \\right.\n\\] Wówczas \\(X_1,\\ldots, X_n\\) są niezależnymi zmiennymi losowymi.\nWynika bezpośrednio z definicji miary produktowej.Przykład 12.4  Zauważmy, że jeżeli \\(b=d=\\sqrt{2}/2\\) oraz \\(,c > \\sqrt{2}/4\\), \n\\[\\begin{equation*}\n    \\{ X_1 \\[c,d]\\} \\cap \\{X_2 \\[,b]\\} = \\emptyset.\n\\end{equation*}\\]\nWobec czego\n\\[\\begin{equation*}\n    \\mathbb{P} [X_1 \\[c,d], X_2 \\[,b]] = 0.\n\\end{equation*}\\]\nZ kolei\n\\[\\begin{equation*}\n    \\mathbb{P}[X_1 \\[c,d]] \\mathbb{P}[X_2 \\[,b]]>0.\n\\end{equation*}\\]\nWobec tego zmienne \\(X_1\\) \\(X_2\\) nie są niezależne.Twierdzenie 12.1  Niech \\(X_1,\\ldots,X_n\\) będą zmiennymi losowymi niech\n\\(X = (X_1,\\ldots, X_n)\\). Następujące warunki są równoważne:\\(X_1,\\ldots,X_n\\) są niezależne;dla dowolnych \\(B_1,\\ldots,B_n\\\\mathcal{B}(\\mathbb{R})\\), zdarzenia \\(\\{X_1\\B_1\\}, \\ldots, \\{X_n\\B_n\\}\\) są niezależne;\\(\\mu_X = \\mu_{X_1}\\otimes\\ldots \\otimes \\mu_{X_n}\\);\\(F_{X}(t_1,\\ldots, t_n) = F_{X_{1}}(t_1)\\ldots F_{X_n}(t_n)\\).dla dowolnych ograniczonych funkcji borelowskich \\(f_1, \\ldots, f_n\\),\n\\[\\begin{equation*}\n   \\mathbb{E}[f_1(X_1) f_2(X_2) \\cdots f_n(X_n)] = \\mathbb{E}[f_1(X_1)] \\cdot \\mathbb{E}[f_2(X_2)]\n   \\cdots \\mathbb{E}[f_n(X_n)].\n\\end{equation*}\\]Proof. Równoważność 1 2 wynika z definicji.\nPokażemy, że 2 implikuje 4. Weźmy \\(B_i \\(-\\infty,t_i]\\). Wówczas\n\\[\\begin{multline*}\n     F_X(t_1,\\ldots, t_n) = \\mathbb{P}[X_1\\le t_1,\\ldots, X_n\\le t_n] \\\\ =\n     \\mathbb{P}[X_1\\le t_1]\\ldots \\mathbb{P}[X_n\\le t_n] = F_{X_1}(t_1)\\ldots F_{X_n}(t_n).\n\\end{multline*}\\]Załóżmy teraz warunek 4 pokażemy, że implikuje 3. Niech \\(X'\\) będzie \\(n\\)-wymiarową zmienną losową\no rozkładzie \\(\\mu_{X_1}\\otimes\\ldots \\otimes \\mu_{X_n}\\). Pokażemy, że \\(X\\) \\(X'\\) mają taką samą dystrybuantę. Wtedy\n\\[\\begin{multline*}\nF_{X'}(t_1,\\ldots, t_n)\n= \\mu_{X_1}\\otimes\\ldots \\otimes \\mu_{X_n}\\big( (-\\infty; t_1] \\times \\ldots \\times (-\\infty, t_n] \\big) \\\\\n=\\mu_{X_1}((-\\infty,t_1]) \\cdot     \\ldots \\cdot \\mu_{X_n}((-\\infty,t_n])\n= F_{X_1}(t_1)\\ldots F_{X_n}(t_n) = F_X(t_1,\\ldots, t_n).\n\\end{multline*}\\]\nZ twierdzenia o jednoznaczności \\(\\mu_{X'}=\\mu_X\\).Sprawdzamy teraz, że warunek 3 implikuje 2.\nDla dowolnych podzbiorów borelowskich \\(B_1,\\ldots, B_n\\) mamy\n\\[\\begin{multline*}\n\\mathbb{P}\\big[ X_1\\B_1,\\ldots, X_n\\B_n \\big] = \\mu_X(B_1\\times \\cdots \\times B_n) \\\\\n= \\mu_{X_1}(B_1)\\cdot\\ldots\\cdot \\mu_{X_n}(B_n) =\n\\mathbb{P}[X_1\\B_1]  \\cdot \\ldots \\cdot \\mathbb{P}[X_n\\B_n].\n\\end{multline*}\\]\nPokażemy wreszcie, że 3. pociąga też 5. Mamy\n\\[\\begin{multline*}\n      \\mathbb{E}[f_1(X_1) f_2(X_2) \\cdots f_n(X_n)] =\n      \\int_{\\mathbb{R}^n} \\prod_{j=1}^nf_j(x_j) \\mu_{\\vec{X}}(\\mathrm{d}x_1\\ldots x_n) =\\\\\n      \\prod_{j=1}^n \\int_\\mathbb{R} f_j(x_j) \\mu_{X_j}(\\mathrm{d}x_j)=\n       \\mathbb{E}[f_1(X_1)] \\cdot \\mathbb{E}[f_2(X_2)]\n      \\cdots \\mathbb{E}[f_n(X_n)].\n\\end{multline*}\\]\nNa koniec uzasadnimy, że 5. pociąga 2. Jeżeli rozważymy \\(f_j(y) = \\mathbf{1}_{B_j}(y)\\), \n\\(f_j(X_j(\\omega)) = \\mathbf{1}_{X_j \\B_j}(\\omega)\\) oraz\n\\[\\begin{equation*}\n\\mathbb{E}[f_j(X_j)] = \\mathbb{P}[X_j\\B_j]\n\\end{equation*}\\]\noraz\n\\[\\begin{equation*}\n\\mathbb{E}[f_1(X_1) f_2(X_2) \\cdots f_n(X_n)] =\n  \\mathbb{P}[X_1\\B_1, X_2\\B_2 \\ldots X_n \\B_n]\n\\end{equation*}\\]Wniosek 12.1  Zmienne losowe \\(X_1,\\ldots, X_n\\) mające rozkłady dyskretne są niezależne wtedy tylko wtedy gdy dla dowolnych\n\\(s_1\\S_{X_1},\\ldots, s_n\\S_{X_n}\\) zachodzi\n\\[\\begin{equation}\\label{eq:kwt1}\n\\mathbb{P}[X_1=s_1,\\ldots, X_n = s_n] = \\mathbb{P}[X_1=s_1]\\cdot\\ldots\\cdot \\mathbb{P}[X_n=s_n]\n\\tag{12.1}\n\\end{equation}\\]Proof. Jeżeli zmienne losowe \\(X_1,\\ldots, X_n\\) są niezależne, warunek (12.1)\njest oczywiście spełniony.\nImplikację odwrotną pokażemy jedynie dla \\(n=2\\) (dla uproszczenia dowodu).\nOznaczmy przez \\(S_{X_1}\\) oraz \\(S_{X_2}\\)\nnośniki rozkładów zmiennych losowych \\(X_1\\) oraz \\(X_2\\) (czyli zbiorem ich wartości).\nKorzystając z warunku\n(12.1) otrzymujemy dla dowolnych zbiorów borelowskich \\(B_1,B_2\\):\n\\[\\begin{multline*}\n  \\mathbb{P}[X_1\\B_1, X_2\\B_2] = \\mathbb{P}[X_1 \\B_1\\cap S_{X_{1}}, X_2 \\B_2 \\cap S_{X_2}]\\\\\n= \\sum_{\\substack{ x_1 \\B_1 \\cap S_{X_1}\\\\  x_2 \\B_2 \\cap S_{X_2}}}\n\\mathbb{P}[X_1 = x_1, X_2 = x_2]\n= \\sum_{x_1 \\B_1 \\cap S_{X_1}}\\sum_{x_2 \\B_2 \\cap S_{X_2}} \\mathbb{P}[X_1 = x_1]\\mathbb{P}[X_2 = x_2]\\\\\n= \\mathbb{P}[X_1 \\B_1]\\mathbb{P}[X_2\\B_2]\n\\end{multline*}\\]Wniosek 12.2  Jeżeli zmienne losowe \\(X\\) \\(Y\\) są niezależne mają wartości oczekiwane, \n\\[\\begin{equation*}\n\\mathbb{E}[XY] = \\mathbb{E}[X]\\mathbb{E}[Y].\n\\end{equation*}\\]Proof. Niech \\(\\vec{X}=(X,Y)\\).\nRozważmy rozkład łączny \\(\\mu_{\\vec{X}}\\) zmiennych \\(X\\) \\(Y\\).\nWówczas \\(\\mu_{\\vec{X}} = \\mu_{X}\\otimes \\mu_Y\\). Mamy zatem\n\\[\\begin{multline*}\n    \\mathbb{E}[XY] = \\int_{\\mathbb{R}^2} xy \\: \\mu_{\\vec{X}}(\\mathrm{d}xy) =\n    \\int_{\\mathbb{R}}\\int_\\mathbb{R} xy \\: \\mu_{X} (\\mathrm{d}x) \\mu_{Y}(\\mathrm{d}y ) = \\\\\n    \\int_{\\mathbb{R}} x \\: \\mu_{X} (\\mathrm{d}x)\n    \\int_{\\mathbb{R}} y \\: \\mu_{Y}(\\mathrm{d}y) = \\mathbb{E}[XY].\n\\end{multline*}\\]Wniosek 12.3  Zmienne losowe \\(X_1,\\ldots, X_n\\) o gęstościach \\(f_1,\\ldots, f_n\\) są niezależne wtedy tylko wtedy,\ngdy wektor\n\\(\\vec{X} = (X_1,\\ldots, X_n)\\) ma gęstość\n\\[\nf(x_1,x_2,\\ldots,x_n) = f_1(x_1)f_2(x_2)\\ldots f_n(x_n).\n\\]Jeżeli zmienne losowe \\(X_1,X_2\\) są niezależne mają absolutnie ciągły rozkład,\nmożna skutecznie liczyć rozkłady ich sum:Twierdzenie 12.2  Załóżmy, że \\(X_1\\) \\(X_2\\) są niezależnymi zmiennymi losowymi o rozkładach\nabsolutnie ciągłych z gęstościami \\(f_1\\) \\(f_2\\). Wówczas\nzmienna losowa \\(Z=X_1+X_2\\) ma rozkład absolutnie ciągły z gęstością\n\\[\n  f_Z(x) = f_1\\ast f_2(x) = \\int_\\mathbb{R} f_1(x-y)f_2(y) \\mathrm{d}y.\n\\]\nFunkcję \\(f_Z\\) nazywamy splotem funkcji \\(f_1\\) \\(f_2\\).Proof. Niech \\(\\vec{X}=(X_1, X_2)\\).\nDla dowolnego \\(t \\\\mathbb{R}\\):\n\\[\\begin{align*}\n    F_Z(t)=\\mathbb{P}[X_1+X_2 \\leq t] & = \\mu_{\\vec{X}}\\left( \\{(x_1,x_2):\\; x_1+x_2 \\leq t\\} \\right)\\\\\n    &= \\int\\int_{\\{(x_1,x_2):\\; x_1+x_2\\leq t \\}} \\mu_{\\vec{X}}(\\mathrm{d}x_1x_2)\\\\\n    &= \\int\\int_{\\{(x_1,x_2):\\; x_1+x_2\\leq t\\}} f_1(x_1) f_2(x_2) \\mathrm{d}x_1 \\mathrm{d}x_2\\\\\n    &= \\int\\int_{\\{(z,y):\\; z\\leq t\\}} f_1(z-y) f_2(y) \\mathrm{d}z \\mathrm{d}y\\\\\n    &= \\int_{(-\\infty,t]}\\int_{\\mathbb{R}} f_1(z-y) f_2(y) \\mathrm{d}y \\mathrm{d}z\\\\\n    &= \\int_{(-\\infty,t]} f_Z(z) \\mathrm{d}z.\n  \\end{align*}\\]\nTeza wynika z twierdzenia o jednoznaczności.Podstawowych intuicji związanych z operacją splotu można nabrać z filmu\nGranta Sandersona.Przykład 12.5  Niech \\(X_1\\) \\(X_2\\) będą niezależnymi zmiennymi losowymi o rozkładzie jednostajnym na \\([0,1]\\),\nczyli \\(U([0,1])\\). Szukamy gęstości \\(X_1+X_2\\).\nZnamy gęstości \\(X_1\\) oraz \\(X_2\\)\n\\[\n  f_1(x_1) = {\\bf 1}_{[0,1]}(x_1), \\qquad\n  f_2(x_2) = {\\bf 1}_{[0,1]}(x_2).\n\\]\nZatem \\(X_1+X_2\\) ma gęstość\n\\[\\begin{align*}\n    f_1\\ast f_2(x) &= \\int_\\mathbb{R}  {\\bf 1}_{[0,1]}(x - y) {\\bf 1}_{[0,1]}(y)\\mathrm{d}y\\\\\n    &=\\int_\\mathbb{R} {\\bf 1}_{[x-1,x]}(y) {\\bf 1}_{[0,1]}(y)\\mathrm{d}y = \\big| [x-1,x] \\cap [0,1] \\big|\n  \\end{align*}\\]\nStąd\n\\[\nf_1\\ast f_2(x) = \\left\\{\n\\begin{array}{cc}\n   0 & \\mbox{dla } x<0 \\\\\n   x & \\mbox{dla } x\\[0,1]\\\\\n   2-x & \\mbox{dla } x\\[1,2]\\\\\n   0 & \\mbox{dla } x>2\n\\end{array}\n\\right.\n\\]Przykład 12.6  Przypomnijmy, że zmienna losowa\n\\(X\\) ma rozkład normalny z parametrami \\(m\\\\mathbb{R}\\) \\(\\sigma>0\\),\n\\(\\mathcal{N}(m,\\sigma^2)\\) jeżeli jej gęstość\njest zadana wzorem\n\\[\n  f(x) = \\frac{1}{\\sqrt{2\\pi} \\sigma} e^{-\\frac{(x-m)^2}{2\\sigma^2}}.\n  \\]Załóżmy, że \\(X_1, X_2\\) są niezależnymi zmiennymi losowymi o rozkładzie \\(N(m_1,\\sigma_1^2)\\) oraz \\(N(m_2,\\sigma_2^2)\\), czyli z gęstościami \\[\n  f_i(x) = \\frac{1}{\\sqrt{2\\pi} \\sigma_i} e^{-\\frac{(x-m_i)^2}{2\\sigma_i^2}} \\qquad =1,2. \\]\nPokazuje się wówczas, że\n\\[   f_1\\ast f_2(x) = \\frac{1}{2\\pi \\sigma_1\\sigma_2} \\int_\\mathbb{R} e^{-\\frac{(x-y-m_1)^2}{2\\sigma_1^2} -\\frac{(y-m_2)^2}{2\\sigma_2^2}  }dy\n     = \\frac{1}{\\sqrt{2\\pi (\\sigma_1^2+\\sigma^2_2)}} e^{-\\frac{(x-m_1-m_2)^2}{2(\\sigma_1^2+\\sigma^2_2)}}\n\\]\nZatem \\(X_1+X_2\\) ma rozkład \\(\\mathcal{N}(m_1+m_2, \\sigma_1^2+\\sigma_2^2)\\).Jeszcze więcej o splotach od Granta Sandersona.","code":""},{"path":"wariancja.html","id":"wariancja","chapter":"13 Wariancja","heading":"13 Wariancja","text":"Dla zmiennej losowej \\(X\\) jej wartość oczekiwana \\(\\mathbb{E}[X]\\) jest średnią ważoną.\nW niektórych przypadkach, kiedy zmienna \\(X\\) jest zbyt skomplikowana,\naby wydobyć jakiekolwiek ilościowe informacje o \\(X\\),\nzmuszeni jesteśmy przybliżać \\(X\\) przez jej średnią \\(\\mathbb{E}[X]\\).\nChcielibyśmy wtedy wiedzieć jaki jest błąd takiego przybliżenia.\ntego będzie nam służyła wariancja.\nWariancję będziemy definiować dla zmiennych losowych, które są dostatecznie regularne.Definicja 13.1  Powiemy, że zmienna losowa \\(X\\) jest całkowalna z kwadratem, jeżeli\n\\(\\mathbb{E}\\left[X^2 \\right]<\\infty\\).Przypomnijmy, że funkcja \\(\\varphi \\colon \\mathbb{R} \\\\mathbb{R}\\) jest wypukła,\njeżeli dla każdych \\(x, y \\\\mathbb{R}\\) oraz \\(\\alpha \\(0,1)\\)\n\\[\\begin{equation}\n    \\varphi(\\alpha x +(1-\\alpha)y) \\leq \\alpha \\varphi(x) +(1-\\alpha)\\varphi(y).\n    \\tag{13.1}\n\\end{equation}\\]\nGeometrycznie powyższy warunek oznacza, że odcinek łączący dwa punkty na wykresie \\((x, \\varphi(x))\\)\noraz \\((y, \\varphi(y))\\) leży w całości ponad wykresem.Lemma 13.1  (Nierówność Jensena) Niech \\(X\\) będzie zmienną losową taką, że \\(\\varphi(X)\\) ma wartość oczekiwaną dla\npewnej wypukłej funkcji \\(\\varphi \\colon \\mathbb{R} \\\\mathbb{R}\\). Wówczas\n\\[\\begin{equation*}\n    \\varphi\\left( \\mathbb{E} [X] \\right) \\leq \\mathbb{E}\\left[ \\varphi(X) \\right].\n\\end{equation*}\\]Proof. Pozostawiamy jako zadanie.Zauważmy, że jeżeli \\(\\mathbb{P}[X=x] =\\alpha\\) \\(\\mathbb{P}[X=y]=1-\\alpha\\), \nnierówność Jensena sprowadza się (13.1).\nRzeczywiście, mamy\n\\[\\begin{align*}\n    \\mathbb{E}[X] & = x \\mathbb{P}[X=x] +y\\mathbb{P}[X=y] \\\\ &= x\\alpha+y(1-\\alpha).\n\\end{align*}\\]\nStąd\n\\[\\begin{equation*}\n    \\varphi\\left(\\mathbb{E}[X] \\right) = \\varphi(\\alpha x +(1-\\alpha)y)\n\\end{equation*}\\]\njest nie większe niż\n\\[\\begin{align*}\n    \\mathbb{E}\\left[\\varphi(X)\\right] &= \\varphi(x) \\mathbb{P}[X=x] +\\varphi(y)\\mathbb{P}[X=y]\n    \\\\&= \\varphi(x)\\alpha+\\varphi(y)(1-\\alpha).\n\\end{align*}\\]Przypomnijmy, że jeżeli funkcja \\(\\varphi\\) jest dwukrotnie różniczkowalna,\njest ona wypukła wtedy tylko wtedy, gdy \\(\\varphi''(x) \\geq 0\\)\ndla wszystkich \\(x \\\\mathbb{R}\\).Zauważmy, że stosując nierówność Jensena funkcji wypukłej \\(\\varphi(x)=x^2\\) otrzymujemy\n\\[\\begin{equation*}\n    \\mathbb{E}[|X|]^2 \\leq \\mathbb{E}\\left[X^2 \\right].\n\\end{equation*}\\]\nOznacza , że każda zmienna całkowalna z kwadratem posiada wartość oczekiwaną.Definicja 13.2  Niech \\(X\\) będzie zmienną losową całkowalną z kwadratem. Liczbę\n\\[\\mathbb{V}ar  [X] =\\mathbb{E}\\left[(X-\\mathbb{E} [X])^2 \\right]\n  \\] nazywamy wariancją zmiennej losowej \\(X\\).Pierwiastek z wariancji nazywamy odchyleniem standardowym\n\\[\n  \\sigma_X =  \\sqrt{\\mathbb{V}ar[ X]}.\n  \\]Wartość oczekiwana odpowiada średniej wartości,\nwariancja opisuje odchylenie od wartości oczekiwanej.\nDla przykładu instytucje finansowe\nopisując inwestycję podają dwa kluczowe parametry:\nstopę zwrotu (wartość oczekiwaną zysku) oraz ryzyko (odchylenie standardowe).\nCelem inwestycji jest taki dobór instrumentów,\naby przy określonej stopie zwrotu zminimalizować ryzyko.Przykład 13.1  Przypuśćmy, że właśnie otrzymaliśmy propozycję nie odrzucenia;\nktoś podarował nam dwa losy na pewną loterię.\nOrganizatorzy loterii sprzedają \\(100\\) losów na cotygodniowe losowanie.\nKażdy z losów jest wybierany w jednorodnym procesie losowym, znaczy, że każdy los może być\nwybrany z takim samym prawdopodobieństwem — szczęśliwy\nwłaściciel wybranego losu wygrywa sto milionów dolarów.\nPozostałe \\(99\\) losów nic nie wygrywa.Możemy teraz wykorzystać nasz prezent na dwa sposoby: albo kupujemy dwa losy na samo losowanie,\nalbo kupimy po jednym losie na dwa różne losowanie.\nKtóra strategia jest lepsza? Spróbujmy przeanalizować\nprzy użyciu zmiennych losowych \\(X_1\\) \\(X_2\\)\nodpowiadających wysokości wygranej dla pierwszego dla drugiego losu.\nWartość oczekiwana \\(X_1\\), w milionach, wynosi\n\\[\n\\mathbb{E}[X_1] = \\frac{99}{100} \\cdot 0 + \\frac{1}{100} \\cdot 100 = 1\n\\]\njest taka sama dla \\(X_2\\). Wartości oczekiwane są addytywne,\ntak więc średnia całkowita wygrana (w milionach) wynosi\n\\[\n\\mathbb{E}[X_1 + X_2] = \\mathbb{E}[X_1] + \\mathbb{E}[X_2] = 2,\n\\]\nniezależnie od tego, jaką strategię wybierzemy.\nMimo obydwie strategie wyglądają różnie.\nNie patrzmy jednak na wartości oczekiwane \nprzeanalizujmy dokładnie rozkład zmiennej losowej \\(X_1 + X_2\\):Gdy kupimy dwa losy na tej samej loterii,\nwówczas mamy \\(98\\%\\) szansy przegranej \\(2\\%\\) szansy wygrania \\(100\\) milionów dolarów.\nJeśli kupimy je na różne losowania, mamy \\(98,01\\%\\) szansy przegranej,\nczyli odrobinę więcej niż poprzednio; mamy \\(0,01\\%\\) szansy wygrania \\(200\\) milionów dolarów,\nco jest również troszkę więcej niż poprzednio, nasze szanse na wygranie \\(100\\) milionów dolarów wynoszą teraz \\(1,98\\%\\).\nTak więc rozkład \\(X_1 + X_2\\) w drugim przypadku jest bardziej rozproszony:\nwartość oczekiwana, \\(100\\) milionów dolarów, jest mniej prawdopodobna,\nale wartości ekstremalne są odrobinę bardziej prawdopodobne.Wariancja ma służyć właśnie analizy pojęcia rozproszenia zmiennej losowej.\nMierzymy rozproszenie jako kwadrat odchylenia zmiennej losowej\nod jej wartości oczekiwanej. W przypadku 1 wariancja wynosi\n\\[\\begin{align*}\n& 0{,}98(0M - 2M)^2 + 0{,}02(100M - 2M)^2 \\\\&= 196M^2,\n\\end{align*}\\]\nw przypadku \\(2\\),\n\\[\\begin{align*}\n& 0{,}9801(0M - 2M)^2 + 0{,}0198(100M - 2M)^2 \\\\ & + 0{,}0001(200M - 2M)^2 \\\\&= 198M^2.\n\\end{align*}\\]\nTak jak oczekiwaliśmy,\ndruga wariancja jest odrobinę większa,\nponieważ rozkład losowy w przypadku \\(2\\) jest odrobinę bardziej rozproszony.Przykład 13.2  Rozważmy jeszcze jeden przykład o podobnej naturze.\nStudent w trakcie roku ma zaliczenia dwa kolokwia. Procentowy wynik\nkażdego kolokwium jest jednostajnie rozłożony na odcinku \\([0,1]\\).\nWyniki obu sprawdzianów są od siebie niezależne.\nJeżeli przez \\(U_1\\) \\(U_2\\) oznaczymy wyniki w odpowiednio pierwszym \ndrugim kolokwium, końcowa ocena studenta\njest wyliczona na podstawie wyniku\n\\[\n    X=U_1+U_2,\n\\]\nŚredni wynik z obu sprawdzianów \\(\\mathbb{E}[X]=1\\).\nPewien student nie mógł przystąpić pierwszego kolokwium,\nwobec czego prowadzący postanowił przeskalować wynik z pierwszego kolokwium.\nCzy jest rozwiązanie korzystne dla studenta? Wówczas ocena jest wyliczana na podstawie\nwyniku\n\\[\n    Y=2U_2\n\\]\nZe średnim wynikiem \\(\\mathbb{E}[Y]=1\\).\nAby dokładniej przeanalizować obie możliwości zauważmy, że\n\\[\\begin{equation*}\n\\mathbb{V}ar[Y] = \\int_0^1 (2x-1)^2 \\mathrm{d}x=4/3.\n\\end{equation*}\\]\nAby policzyć wariancję zmiennej \\(X\\) przypomnijmy, że ma ona gęstość zadaną przez\n\\[\\begin{equation*}\nf_X(x) = x \\mathbf{1}_{[0,1]}(x) + (2-x)\\mathbf{1}_{(1,2]}(x).\n\\end{equation*}\\]\nStąd\n\\[\\begin{align*}\n\\mathbb{V}ar[X] =& \\int_0^1 (x-1)^2 x \\mathrm{d}x \\\\ &+\\int_1^2(1-x)^2(2-x) \\mathrm{d}x =2/3.\n\\end{align*}\\]\nRozwiązanie zaproponowane przez prowadzącego ma istotnie większą wariancję. Sugeruje , że\nw przypadku przeskalowania wyniku drugiego kolokwium ostateczny wynik jest bardziej rozproszony.\nWydać też na symulacjach.Powodem jest , że rozkład \\(Y\\) dopuszcza wartości ekstremalne z większym prawdopodobieństwem.\nRzeczywiście, zauważmy, że \\(Y\\) ma rozkład o gęstości \\(f_Y(x) = \\mathbf{1}_{[0,2]}(x)/2\\).Twierdzenie 13.1  Niech \\(X\\) \\(Y\\) będą zmiennymi losowymi takimi, że \\(\\mathbb{E} [X^2]\\),\\(\\mathbb{E}[Y^2] <\\infty\\).\nWówczas\\(\\mathbb{V}ar[ X] <\\infty\\)\\(\\mathbb{V}ar[ X] = \\mathbb{E} [X^2] - (\\mathbb{E} [X])^2\\)\\(\\mathbb{V}ar[ X ]\\ge 0\\)\\(\\mathbb{V}ar[aX] = ^2\\mathbb{V}ar[X]\\)\\(\\mathbb{V}ar[X+] = \\mathbb{V}ar[X]\\)\\(\\mathbb{V}ar[ X] = 0\\) wtedy tylko wtedy, gdy \\(X\\) jest stałe z prawdopodobieństwem jedenJeżeli \\(X\\) \\(Y\\) są niezależne, \\(\\mathbb{V}ar[X+Y] = \\mathbb{V}ar[X] + \\mathbb{V}ar[Y]\\)Proof. Punkty 1 pokazaliśmy już wyżej. Punkt 2. wynika z rachunku\n\\[\\begin{align*}\n    \\mathbb{V}ar[X] & = \\mathbb{E}\\left[X^2 -2X\\mathbb{E}[X] + \\mathbb{E}[X]^2\\right]\\\\\n    & = \\mathbb{E}\\left[X^2 \\right] - 2 \\mathbb{E}\\left[X\\mathbb{E}[X] \\right] + \\mathbb{E}[X]^2\\\\\n    & = \\mathbb{E}\\left[X^2 \\right] - \\mathbb{E}[X]^2.\n\\end{align*}\\]\nDowód punktów 3-6 pozostawiamy jako ćwiczenie.\nPunkt 7 wynika z punktu 2:\n\\[\\begin{align*}\n\\mathbb{V}ar(X+Y)   = &   \\mathbb{E}\\left[ (X+Y)^2 \\right] - \\mathbb{E}[X + Y]^2\\\\\n     = & \\mathbb{E}[X^2] + 2 \\mathbb{E}[XY] + \\mathbb{E} [Y^2]  \\\\ &- (\\mathbb{E}[X]^2 +\n    2\\mathbb{E}[X] \\cdot \\mathbb{E} [Y] +\\mathbb{E}[ Y]^2)\\\\\n   = & \\mathbb{E}[ X^2]  - (\\mathbb{E}[X]^2  + \\mathbb{E} [Y^2] \\\\\n   &- (\\mathbb{E}[Y])^2  = \\mathbb{V}ar[ X] + \\mathbb{V}ar[ Y].\n\\end{align*}\\]\n□Jeżeli \\(X\\) ma rozkład dyskretny zadany przez\n\\(\\mathbb{P}[X=x_i] = p_i\\), \\(m = \\mathbb{E} [X]\\), \n\\[\\begin{align*}\n\\mathbb{V}ar[ X] & = \\sum_i p_i (x_i - m)^2  \\\\ &= \\sum_i  x_i^2p_i - m^2\n\\end{align*}\\]\nJeżeli natomiast \\(X\\) ma rozkład absolutnie ciągły z gęstością \\(g\\) \\(m = \\mathbb{E}[X]\\), \n\\[\\begin{align*}\n\\mathbb{V}ar [X]  & = \\int_\\mathbb{R} (x-m)^2 g(x)\\mathrm{d}x \\\\\n& = \\int_\\mathbb{R} x^2 g(x)\\mathrm{d}x - m^2.\n\\end{align*}\\]Przykład 13.3  Załóżmy, że zmienna losowa \\(X\\) ma rozkład geometryczny z parametrem \\(p>0\\)\n(\\(X\\sim{\\rm Geom}(p)\\)), tzn. \\(\\mathbb{P}[X=k] = p(1-p)^{k-1}\\), dla \\(k\\\\mathbb{N}\\).\nPrzypomnijmy, że \\(X\\) oznacza moment pierwszego sukcesu w nieskończonym schemacie Bernoulliego.\nIle wynosi \\(\\mathbb{V}ar[X]\\)?\nDla dowolnych \\(p, q \\(0,1)\\) mamy\n\\[\\begin{align*}\n    \\sum_{k=0}^\\infty q^kp & = \\frac{p}{1-q}\\\\\n    \\sum_{k=0}^\\infty k q^{k-1}p & = \\frac{p}{(1-q)^2}\\\\\n    \\sum_{k=0}^\\infty k(k-1) q^{k-2}p & = \\frac{2p}{(1-q)^3}\\\\\n    \\sum_{k=0}^\\infty k(k-1) q^{k-1}p & = \\frac{2pq}{(1-q)^3}.\n\\end{align*}\\]\nJeżeli podstawimy \\(q=1-p\\), druga czwarta równość dają\n\\[\\begin{align*}\n    \\mathbb{E}[X]& = \\sum_{k=0}^\\infty k q^{k-1}p  = \\frac{p}{(1-q)^2}=\\frac 1p\\\\\n    \\mathbb{E}[X(X-1)] & = \\sum_{k=0}^\\infty k(k-1) q^{k-1}p  = \\frac{2pq}{(1-q)^3} \\\\ &=\n    \\frac{2(1-p)}{p^2}\n\\end{align*}\\]\nWówczas\n\\[\\begin{align*}\n   \\mathbb{V}ar[ X] &  = \\mathbb{E}\\left[X^2\\right] - \\mathbb{E}[X]^2 \\\\ &=\n   \\mathbb{E}[X(X-1)] +\\mathbb{E}[X] -\\mathbb{E}[X]^2\n   \\\\ & = \\frac{1-p}{p^2}\n\\end{align*}\\]Przykład 13.4  Jeżeli \\(X\\sim \\mathrm{Bin}(n,p)\\)\n(\\(X\\) ma rozkład dwumianowy z parametrami \\(n,p\\)), \\(X\\) możemy przestawić\nw postaci \\(X = X_1+\\ldots + X_n\\), gdzie\n\\[X_i = \\left\\{\\begin{array}{cc}\n                 1 & \\mbox{ w $$-tym doświadczeniu jest sukces  } \\\\\n                 0 & \\mbox{ w $$-tym doświadczeniu jest porażka }\n               \\end{array}\n\\right.\n\\]\nZmienne \\(X_i\\) są niezależne oraz\n\\[\\begin{align*}\n\\mathbb{E} X_i & = p,\\\\\n\\mathbb{V}ar[ X_i] & = \\mathbb{E} \\left[X_i^2\\right] - (\\mathbb{E} X_i)^2\n\\\\ &= \\mathbb{E} [X_i] - p^2 = p(1-p).\n\\end{align*}\\]\nZatem z powyższego twierdzenia\n\\[\\begin{align*}\n  \\mathbb{E} [X] & = \\sum_{=1}^n \\mathbb{E} [X_i] = np\\\\\n  \\mathbb{V}ar[ X] & = \\sum_{=1}^n \\mathbb{V}ar [X_i] = np(1-p)\n\\end{align*}\\]Przykład 13.5  Jeżeli \\(X\\sim \\mathcal{N}(m,\\sigma^2)\\) (zmienna losowa \\(X\\) ma rozkład normalny z\nparametrami \\(m, \\sigma^2\\)), \n\\[\\begin{align*}\n  \\mathbb{E} X & =\n  \\frac{1}{\\sqrt{2\\pi}\\sigma} \\int_\\mathbb{R} x e^{-\\frac{(x-m)^2}{2\\sigma^2}} \\mathrm{d}x\n  \\\\ & \\overset{ y=(x-m)/\\sigma}{ =} \\frac{1}{\\sqrt{2\\pi}} \\int_\\mathbb{R} (\\sigma y + m )\n  e^{-y^2/2} \\mathrm{d}y = m\n\\end{align*}\\]\noraz\n\\[\\begin{align*}\n  \\mathbb{V}ar[ X] =&  \\mathbb{E}(X-m)^2  \n  \\\\ =&  \\frac{1}{\\sqrt{2\\pi}\\sigma} \\int_\\mathbb{R} (x-m)^2 e^{-\\frac{(x-m)^2}{2\\sigma^2}} \\mathrm{d}x\\\\\n  &\\overset{ y=(x-m)/\\sigma}{ =} \\frac{1}{\\sqrt{2\\pi}} \\int_\\mathbb{R} \\sigma^2 y^2   e^{-y^2/2} \\mathrm{d}y \\\\\n  =&\\frac{\\sigma^2}{\\sqrt{2\\pi}}\\big( -y e^{-\\frac{y^2}2} \\big)\\Big|_{-\\infty}^{+\\infty}\n   \\\\ &+ \\frac{\\sigma^2}{\\sqrt{2\\pi}} \\int_\\mathbb{R} e^{-y^2/2}\\mathrm{d}y = \\sigma^2.\n\\end{align*}\\]","code":""},{"path":"kowariancja.html","id":"kowariancja","chapter":"14 Kowariancja","heading":"14 Kowariancja","text":"Wariancja stanowi ilościowy opis rozproszenia rozkładu zmiennej.\nOpiszemy teraz jej dwuliniowy odpowiednik, który można\ntraktować jak ilościowy opis zależności dwóch zmiennych losowych.Definicja 14.1  Niech \\(X\\) \\(Y\\) będą zmiennymi losowymi całkowalnymi z kwadratem.\nLiczbę\n\\[\n  {\\rm Cov}(X,Y) = \\mathbb{E}\\left[ (X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y]) \\right]\n  \\] nazywamy kowariancją \\(X\\) \\(Y\\).Jeżeli \\(\\mathrm{Cov}(X,Y)=0\\), \\(X\\) \\(Y\\) nazywamy nieskorelowanymi.Zauważmy, że przy powyższych założeniach kowariancja jest dobrze zdefiniowana.Lemma 14.1  (Nierówność Schwarza) Dla zmiennych losowych \\(X\\) \\(Y\\),\n\\[\n\\mathbb{E}[|XY|] \\le \\big( \\mathbb{E}[X^2] \\big)^{1/2}\\big( \\mathbb{E}[ Y^2] \\big)^{1/2},\n\\]Proof. Pozostawiamy jako zadanie.□Zastępując \\(X\\) przez \\(X -\\mathbb{E}[X]\\) oraz \\(Y\\) przez \\(Y -\\mathbb{E}[Y]\\) otrzymujemy\n\\[\n{\\rm Cov}(X,Y) \\le \\big( \\mathbb{V}ar[ X] \\big)^{1/2}\\big( \\mathbb{V}ar[ Y] \\big)^{1/2}.\n\\]\nJeżeli więc \\(X\\) \\(Y\\) są całkowalne z kwadratem, \\(\\mathrm{Cov}(X,Y)<\\infty\\).Twierdzenie 14.1  Niech \\(X\\), \\(Y\\) \\(Z\\) będą zmiennymi losowymi całkowalnymi z kwadratem\\(\\mathrm{ Cov}(X,Y) = \\mathbb{E}[XY] -\\mathbb{E}[X] \\mathbb{E}[Y]\\).\\(\\mathrm{Cov}(X,X) = \\mathbb{V}ar [X]\\).\\(\\mathrm{Cov} (X,Y) = \\mathrm{Cov} (Y,X)\\).Kowariancja jest operatorem dwuliniowym\\[\\begin{equation*}\n    {\\rm Cov}(aX+,Z) = {\\rm Cov}(X,Z)\n  + b  {\\rm Cov}(Y,Z).\n\\end{equation*}\\]Proof. Ćwiczenie.\n□Przykład 14.1  Niech \\(X\\) \\(Y\\) będą takie, że dwuwymiarowy wektor losowy \\(\\vec{X}=(X,Y)\\)\nma dwuwymiarowy rozkład normalny z parametrami \\(\\vec{m}=(0,0)\\) oraz\n\\[\\begin{equation*}\n    \\Sigma = \\left( \\begin{array}{cc} 1 & \\rho \\\\ \\rho & 1 \\end{array} \\right)\n\\end{equation*}\\]\ndla \\(\\rho \\(-1,1)\\).\nPrzypomnijmy, że wektor \\(\\vec{X}\\) ma rozkład o gęstości\n\\[\\begin{equation*}\nf_\\vec{X}(x,y) =\n\\frac{1}{2 \\pi \\mathrm{det}(\\Sigma)^{1/2}} e^{ -\\langle \\Sigma^{-1}(x,y)^T, (x,y)^T \\rangle /2}\n\\end{equation*}\\]\nSprawdzimy najpierw ile wynosi średnia \\(X\\). Stosując podstawienie \\(z=-x\\) \\(w=-y\\)\notrzymujemy\n\\[\\begin{align*}\n    \\mathbb{E}[X] & = \\int_{\\mathbb{R}^2} x f_{\\vec{X}}(x,y) \\mathrm{d}xy \\\\\n    & = \\int_{\\mathbb{R}^2} -z f_{\\vec{X}}(z,w) \\mathrm{d}zw.\n\\end{align*}\\]\nWobec czego \\(\\mathbb{E}[X]=0\\). Podobnie sprawdzamy, że \\(\\mathbb{E}[Y]=0\\).\nIle wynosi \\(\\mathrm{Cov}(X,Y)\\)? Na pierwszy rzut oka wyrażenie całkowe\n\\[\\begin{align*}\n    &\\mathrm{Cov}(X,Y) = \\\\ &\\int_{\\mathbb{R}^2}\n\\frac{xy}{2 \\pi \\mathrm{det}(\\Sigma)^{1/2}} e^{ -\\langle \\Sigma^{-1}(x,y)^T, (x,y)^T \\rangle /2}\n\\mathrm{d}xy\n\\end{align*}\\]\nwygląda niezachęcająco. Aby się z nim efektywnie uporać musimy przedstawić\nwyraz wykładniczy w prostszej postaci. Sprowadza się analizy wykładnika\n\\[\\begin{equation*}\n\\langle \\Sigma^{-1}(x,y)^T, (x,y)^T \\rangle\n\\end{equation*}\\]\nczyli formy kwadratowej związanej z macierzą \\(\\Sigma\\). W pierwszym kroku diagonalizujemy\n\\(\\Sigma\\). Otrzymując\n\\[\\begin{equation*}\n    \\Sigma = Q \\left( \\begin{array}{cc} 1+\\rho & 0 \\\\ 0 & 1-\\rho \\end{array} \\right) Q^T,\n\\end{equation*}\\]\ngdzie\n\\[\\begin{equation*}\n    Q = \\left( \\begin{array}{cc} \\sqrt{2}/2 & \\sqrt{2}/2 \\\\ \\sqrt{2}/2 & -\\sqrt{2}/2 \\end{array} \\right)\n\\end{equation*}\\]\njest macierzą symetrii względem prostej \\(y=\\tan(\\pi/8)x\\). Jeżeli zatem położymy\n\\[\\begin{equation}\n\\left( \\begin{array}{c} z  \\\\ w \\end{array} \\right) = Q^T\n\\left( \\begin{array}{c} x  \\\\ y \\end{array} \\right)\n\\tag{14.1}\n\\end{equation}\\]\n\n\\[\\begin{equation*}\n\\langle \\Sigma^{-1}(x,y)^T, (x,y)^T \\rangle =\\\\\n\\left\\langle\\left( \\begin{array}{cc} 1/(1+\\rho) & 0 \\\\ 0 & 1/(1-\\rho) \\end{array} \\right)  \n\\left( \\begin{array}{c} z  \\\\ w \\end{array} \\right) ,\n\\left( \\begin{array}{c} z  \\\\ w \\end{array} \\right) \\right\\rangle \\\\\n=\\frac{z^2}{1+\\rho} + \\frac{w^2}{1-\\rho}.\n\\end{equation*}\\]\nWspółrzędne \\((z,w)\\) odpowiednie podstawienie dla naszej całki. Zauważmy, że \\(\\mathrm{det}(Q)=1\\)\noraz, że\n\\[\\begin{equation*}\nxy=\\frac{z^2-w^2}{2}.\n\\end{equation*}\\]\nStąd, stosując podstawienie (14.1) w całce otrzymujemy\n\\[\\begin{equation*}\n\\mathrm{Cov}(X,Y) = \\\\\\int_{\\mathbb{R}^2} \\frac{z^2-w^2}{2}\n\\frac{1}{\\sqrt{2\\pi}(1+\\rho)} e^{-z^2/2(1+\\rho)} \\\\\n\\frac{1}{\\sqrt{2\\pi}(1-\\rho)} e^{-w^2/2(1-\\rho)} \\mathrm{d}zw.\n\\end{equation*}\\]\nWykorzystując znane nam już tożsamości\n\\[\\begin{equation*}\n\\int_\\mathbb{R}\\frac{1}{\\sqrt{2\\pi}(1+\\rho)} e^{-z^2/2(1+\\rho)}\\mathrm{d}z =\\\\\n\\int_\\mathbb{R}\\frac{1}{\\sqrt{2\\pi}(1-\\rho)} e^{-w^2/2(1-\\rho)} \\mathrm{d}w=1\n\\end{equation*}\\]\noraz\n\\[\\begin{equation*}\n\\int_\\mathbb{R}\\frac{z^2}{\\sqrt{2\\pi}(1+\\rho)} e^{-z^2/2(1+\\rho)}\\mathrm{d}z = 1+\\rho\n\\end{equation*}\\]\\[\\begin{equation*}\n\\int_\\mathbb{R}\\frac{w^2}{\\sqrt{2\\pi}(1-\\rho)} e^{-w^2/2(1-\\rho)} \\mathrm{d}w=1-\\rho\n\\end{equation*}\\]\nOtrzymujemy\n\\[\\begin{equation*}\n\\mathrm{Cov}(X,Y) = \\frac{1+\\rho}{2} - \\frac{1-\\rho}{2} = \\rho.\n\\end{equation*}\\]Remark. Przy użyciu powyższych rachunków można pokazać, że zmienne losowe \\(Z\\) \\(W)\\) dane przez\n\\[\\begin{equation*}\n\\left( \\begin{array}{c} Z  \\\\ W \\end{array} \\right) = Q^T\n\\left( \\begin{array}{c} X  \\\\ Y \\end{array} \\right)\n\\end{equation*}\\]\nsą niezależne. Jeżeli wylosujemy \\(100\\) punktów z rozkładu normalnego z powyższego przykładu\n(dla \\(\\rho=2/3\\)) widzimy, że współrzędnych \\(x,y\\) punkty układają się wzdłuż prostej \\(x=y\\).\n\nwspółrzędnych \\(zw\\) natomiast punkty układają się wzdłuż osi \\(z\\).Remark. Jeżeli \\(X,Y\\) są niezależne, \\(\\mathrm{Cov}(X,Y)=0\\) (wynika z pkt. 1 powyższego twierdzenia),\nale nie jest prawdziwa odwrotna implikacja.\nKowariancja mierzy jak bardzo zmienne losowe \\(X\\), \\(Y\\) są zależne.Twierdzenie 14.2  Jeżeli \\(\\mathrm{E}[ X_i^2]<\\infty\\) dla \\(=1,\\ldots, n\\), \n\\[\n  \\mathbb{V}ar(X_1+\\ldots+ X_n) = \\sum_{k=1}^n \\mathbb{V}ar X_k \\\\+ 2 \\sum_{k<l}{\\rm  Cov}(X_k,X_l).\n  \\]\nW szczególności jeżeli \\(X_i\\) są wzajemnie nieskorelowane, \n\\[\n  \\mathbb{V}ar(X_1+\\ldots+ X_n) = \\sum_{k=1}^n \\mathbb{V}ar X_k .\n  \\]Proof. Teza wynika z bezpośredniego rachunku. Mamy\n\\[\\begin{align*}\n  &\\mathbb{V}ar(X_1+\\ldots+ X_n)    \\\\\n  = & \\mathbb{E}\\Big[  X_1+\\ldots + X_n - \\mathbb{E}\\big[X_1+\\ldots + X_n \\big]  \\Big]^2\\\\\n  = & \\mathbb{E}\\bigg[ \\sum_{j=1}^n (X_j -\\mathbb{E} X_j) \\bigg]^2\\\\\n  = &  \\sum_{j=1}^n\\mathbb{E}(X_j-\\mathbb{E} X_j)^2 \\\\\n  & + 2\\sum_{k<l} \\mathbb{E}\\big[(X_k-\\mathbb{E} X_k)(X_l-\\mathbb{E} X_l,)\\big]\\\\\n  = & \\sum_{k=1}^n \\mathbb{V}ar X_k + 2 \\sum_{k<l}{\\rm Cov}(X_k,X_l).\n  \\end{align*}\\]\n□Przykład 14.2  Niech \\(\\sigma\\) będzie losową permutacją liczb \\(1,\\ldots,n\\) niech\n\\(X\\) oznacza liczbę punktów stałych \\(\\sigma\\). Szukamy \\(\\mathbb{V}ar [X]\\).Niech \\[X_i = \\left\\{\\begin{array}{cc}\n                 1 & \\mbox{ jeżeli $$ jest punktem  stałym} \\\\\n                 0 & \\mbox{ jeżeli $$ nie jest punktem  stałym}\n               \\end{array}\n\\right.\n\\] Wtedy \\(X = \\sum_{=1}^n X_i\\). Wyliczamy\n\\[\\begin{align*}\n\\mathbb{E} [X_i] &= \\frac 1n,\\quad \\mathbb{E} [X] = 1,\\\\\n\\mathbb{E} [X_iX_j] &= \\mathbb{P}[X_i X_j = 1] = \\frac{1}{n(n-1)},\\quad \\mbox{dla }\\=j \\\\\n\\mathbb{V}ar [X_i] &= \\mathbb{E} \\left[X_i^2\\right] - (\\mathbb{E} [X_i])^2 = \\frac{n-1}{n^2}.\n\\end{align*}\\]\nWówczas z powyższego twierdzenia\n\\[\\begin{align*}\n\\mathbb{V}ar X = &  n\\cdot \\frac{n-1}{n^2} \\\\\n& +  2\\cdot \\frac{n(n-1)}2\\cdot\\bigg( \\frac 1{n(n-1)}  - \\frac 1{n^2} \\bigg)\n\\\\ =& \\frac{n-1}n + n\\bigg(1 - \\frac{n-1}{n}\\bigg) = 1.\n\\end{align*}\\]","code":""},{"path":"regresja-liniowa.html","id":"regresja-liniowa","chapter":"15 Regresja liniowa","heading":"15 Regresja liniowa","text":"Dane są dwie zmienne losowe \\(X\\) \\(Y\\).\nZałóżmy, że są one skorelowane. Możemy myśleć, że \\(Y\\) zależy od \\(X\\)\n(np. \\((X,Y)\\) (wzrost, waga), (liczba klientów, dochód), (przebieg samochodu, zużycie paliwa),\n(indeks Dow Jones, indeks Wig 20)).\nChcemy na podstawie obserwacji \\(X(\\omega_0)\\) wyznaczyć \\(Y(\\omega_0)\\).\nW tym celu szukamy funkcji \\(f\\) takiej, że \\(Y = f(X)\\).Regresja liniowa polega na tym, że szuka się funkcji liniowej.\nInnymi słowy pracujemy przy założeniu, że zależność \\(Y\\) od \\(X\\) jest liniowa.Przykład 15.1  Niech \\(X(\\omega)\\) \\(Y(\\omega)\\) będzie budżetem filmu \\(\\omega\\). Za zbiór zdarzeń elementarnych\nprzyjmujemy \\(\\Omega\\) będące zbiorem filmów (bazą internetową) zawierającą informacje o budżecie.\nDla potrzeby tego przykładu posłużymy się bazą TMDB.\nPoniżej przedstawiamy wstępną analizę naszych zmiennych wykonaną w języku R.Na pierwszy rzut oka widzimy, że dane układają się symetrycznie wzdłuż osi budżetu. Sugeruje \nniezależność bądź bardzo niską korelację.Przykład 15.2  Rozważmy podobny przykład, lecz tym razem badać będziemy \\(X\\) będące rokiem produkcji \\(Y\\) będące\nśrednią ocen na IMBD. Za \\(\\Omega\\) obieramy bazę\n1000 najwyżej ocenianych filmów z IMDB. Należy mieć na względzie, że\ntutaj oceny liczone są z dokładnością jednego miejsca po przecinku.W tym przypadku widzimy tendencję spadkową.Zanim dokończymy analizę przedstawionych wyżej przypadków omówimy problem ogólny.\nBędziemy chcieli wyznaczyć liniową zależność między \\(X\\) \\(Y\\). Pytamy dla jakich\nwartości \\(,b \\\\mathbb{R}\\) zmienna\n\\[\\begin{equation*}\n\\hat{Y} = aX+b\n\\end{equation*}\\]\nnajlepiej przybliża \\(Y\\)?\nBłąd naszego przybliżenia będziemy liczyli w sensie średniokwadratowym\nChcemy zminimalizować wartość\n\\[\ng(,b) = \\mathbb{E} \\left[ \\left( Y - \\hat{Y} \\right)^2 \\right].\n\\]\nW tym celu liczymy pochodne cząstkowe szukamy punktu,\nw którym zeruje się gradient\n\\[\\begin{align*}\n   \\frac{\\partial g}{\\partial } & = 2a\\mathbb{E}\\left[ X^2 \\right] -\n   2\\mathbb{E} [XY] + 2 b \\mathbb{E}[X] = 0,\\\\\n   \\frac{\\partial g}{\\partial b} & =  2b - 2\\mathbb{E}[ Y] + 2a \\mathbb{E}[ X]  = 0.\n\\end{align*}\\]\nRozwiązując powyższy układ równań otrzymujemy\n\\[\n= \\frac{{\\rm Cov}(X,Y)}{\\mathbb{V}ar[ X] }, \\qquad\nb= -\\frac{{\\rm Cov}(X,Y)}{\\mathbb{V}ar[ X]} \\mathbb{E}[ X] + \\mathbb{E}[ Y].\n\\]\nOznacza , że szukana przez nas funkcja \\(f\\) ma postać\n\\[\nf(x) = y = \\frac{{\\rm Cov}(X,Y)}{\\mathbb{V}ar[ X]}(x-\\mathbb{E}[ X]) + \\mathbb{E}[Y].\n\\]\nCzyli\n\\[\n\\hat{Y}(\\omega) =\n\\frac{{\\rm Cov}(X,Y)}{\\mathbb{V}ar[ X]}(X(\\omega)-\\mathbb{E}[ X]) + \\mathbb{E}[Y].\n\\]\nZauważmy, że wyznaczenia prostej regresji nie trzeba znać całego rozkładu\nłącznego \\((X,Y)\\), ale wystarczy \\(\\mathbb{E}[ X]\\), \\(\\mathbb{E}[ Y]\\), \\(\\mathbb{V}ar[ X]\\),\nCov\\((X,Y)\\). Zauważmy też, że jeżeli \\(X\\) \\(Y\\) są dodatnio skorelowane, czyli \\(\\mathrm{Cov}(X,Y)>0\\),\n“rosnące wartości \\(X\\) oznaczają rosnące wartości \\(Y\\)’’Przykład 15.3  Wróćmy przykładu oceny filmu jego budżetu. Mając\nnasze dane możemy obliczyć\nśrednie, kowariancję oraz odpowiednią wariancję. Zakładamy, że wylosowanie każdego filmu\njest jednakowo prawdopodobne.Ocena = 6.210116 + 4.41581e-10 * budżetWidzimy, że w tym wypadku współczynnik korelacji jest bardzo mały.\nPrzy tego typu wynikach nie ma statystycznej istotności:\nnie możemy stwierdzić, że budżet cokolwiek tłumaczy.Przykład 15.4  Na koniec wróćmy przykładu oceny filmu roku produkcji.Ocena = 11.03454 + -0.001549247 * rokW tym przypadku widzimy, że wpływ roku jest mały (ale istotnie większy niż w przypadku budżetu!).Przykład 15.5  Rozważmy ostatni przykład. Jak budżet filmu ma się jego dochodów?Dochody = -5579407 + 2.956959 * budżetWidzimy, że w tym wypadku współczynnik korelacji jest duży.","code":"\n# wczytujemy i czyścimy dane\nmovies <- read.csv(\"R/tmdb_5000_movies.csv\")\nmovies_clean <- subset(movies, budget > 0 & !is.na(vote_average))\n\n# Ustawiamy tło i kolory na wykresie\npar(bg = \"#002b36\", col.axis = \"#eee8d5\", col.lab = \"#eee8d5\", \n    col.main = \"#eee8d5\", fg = \"#eee8d5\")\n\n# Zaznaczamy punkty\nplot(movies_clean$budget, movies_clean$vote_average,\n     main = \"Ocena filmu vs budżet\",\n     xlab = \"Budżet (USD)\",\n     ylab = \"Średnia ocena\",\n     pch = 20,\n     col = \"#eee8d5\")\nimdb <- read.csv(\"R/imdb_top_1000.csv\")\nimdb_clean <- subset(imdb, grepl(\"^[0-9]{4}$\", \n                 Released_Year) & !is.na(IMDB_Rating))\n\n\npar(bg = \"#002b36\", col.axis = \"#eee8d5\", col.lab = \"#eee8d5\", \n    col.main = \"#eee8d5\", fg = \"#eee8d5\")\n\n# Punkty\nplot(imdb_clean$Released_Year, imdb_clean$IMDB_Rating,\n     main = \"Ocena IMDb względem roku produkcji\",\n     xlab = \"Rok produkcji\",\n     ylab = \"Ocena IMDb\",\n     pch = 20,\n     col = \"#eee8d5\")\n# wczytujemy i czyścimy dane\nmovies <- read.csv(\"R/tmdb_5000_movies.csv\")\nmovies_clean <- subset(movies, budget > 0 & !is.na(vote_average))\n\n# liczymy wartości oczekiwane\nEx <- mean(movies_clean$budget)\nEy <- mean(movies_clean$vote_average)\n\n# Kowariancja\nCovXY <- sum( (movies_clean$budget - Ex) * \n         (movies_clean$vote_average - Ey))\n\n# wariancja X -budżetu\nVarX <- sum( (movies_clean$budget - Ex)^2 )\n\n# liczymy współczynniki\na <- CovXY / VarX\nb <- Ey - a * Ex\n\ncat('<span style=\"color: #eee8d5;\">Ocena = ', \n    b, ' + ', a, ' * budżet<\/span><br>')\n# Ustawiamy tło i kolory na wykresie\npar(bg = \"#002b36\", col.axis = \"#eee8d5\", col.lab = \"#eee8d5\", \n    col.main = \"#eee8d5\", fg = \"#eee8d5\")\n\n# Zaznaczamy punkty\nplot(movies_clean$budget, movies_clean$vote_average,\n     main = \"Ocena filmu vs budżet\",\n     xlab = \"Budżet (USD)\",\n     ylab = \"Średnia ocena\",\n     pch = 20,\n     col = \"#eee8d5\")\n\n# Prosta regresji\n\n# Zakres budżetu (x)\nx_vals <- range(movies_clean$budget)\n# Odpowiednie wartości y\ny_vals <- b + a * x_vals\n\nlines(x_vals, y_vals, col = \"#2aa198\", lwd = 2)\nimdb <- read.csv(\"R/imdb_top_1000.csv\")\nimdb_clean <- subset(imdb, grepl(\"^[0-9]{4}$\", \n                 Released_Year) & !is.na(IMDB_Rating))\nimdb_clean$Released_Year <- as.numeric(imdb_clean$Released_Year)\n\n# liczymy wartości oczekiwane\nEx <- mean(imdb_clean$Released_Year)\nEy <- mean(imdb_clean$IMDB_Rating)\n\n# Kowariancja\nCovXY <- sum( (imdb_clean$Released_Year - Ex) * \n         (imdb_clean$IMDB_Rating - Ey))\n\n# wariancja X -budżetu\nVarX <- sum( (imdb_clean$Released_Year - Ex)^2 )\n\n# liczymy współczynniki\na <- CovXY / VarX\nb <- Ey - a * Ex\n\ncat('<span style=\"color: #eee8d5;\">Ocena = ', \n    b, ' + ', a, ' * rok<\/span><br>')\npar(bg = \"#002b36\", col.axis = \"#eee8d5\", col.lab = \"#eee8d5\", \n    col.main = \"#eee8d5\", fg = \"#eee8d5\")\n\n# Punkty\nplot(imdb_clean$Released_Year, imdb_clean$IMDB_Rating,\n     main = \"Ocena IMDb względem roku produkcji\",\n     xlab = \"Rok produkcji\",\n     ylab = \"Ocena IMDb\",\n     pch = 20,\n     col = \"#eee8d5\")\n\n# Prosta regresji\n\n# Zakres budżetu (x)\nx_vals <- range(imdb_clean$Released_Year)\n# Odpowiednie wartości y\ny_vals <- b + a * x_vals\n\nlines(x_vals, y_vals, col = \"#2aa198\", lwd = 2)\n# wczytujemy i czyścimy dane\nmovies <- read.csv(\"R/tmdb_5000_movies.csv\")\nmovies_clean <- subset(movies, budget > 0 & !is.na(vote_average))\n\n# liczymy wartości oczekiwane\nEx <- mean(movies_clean$budget)\nEy <- mean(movies_clean$revenue)\n\n# Kowariancja\nCovXY <- sum( (movies_clean$budget - Ex) * \n         (movies_clean$revenue - Ey))\n\n# wariancja X -budżetu\nVarX <- sum( (movies_clean$budget - Ex)^2 )\n\n# liczymy współczynniki\na <- CovXY / VarX\nb <- Ey - a * Ex\n\ncat('<span style=\"color: #eee8d5;\">Dochody = ', \n    b, ' + ', a, ' * budżet<\/span><br>')\n# Ustawiamy tło i kolory na wykresie\npar(bg = \"#002b36\", col.axis = \"#eee8d5\", col.lab = \"#eee8d5\", \n    col.main = \"#eee8d5\", fg = \"#eee8d5\")\n\n# Zaznaczamy punkty\nplot(movies_clean$budget, movies_clean$revenue,\n     main = \"Dochody filmu vs budżet\",\n     xlab = \"Budżet (USD)\",\n     ylab = \"Dochody (USD)\",\n     pch = 20,\n     col = \"#eee8d5\")\n\n# Prosta regresji\n# Zakres budżetu (x)\nx_vals <- range(movies_clean$budget)\n# Odpowiednie wartości y\ny_vals <- b + a * x_vals\n\nlines(x_vals, y_vals, col = \"#2aa198\", lwd = 2)"},{"path":"parametry-wielowymiarowe.html","id":"parametry-wielowymiarowe","chapter":"16 Parametry wielowymiarowe","heading":"16 Parametry wielowymiarowe","text":"Omówimy pokrótce wielowymiarowe parametry wektorów losowych.\nOd tej pory przyjmujemy konwencję, że wszystkie rozważane wektory są pionowe.","code":""},{"path":"parametry-wielowymiarowe.html","id":"wektor-średnich","chapter":"16 Parametry wielowymiarowe","heading":"16.1 Wektor średnich","text":"Definicja 16.1  Niech \\(\\vec{X}=(X_1, \\ldots , X_d)^T\\) będzie \\(d\\)-wymiarowym wektorem losowym. Powiemy, że\n\\(\\vec{X}\\) ma wartość oczekiwaną\njeżeli wszystkie zmienne losowe \\(X_1, \\ldots , X_d\\) mają\nwartości oczekiwane. Wówczas wektor\n\\[\n  \\mathbb{E}\\left[ \\vec{X} \\right] = (\\mathbb{E} [X_1],\\ldots,\\mathbb{E} [X_d])\n  \\]\nnazywamy wartością oczekiwaną zmiennej losowej \\(\\vec{X}\\).Przykład 16.1  Niech \\(\\vec{Y}=(Y_1, \\ldots, Y_d)^T\\)\nbędzie \\(d\\)-wymiarowym wektorem losowym z rozkładem \\(\\mathcal{N}(\\vec{m}, \\Sigma)\\),\ngdzie \\(\\vec{m} = (m_1, m_2, \\ldots, m_d)^T\\).\nPrzypomnijmy, że oznacza , że ma gęstość zadaną przez\n\\[\\begin{equation*}\nf_{\\vec{Y}}\\left(\\vec{y} \\right) = \\frac{1}{(2\\pi)^{d/2} \\mathrm{det}(\\Sigma)^{1/2}}\n\\exp \\left\\{ - \\langle \\Sigma^{-1}(\\vec{y}-\\vec{m}), \\vec{y}-\\vec{m} \\rangle/2 \\right\\}\n\\end{equation*}\\]\nAby wyznaczyć wektor \\(\\mathbb{E}[\\vec{Y}]\\) ustalmy \\(j\\[d] = \\{1,2, \\ldots, d\\}\\) napiszmy\nstosując podstawienie \\(\\vec{y}=\\vec{z}+\\vec{m}\\), że\n\\[\\begin{align*}\n    \\mathbb{E}[Y_j] & = \\int_{\\mathbb{R}^d} y_j f_{\\vec{Y}}\\left(\\vec{y} \\right) \\mathrm{d}\\vec{y} \\\\\n        & = \\int_{\\mathbb{R}^d} (z_j +m_j) f_{\\vec{Y}}\\left(\\vec{z} +\\vec{m} \\right) \\mathrm{d}\\vec{z}\n\\end{align*}\\]\nZauważmy, że\n\\[\\begin{equation*}\nf_{\\vec{Y}}\\left(\\vec{z}+\\vec{m} \\right) = \\frac{1}{(2\\pi)^{d/2} \\mathrm{det}(\\Sigma)^{1/2}}\n\\exp \\left\\{ - \\langle \\Sigma^{-1}\\vec{z}, \\vec{z}\\rangle/2 \\right\\}\n\\end{equation*}\\]\njest symetryczna względem zera (\\(f_{vec{Y}} (\\vec{z}+\\vec{m}) = f_{vec{Y}}(-\\vec{z}+\\vec{m})\\)) gęstością rozkładu\n\\(\\mathcal{N}(\\vec{0}, \\Sigma)\\). W szczególności\n\\[\\begin{equation*}\n    \\int_{\\mathbb{R}^d} f_{\\vec{Y}}\\left(\\vec{z} +\\vec{m} \\right) \\mathrm{d}\\vec{z}=1\n\\end{equation*}\\]\noraz\n\\[\\begin{equation*}\n    \\int_{\\mathbb{R}^d} z_jf_{\\vec{Y}}\\left(\\vec{z} +\\vec{m} \\right) \\mathrm{d}\\vec{z}=0.\n\\end{equation*}\\]\nOstatecznie stąd \\(\\mathbb{E}[Y_j]=m_j\\) co za tym idzie\n\\[\\begin{equation*}\n    \\mathbb{E}\\left[\\vec{Y} \\right] = \\vec{m}.\n\\end{equation*}\\]Twierdzenie 16.1  Niech \\(\\vec{X}=(X_1, \\ldots , X_d)^T\\) będzie \\(d\\)-wymiarowym wektorem losowym. Wówczas \\(\\vec{X}\\) ma wartość oczekiwaną\nwtedy tylko wtedy, gdy zmienna losowa\n\\[\\begin{equation*}\n    \\left\\|\\vec{X} \\right\\| = \\sqrt{\\sum_{j=1}^d X_j^2}\n  \\end{equation*}\\]\nma wartość oczekiwaną.\nWówczas\n\\[\\begin{equation}\n     \\left\\|\\mathbb{E}\\left[\\vec{X}\\right]\\right\\|\\le \\mathbb{E}\\left[\\left\\|\\vec{X}\\right\\|\\right].\n  \\tag{16.1}\n  \\end{equation}\\]\nJeżeli \\(= (A_{,j})_{\\leq m, j\\leq d}\\) jest macierzą \\(m \\times d\\), \\(\\vec{Y}\\) jest \\(m\\)-wymiarowym\nwektorem losowym o średniej\n\\[\\begin{equation*}\n    \\mathbb{E}\\left[\\vec{Y} \\right] = \\mathbb{E}\\left[\\vec{Y}\\right].\n  \\end{equation*}\\]\nJeżeli \\(\\vec{Y}\\) jest wektorem losowy posiadającym wartość oczekiwaną, \n\\[\\begin{equation*}\n\\mathbb{E}\\left[\\vec{X}+b\\vec{Y}\\right] = \\mathbb{E}\\left[\\vec{X}\\right] + b \\mathbb{E}\\left[\\vec{Y}\\right]\n\\end{equation*}\\]\ndla dowolnych rzeczywistych \\(\\) \\(b\\).Proof. Pierwszy postulat wynika z nierówności\n\\[\\begin{equation*}\n        |X_j| \\le \\left\\|\\vec{X}\\right\\| \\le \\sum_{=1}^d|X_i|.\n  \\end{equation*}\\]\nDruga nierówność jest konsekwencją podaddytywności pierwiastka: \\(\\sqrt{x+y} \\leq \\sqrt{x}+\\sqrt{y}\\) dla dowolnych\nnieujemnych \\(x\\) \\(y\\).\nAby uzasadnić (16.1) rozważmy dowolny wektor długości jeden \\(\\vec{v}=(v_1,\\ldots,v_d)^T\\).\nMamy\n\\[\n  \\langle \\mathbb{E}\\left[ \\vec{X}\\right],\\vec{v} \\rangle = \\sum_{j=1}^d \\mathbb{E} [X_j] \\cdot v_j\n   = \\mathbb{E}\\left[ \\langle \\vec{X},\\vec{v} \\rangle\\right] \\le\n   \\mathbb{E}\\left[ \\left\\|\\vec{X}\\right\\|\\left\\|\\vec{v}\\right\\|\\right] =\n   \\mathbb{E}\\left[\\left\\|\\vec{X}\\right\\|\\right].  \\]\nPrzyjmując \\(v = \\mathbb{E}\\vec{X}/ |\\vec{X}|\\) otrzymujemy (16.1).\nNiech teraz \\(\\) będzie dowolną macierzą \\(m\\times d\\). Przypomnijmy, że wówczas \\(j\\)-ta współrzędna\nwektora \\(\\vec{Y}\\) jest równa\n\\[\\begin{equation*}\n    \\left(\\vec{Y} \\right)_j = \\sum_{k=1}^dA_{,k}Y_k.\n  \\end{equation*}\\]\nMamy zatem\n\\[\\begin{equation*}\n    \\mathbb{E} \\left[\\left(\\vec{Y} \\right)_j\\right]\n    =\\mathbb{E} \\left[\\sum_{k=1}^d A_{j,k}Y_k\\right]\n    =\\sum_{k=1}^d A_{j,k}\\mathbb{E} \\left[Y_k\\right]\n    = \\left(\\mathbb{E}\\left[\\vec{Y} \\right]\\right)_j.\n  \\end{equation*}\\]\nOstatnia własność wynika wprost w liniowości wartości oczekiwanej zmiennych losowych.\n□","code":""},{"path":"parametry-wielowymiarowe.html","id":"macierz-kowariancji","chapter":"16 Parametry wielowymiarowe","heading":"16.2 Macierz kowariancji","text":"Definicja 16.2  Powiemy, że wektor losowy \\(\\vec{X}=(X_1, \\ldots, X_d)\\) jest całkowalny z kwadratem jeżeli\nwszystkie zmienne \\(X_1, \\ldots , X_d\\) są całkowalne z kwadratem.Rozumując analogicznie jak w ostatnim twierdzeniu łatwo pokazać, że wektor\n\\(\\vec{X}\\) jest całkowalny z kwadratem wtedy tylko wtedy, gdy zmienna losowa \\(\\|\\vec{X}\\|\\) jest\ncałkowalna z kwadratem.Definicja 16.3  Niech \\(\\vec{X}=(X_1,\\ldots,X_n)\\) będzie \\(n\\)-wymiarowym wektorem losowym całkowalnym z kwadratem.\nMacierz \\(Q^{\\vec{X}} = \\left( Q^{\\vec{X}}_{,j} \\right)_{,j\\leq n}\\) daną przez\n\\[\\begin{equation*}\n    Q^{\\vec{X}}_{,j} = \\mathrm{Cov}(X_i, X_j)\n\\end{equation*}\\]\nnazywamy macierzą kowariancji wektora \\(\\vec{X}\\).Macierz kowariancji jest wielowymiarowym uogólnieniem wariancji.\nMamy\n\\[\n  Q^{\\vec{X}} = \\left[\n  \\begin{array}{cccc}\n    {\\rm Cov}(X_1,X_1) & {\\rm Cov}(X_1,X_2) & \\ldots & {\\rm Cov}(X_1,X_n) \\\\\n    {\\rm Cov}(X_2,X_1) & \\cdots  &  &  \\\\\n    \\vdots &  & \\ddots &  \\\\\n    {\\rm Cov}(X_n,X_1) & \\ldots  &  & {\\rm Cov}(X_n,X_n)\n  \\end{array}\n  \\right]\n  \\]Jeżeli zmienne losowe \\(X_i\\) są nieskorelowane, \\(Q\\) jest macierzą diagonalną.Twierdzenie 16.2  Macierz kowariancji \\(Q^{\\vec{X}}\\) wektora losowego \\(\\vec{X}\\) jest symetryczna oraz\nnieujemnie określona (tzn. dla każdych \\(t_1,\\ldots, t_n\\), \\(\\sum t_it_j {Q^{\\vec{X}}_{ij}}\\ge 0\\)).\nDodatkowo, jeżeli \\(\\) jest macierzą \\(m \\times n\\), macierz kowariancji\nwektora losowego \\(\\vec{Y}=\\vec{X}\\) jest równa\n\\[\\begin{equation*}\n    Q^{\\vec{Y}} = Q^{\\vec{X}} = Q^{\\vec{X}} ^T.\n\\end{equation*}\\]\nWreszcie, jeżeli \\(\\vec{Z}=\\vec{X}+\\vec{}\\), dla ustalonego wektora \\(\\vec{} \\\\mathbb{R}^d\\), \n\\[\\begin{equation*}\nQ^{\\vec{Z}} = Q^{\\vec{X}+\\vec{}} = Q^{\\vec{X}}\n\\end{equation*}\\]Proof. Macierz jest symetryczna, bo \\({\\rm Cov}(X_i,X_j) = {\\rm Cov}(X_j,X_i)\\). dowodu drugiej części twierdzenia weźmy\ndowolny ciąg \\(t_1,\\ldots,t_n\\) zdefiniujmy \\(Y = \\sum_{j=1}^n t_j X_j\\). Wtedy\n\\[\\begin{multline*}\n  0\\le \\mathbb{V}ar [Y] = \\mathbb{E} \\left[ \\left(   \\sum_{j=1}^n t_j(X_j - \\mathbb{E} [X_j]) \\right)^2    \\right]\\\\\n  = \\sum_{,j=1}^n \\mathbb{E} \\big[ t_i(X_i - \\mathbb{E} X_i) t_j (X_j - \\mathbb{E}[ X_j])  \\big]\n  = \\sum_{,j=1}^n t_i t_j {\\rm Cov} (X_i, X_j).\n  \\end{multline*}\\]\nAby uzasadnić ostatni wzór zauważmy, że operację wartości oczekiwanej możemy w naturalny sposób\nrozszerzyć macierzy losowych.\nZauważmy też, że mnożąc przez siebie wektor pionowy długości \\(n\\) wektor poziomy długości \\(n\\)\notrzymujemy macierz \\(n\\times n\\). Dokładniej\n\\[\\begin{equation*}\n    \\left(\\vec{X} - \\mathbb{E}\\left[\\vec{X}\\right] \\right)\n    \\left(\\vec{X} - \\mathbb{E}\\left[\\vec{X}\\right] \\right)^T= \\\\\n  \\left[\n  \\begin{array}{cccc}\n    (X_1 - \\mathbb{E}[X_1]) (X_1-\\mathbb{E}[X_1]) & \\ldots & \\ldots\n& (X_1-\\mathbb{E}[X_1])(X_n-\\mathbb{E}[X_n]) \\\\\n    (X_2-\\mathbb{E}[X_2])(X_1-\\mathbb{E}[X_1]) & \\cdots  &  &  \\\\\n    \\vdots &  & \\ddots &  \\\\\n    (X_n-\\mathbb{E}[X_n])(X_1-\\mathbb{E}[X_1]) & \\ldots  &  & (X_n-\\mathbb{E}[X_n])(X_n-\\mathbb{E}[X_n])\n  \\end{array}\n  \\right]\n\\end{equation*}\\]\nCzyli\n\\[\\begin{equation*}\n    Q^{\\vec{X}} = \\mathbb{E} \\left[\\left(\\vec{X} - \\mathbb{E}\\left[\\vec{X}\\right] \\right)\n    \\left(\\vec{X} - \\mathbb{E}\\left[\\vec{X}\\right] \\right)^T\\right]\n\\end{equation*}\\]\nMamy zatem\\[\\begin{align*}\n    Q^{\\vec{X}} = &\n    \\mathbb{E} \\left[\\left(\\vec{X} - \\mathbb{E}\\left[\\vec{X}\\right] \\right)\n    \\left(\\vec{X} - \\mathbb{E}\\left[\\vec{X}\\right] \\right)^T\\right] \\\\\n    = & \\mathbb{E} \\left[\\left(\\vec{X} - \\mathbb{E}\\left[\\vec{X}\\right] \\right)\n    \\left(\\vec{X} - \\mathbb{E}\\left[\\vec{X}\\right] \\right)^T\\right]^T \\\\ = & Q^{\\vec{X}}^T.\n\\end{align*}\\]\nOstatnia własność wynika z niezmienniczości kowariancji na przesunięcia, tj.\n\\[\\begin{equation*}\n    \\mathrm{Cov}(Z_i, Z_j) =\n    \\mathrm{Cov}(X_i+a_i, X_j+a_j) =\n    \\mathrm{Cov}(X_i, X_j).\n\\end{equation*}\\]\n□Przykład 16.2  Załóżmy, że \\(n\\)-wymiarowy wektor \\(\\vec{X}=(X_1, X_2, \\ldots, X_n)\\)\nma rozkład \\(\\mathcal{N}(\\vec{0}, \\mathrm{Id})\\).\nWówczas\n\\[\\begin{equation*}\nf_{\\vec{X}}(\\vec{x}) = \\frac{1}{(2\\pi)^{n/2}} e^{-(x_1^2+\\ldots + x_n^2)/2} = \\prod_{j=1}^n \\frac{1}{\\sqrt{2\\pi}} e^{-x_j^2/2}.\n\\end{equation*}\\]\nSkoro każdy składnik produktu gęstość standardowego rozkładu normalnego, \\(X_1, \\ldots, X_n\\) są niezależne\nze standardowym rozkładem normalnym. W szczególności zmienne te są nieskorelowane o wariancji jeden, czyli\n\\[\\begin{equation*}\n    Q^{\\vec{X}} = \\mathrm{Id}.\n\\end{equation*}\\]\nNiech teraz \\(\\) będzie odwracalną macierzą, \\(\\vec{m}\\) ustalonym wektorem.\nRozważmy \\(\\vec{Y} = \\vec{X}+\\vec{m}\\). Wiemy, że wówczas \\(\\vec{Y}\\) ma rozkład \\(\\mathcal{N}(\\vec{m}, \\Sigma)\\), gdzie\n\\(\\Sigma=AA^T\\).\nMamy\n\\[\\begin{equation*}\n    Q^{\\vec{Y}} = Q^{\\vec{X}+\\vec{m}} = Q^{\\vec{X}} = Q^{\\vec{X}}^T = AA^T=\\Sigma.\n\\end{equation*}\\]","code":""},{"path":"nierówności.html","id":"nierówności","chapter":"17 Nierówności","heading":"17 Nierówności","text":"Przypomnijmy nierówność, która pojawiła się podczas wykładu z Miary Całki:Twierdzenie 17.1  (Nierówność H\"oldera) Jeżeli \\(\\mathbb{E}\\left[ |X|^p \\right] <\\infty\\) \\(\\mathbb{E}\\left[ |Y|^q\\right] <\\infty\\)\ndla \\(p,q>1\\) takich, że \\(1/p+1/q=1\\) , \n\\[\n\\mathbb{E} \\left[ |XY| \\right] \\le \\left( \\mathbb{E}\\left[ |X|^p \\right]\\right)^{1/p}\n\\left( \\mathbb{E}\\left[ |Y|^q\\right]\\right)^{1/q}.\n\\]","code":""},{"path":"nierówności.html","id":"nierówność-czebyszewa","chapter":"17 Nierówności","heading":"17.1 Nierówność Czebyszewa","text":"W rachunku prawdopodobieństwa częściej używana jest nierówność Czebyszewa, która w najogólniejszej\nwersji mówiTwierdzenie 17.2  (Nierówność Czebyszewa) Niech \\(X\\) będzie zmienną losową niech \\(f:[0,\\infty) \\mapsto [0,\\infty)\\) będzie niemalejącą funkcją taką, że\n\\(f(x)>0\\) dla każdego \\(x>0\\). Wtedy dla każdego \\(\\lambda > 0\\)\n\\[\n   \\mathbb{P}[|X|\\ge \\lambda] \\le \\frac{\\mathbb{E} [f(|X|)]}{f(\\lambda)}.\n   \\]Proof. Piszemy\n\\[\n\\mathbb{P}[|X|\\ge \\lambda] = \\mathbb{E}\\left[{\\bf 1}_{\\{|X|\\ge \\lambda\\}} \\right]\n\\le \\frac 1{f(\\lambda)} \\mathbb{E}\\left[ f(|X|){\\bf 1}_{\\{|X|>\\lambda\\}}  \\right]\n\\le \\frac{\\mathbb{E}[ f(|X|)]}{f(\\lambda)}.\n\\]\n□Zazwyczaj używa się powyższej nierówności w szczególnych przypadkach. Jeżeli zmienna losowa \\(X\\) jest nieujemna, \\(f(x)=x\\),\nnierówność przyjmuje klasyczną postać.Wniosek 17.1  Jeżeli \\(X\\geq0\\), dla każdej \\(\\lambda >0\\) mamy\n\\[\\begin{equation}\n    \\mathbb{P}[X \\geq \\lambda] \\leq \\frac{\\mathbb{E}[X]}{\\lambda}.\n    \\tag{17.1}\n\\end{equation}\\]Jeżeli natomiast zastąpimy \\(X\\) przez \\(X -\\mathbb{E}[X]\\) przyjmiemy \\(f(x)=x^2\\), w nierówności Czebyszewa\npojawia się wariancja \\(X\\).Wniosek 17.2  Jeżeli \\(X\\) jest całkowalna z kwadratem, dla każdej \\(\\lambda>0\\) mamy\n\\[\\begin{equation}\n      \\mathbb{P}\\left[ |X-\\mathbb{E} [X]|\\ge \\lambda  \\right] \\le \\frac{\\mathbb{V}ar[X]}{\\lambda^2}.\n      \\tag{17.2}\n\\end{equation}\\]Zobaczy teraz jak te dwie nierówności działają w praktyce.Przykład 17.1  Niech \\(n\\\\mathbb{N}\\). Rozważmy \\(n\\) rzutów symetryczną monetą. Niech \\(X\\) będzie liczbą otrzymanych orłów.\nChcemy oszacować prawdopodobieństwo, że \\(X\\) wynosi co najmniej \\(3n/4\\). Aby zastosować nierówność (17.1)\nprzypomnijmy, że \\(\\mathbb{E}[X]=n/2\\). Rzeczywiście, \\(X = \\sum_{j=1}^n X_j\\), gdzie \\(X_j\\\\{0,1\\}\\) \n\\(X_j=1\\) wtedy tylko wtedy,\ngdy w \\(j\\)-tym rzucie wypadł orzeł. Wówczas\n\\[\\begin{equation*}\n\\mathbb{P}[X_j=1] = \\mathbb{P}[X_j=1]=1/2.\n\\end{equation*}\\]\nStąd \\(\\mathbb{E}[X_j]=1/2\\) co za tym idzie \\(\\mathbb{E}[X]=n/2\\). Stosując nierówność (17.1) otrzymujemy\n\\[\\begin{equation*}\n    \\mathbb{P}[X> 3n/4] \\leq \\frac{4}{3n}\\mathbb{E}[X] = \\frac 23.\n\\end{equation*}\\]\nAby porównać z nierównością (17.2) przypomnijmy jeszcze, że \\(\\mathbb{V}ar[X]=n/4\\). Skoro\n\\[\\begin{equation*}\n    \\mathbb{V}ar[X_j]=\\mathbb{E}\\left[X_j^2\\right] - \\mathbb{E}[X_j]^2 = \\frac 12 -\\frac 14 = \\frac 14.\n\\end{equation*}\\]\nSkoro zmienne \\(\\{X_j\\}_j\\) są niezależne, \n\\[\\begin{equation*}\n    \\mathbb{V}ar[X] = \\sum_{j=1}^n \\mathbb{V}ar[X_j] = n/4.\n\\end{equation*}\\]\nWracając nierówności (17.2) otrzymujemy\n\\[\\begin{align*}\n    \\mathbb{P}[X>3n/4] & = \\mathbb{P}[X - \\mathbb{E}[X]> n/4] \\\\\n    & \\leq \\mathbb{P}[|X - \\mathbb{E}[X]|> n/4] \\leq \\frac{4}{n}.\n\\end{align*}\\]","code":""},{"path":"nierówności.html","id":"nierówność-chernoffa","chapter":"17 Nierówności","heading":"17.2 Nierówność Chernoffa","text":"Przy analizie ostatniego przykładu pojawia się naturalne pytanie o , czy można otrzymać lepsze szacowania.Definicja 17.1  Funkcją generująca momenty zmiennej losowej \\(X\\) nazywamy \\(M_X \\colon \\mathbb{R} \\(0, +\\infty]\\) zadaną przez\n\\[\\begin{equation*}\n    M_X(\\beta) = \\mathbb{E} \\left[ e^{\\beta X} \\right].\n\\end{equation*}\\]Zanim przejdziemy zastosowań \\(M_X\\) wyjaśnimy jej nazwę. Dla \\(n\\\\mathbb{N}\\), \\(n\\)-tym momentem zmiennej losowej \\(X\\)\nnazywamy \\(\\mathbb{E}\\left[X^n\\right]\\). Załóżmy, że dla pewnej dodatniej \\(\\beta_0\\), \\(M_X(\\beta_0)\\),\n\\(M_X(-\\beta_0)<\\infty\\).\nWówczas\n\\[\\begin{equation*}\n\\mathbb{E}\\left[e^{\\beta_0|X|} \\right] \\leq \\mathbb{E}\\left[e^{\\beta_0X} + e^{-\\beta_0X} \\right]<\\infty.\n\\end{equation*}\\]\nDla \\(h<\\beta_0/2\\) mamy\n\\[\\begin{equation*}\n    \\left| \\frac{e^{hX}-1}{h} \\right| \\leq |X| e^{hX} \\leq C e^{2h|X|} \\leq C e^{\\beta_0|X|}.\n\\end{equation*}\\]\nSkoro\n\\[\\begin{equation*}\n    \\frac{M_X(h) - 1}{h} =\n    \\mathbb{E}\\left[ \\frac{e^{hX}-1}{h} \\right]\n\\end{equation*}\\]\nkorzystając z twierdzenia o zbieżności ograniczonej\n\\[\\begin{equation*}\n    M_X'(0) = \\mathbb{E}[X].\n\\end{equation*}\\]\nRozumując analogicznie pokazujemy, że jeżeli \\(M_X(\\beta_0)\\), \\(M_X(-\\beta_0)<\\infty\\) dla każdego\n\\(n \\\\mathbb{N}\\),\n\\[\\begin{equation*}\n    M^{(n)}_X(0) = \\mathbb{E}\\left[X^n \\right].\n\\end{equation*}\\]\nStąd, dla \\(|\\beta|<\\beta_0\\),\n\\[\\begin{equation*}\n    M_X(\\beta) = \\sum_{j=0}^\\infty \\frac{\\beta^n}{j!}\\mathbb{E}\\left[X^n \\right].\n\\end{equation*}\\]Przykład 17.2  Rozważmy \\(X\\) o rozkładzie \\(\\mathrm{Exp}(\\alpha)\\) dla \\(\\alpha>0\\).\nDla \\(\\beta<\\alpha\\),\n\\[\\begin{equation*}\nM_X(\\beta) = \\int_0^\\infty \\alpha e^{-\\alpha x} e^{\\beta x} \\mathrm{d}x = \\frac{\\alpha}{\\alpha-\\beta}\n\\end{equation*}\\]\noraz \\(M_X(\\beta) = \\infty\\) dla \\(\\beta \\geq \\alpha\\). Wykres \\(M_X\\) dla \\(\\alpha=3\\) wygląda następująco.Widzimy, że dla \\(n\\\\mathbb{N}\\),\n\\[\\begin{equation*}\n    M_X^{(n)}(\\beta) =\\alpha n!(\\alpha-\\beta)^{-n-1}\n\\end{equation*}\\]\nco za tym idzie\n\\[\\begin{equation*}\n    \\mathbb{E}\\left[X^n \\right]= M_X^{(n)}(0) = \\frac{n!}{\\alpha^n}.\n\\end{equation*}\\]Wykorzystując funkcję tworzącą momenty możemy wyciągnąć jeszcze jeden wniosek z nierówności Czebyszewa.Wniosek 17.3  (Nierówność Chernoffa) Dla dowolnej \\(\\lambda \\geq 0\\),\n\\[\\begin{equation*}\n    \\mathbb{P}\\left[X \\geq \\lambda \\right] \\leq \\inf_{\\beta >0} e^{-\\lambda \\beta}M_X(\\beta).\n\\end{equation*}\\]Proof. Ustalmy dowolną \\(\\beta>0\\). Dla \\(f(x) = e^{x\\beta}\\) mamy\n\\[\\begin{equation*}\n\\mathbb{P}[X\\geq \\lambda ]\\leq e^{-\\lambda \\beta}\\mathbb{E}\\left[e^{\\beta X} \\right] = e^{-\\lambda \\beta}M_X(\\beta).\n\\end{equation*}\\]\nTeza wynika teraz przez rozważenie kresu dolnego dla \\(\\beta>0\\).\n□Przykład 17.3  Wróćmy jeszcze raz prawdopodobieństwa wyrzucenia więcej niż \\(3n/4\\) orłów przy \\(n\\) rzutach symetryczną monetą.\nPrzypomnijmy, że liczba otrzymanych orłów \\(X\\) zapisuje się jako \\(X = \\sum_{j=1}^nX_j\\), gdzie zmienne\n\\(\\{X_j\\}_j\\) są niezależne \n\\[\\begin{equation*}\n    \\mathbb{P}\\left[X_j=0 \\right] =\n    \\mathbb{P}\\left[X_j=1 \\right] = 1/2.\n\\end{equation*}\\]\nStąd\n\\[\\begin{equation*}\n    M_X(\\beta) = \\mathbb{E} \\left[e^{\\beta X_1}e^{\\beta X_2}\\cdots e^{\\beta X_n} \\right] = M_{X_1}(\\beta)^n.\n\\end{equation*}\\]\nMamy\n\\[\\begin{equation*}\n    M_{X_1}(\\beta) = \\frac{e^{\\beta} +1}{2}.\n\\end{equation*}\\]Różniczkując funkcję pod kresem sprawdzamy, że minimum jest przyjęte dla \\(\\beta = \\log(3)\\). Stąd\n\\[\\begin{equation*}\n    \\mathbb{P}[X \\geq 3n/4] \\leq  (0.88)^n\n\\end{equation*}\\]Przykład 17.4  Widzimy, że im mniejszy \\(\\epsilon\\) tym punkt realizujący minimum jest bliżej zera.\nZauważmy, że\n\\[\\begin{equation*}\n\\frac{\\mathrm{d}}{\\mathrm{d}\\beta}\n\\left.e^{-\\beta \\epsilon} \\frac{e^{\\beta/2} +e^{-\\beta/2}}{2} \\right|_{\\beta=0} = -\\epsilon<0.\n\\end{equation*}\\]\nStąd\n\\[\\begin{equation*}\n\\inf_{\\beta>0} e^{-\\beta \\epsilon} \\frac{e^{\\beta/2} +e^{-\\beta/2}}{2} =\\gamma_\\epsilon<1.\n\\end{equation*}\\]\nCzyli\n\\[\\begin{equation*}\n    \\mathbb{P}[X\\geq (1/2+\\epsilon)n]\\leq \\gamma_\\epsilon^n.\n\\end{equation*}\\]\nPrzez symetrię (zamieniając orły na reszki)\n\\[\\begin{equation*}\n    \\mathbb{P}[X\\leq (1/2-\\epsilon)n]\\leq \\gamma_\\epsilon^n.\n\\end{equation*}\\]\nStąd\n\\[\\begin{equation*}\n    \\mathbb{P}[|X/n -1/2|\\geq \\epsilon]\\leq 2\\gamma_\\epsilon^n.\n\\end{equation*}\\]","code":""},{"path":"rodzaje-zbieżności-zmiennych-losowych.html","id":"rodzaje-zbieżności-zmiennych-losowych","chapter":"18 Rodzaje zbieżności zmiennych losowych","heading":"18 Rodzaje zbieżności zmiennych losowych","text":"Ostatni omówiony przez nas przykład pokazuje, że jeżeli \\(X_n\\) jest liczbą otrzymanych orłów przy \\(n\\\\mathbb{N}\\) rzutach\nsymetryczną monetą, \n\\[\\begin{equation*}\nX_n/n \\approx 1/2.\n\\end{equation*}\\]\nAby uściślić powyższą relację musimy omówić różne rodzaje zbieżności zmiennych losowych.","code":""},{"path":"rodzaje-zbieżności-zmiennych-losowych.html","id":"zbieżność-prawie-na-pewno","chapter":"18 Rodzaje zbieżności zmiennych losowych","heading":"18.1 Zbieżność prawie na pewno","text":"Definicja 18.1  Załóżmy, że \\(\\vec{X}_n\\) jest ciągiem \\(d\\)-wymiarowych wektorów losowych.\nMówimy, że \\(\\vec{X}_n\\) zbiega wektora losowego \\(\\vec{X}\\) prawie na pewno,\njeżeli\n\\[\\begin{equation*}\n  \\mathbb{P}\\left[ \\left\\{\\omega: \\vec{X}_n(\\omega)\\ \\vec{X}(\\omega)\\right\\} \\right]=1.\n\\end{equation*}\\]\nPiszemy wówczas\n\\(\\vec{X}_n\\\\vec{X}\\) p.n.Zbieżność prawie na pewno jest koncepcyjnie najprostszym rodzajem zbieżności z jaką będziemy mieć czynienia.Przykład 18.1  Niech \\(\\Omega = [-1,1]\\) z miarą Lebesgue’s. Rozważmy ciąg zmiennych losowych\n\\(X_n(\\omega) = \\omega^n\\). Wówczas dla \\(X(\\omega) =0\\) mamy\n\\[\\begin{equation*}\n\\left\\{\\omega: X_n(\\omega)\\ X(\\omega)\\right\\} = (-1,1).\n\\end{equation*}\\]\nPrzy czym \\(\\mathbb{P}[(-1,1)]=1\\). Zbiór \\(\\{-1,1\\}\\) na którym zbieżność nie zachodzi ma prawdopodobieństwo zero.\n\\(X_n\\) zbiega zatem prawie na pewno \\(X\\)\nZauważmy jednak, że dla \\(Y(\\omega) = \\mathbf{1}_{\\{1\\}}(\\omega)\\) mamy\n\\[\\begin{equation*}\n\\left\\{\\omega: X_n(\\omega)\\ Y(\\omega)\\right\\} = (-1,1].\n\\end{equation*}\\]\nZatem \\(X_n\\) zbiega prawie na pewno \\(Y\\). Zmienne losowe \\(X\\) \\(Y\\) są różne, ale miara probabilistyczna\n\\(\\mathbb{P}\\) tej różnicy nie widzi. Mamy bowiem\n\\[\\begin{equation*}\n\\left\\{\\omega: X(\\omega) \\neq Y(\\omega)\\right\\} = \\{1\\}\n\\end{equation*}\\]\nco jest zbiorem o prawdopodobieństwie zero.Definicja 18.2  Niech \\(\\vec{X}\\) \\(\\vec{Y}\\) będą wektorami losowymi. Powiemy, że \\(\\vec{X}\\) jest prawie na pewno równy \\(\\vec{Y}\\), jeżeli\n\\[\\begin{equation*}\n  \\mathbb{P}\\left[ \\left\\{\\omega: \\vec{X}(\\omega)=  \\vec{Y}(\\omega)\\right\\} \\right]=1.\n\\end{equation*}\\]\nPiszemy wtedy \\(\\vec{X}=\\vec{Y}\\) p.n.Powyższa terminologia pozwala stwierdzić, że granica p.n. jest jedyna.Lemma 18.1  Niech \\(\\{\\vec{X}_n\\}_{n\\\\mathbb{N}}\\) będzie ciągiem wektorów losowych.\nJeżeli \\(\\vec{X}_n\\) zbiega prawie na pewno \\(\\vec{X}\\)\noraz \\(\\vec{X}_n\\) zbiega prawie na pewno \\(\\vec{Y}\\), \\(\\vec{X} = \\vec{Y}\\) p.n.Proof. Pozostawiamy jako ćwiczenie.\n□Przykład 18.2  Niech \\(\\{X_n\\}_{n \\\\mathbb{N}}\\) będzie ciągiem zmiennych losowych takich, że \\(X_n\\) ma rozkład \\(\\mathrm{Exp}(n)\\). Wówczas\n\\[\\begin{equation}\n    \\lim_{n \\\\infty}\\sqrt{n}X_n  =0.\n    \\tag{18.1}\n\\end{equation}\\]\nRzeczywiście, dla \\(\\epsilon>0\\),\n\\[\\begin{equation*}\n\\mathbb{P}\\left[\\sqrt{n}X_n>\\epsilon \\right] =\\exp\\left\\{-\\epsilon \\sqrt{n}\\right\\}.\n\\end{equation*}\\]\nDla dostatecznie dużego \\(n\\), prawa strona powyższej tożsamości jest mniejsza niż \\(n^{-2}\\).\nZ Lematu Borrela-Cantellego\n\\[\\begin{equation*}\n\\mathbb{P}\\left[ \\sqrt{n} X_n >\\epsilon \\mbox{ dla niesk. wielu }n \\right]=0.\n\\end{equation*}\\]\nRównoważnie\n\\[\\begin{equation*}\n\\mathbb{P}\\left[ \\sqrt{n}X_n \\leq \\epsilon \\mbox{ dla dużych }n \\right]=1.\n\\end{equation*}\\]\nAle\n\\[\\begin{equation*}\n\\left\\{ \\sqrt{n}X_n \\leq \\epsilon \\mbox{ dla dużych }n \\right\\}=\n\\left\\{ \\limsup_{n \\\\infty}\\sqrt{n}X_n \\leq \\epsilon\\right\\}.\n\\end{equation*}\\]\nReasumując\n\\[\\begin{equation*}\n\\mathbb{P}\\left[ \\limsup_{n \\\\infty}\\sqrt{n}X_n \\leq \\epsilon\\right] =1.\n\\end{equation*}\\]\nRozważając teraz \\(\\epsilon \\0\\) korzystając z ciągłości prawdopodobieństwa\n\\[\\begin{equation*}\n\\mathbb{P}\\left[ \\limsup_{n \\\\infty}\\sqrt{n} X_n =0\\right] =1.\n\\end{equation*}\\]\nNiech\n\\[\\begin{equation*}\n=\\left\\{ \\limsup_{n \\\\infty}\\sqrt{n}X_n =0\\right\\}.\n\\end{equation*}\\]\nAle skoro \\(X_n\\geq 0\\), dla \\(\\omega \\\\),\n\\[\\begin{equation*}\n0\\leq \\liminf_{n \\\\infty}\\sqrt{n} X_n(\\omega)\n\\leq\\limsup_{n \\\\infty}\\sqrt{n} X_n(\\omega) =0.\n\\end{equation*}\\]\nCzyli dla \\(\\omega \\\\),\n\\[\\begin{equation*}\n    \\lim_{n \\\\infty}\\sqrt{n} X_n(\\omega) =0.\n\\end{equation*}\\]\nPokazaliśmy właśnie, że\n\\[\\begin{equation*}\n\\subseteq \\left\\{\\omega \\: : \\:\n\\lim_{n \\\\infty}\\sqrt{n} X_n(\\omega) =0\n\\right\\}\n\\end{equation*}\\]\nSkoro \\(\\mathbb{P}[]=1\\), \n\\[\\begin{equation*}\n\\mathbb{P} \\left[\\left\\{\\omega \\: : \\:\n\\lim_{n \\\\infty}\\sqrt{n} X_n(\\omega) =0\n\\right\\}\\right]=1.\n\\end{equation*}\\]\nCzyli (18.1) rzeczywiście zachodzi.\nZbieżność możemy zaobserwować również na symulacjach.Twierdzenie 18.1  Jeżeli \\(\\vec{X}_n \\overset{{\\rm p.n.}}{\\} \\vec{X}\\) \\(\\vec{Y}_n \\overset{{\\rm p.n.}}{\\} \\vec{Y}\\), \\(\\vec{X}_n + b\\vec{Y}_n \\overset{{\\rm p.n.}}{\\} \\vec{X}+b\\vec{Y}\\).\\(\\langle \\vec{X}_n,\\vec{Y}_n \\rangle \\overset{{\\rm p.n.}}{\\} \\langle\\vec{X},\\vec{Y}\\rangle\\).Proof. Ćwiczenie.\n□","code":"\n# Ustawienie liczby obserwacji\nn_vals <- 1:10000\n\n# Wygeneruj zmienne losowe X_n ~ Exp(n)\n#set.seed(123)  # dla powtarzalności\nX_n <- rexp(length(n_vals), rate = n_vals)\n\n# Oblicz sqrt(n)* X_n\nratios <- sqrt(n_vals) * X_n\n\n# Ustawienia kolorystyczne (Solarized)\nbg_color <- \"#002b36\"\nfg_color <- \"#eee8d5\"\nprimary_color <- \"#2aa198\"\npar(bg = bg_color)\n\n# rysowanie wykresu\nplot(n_vals, ratios,\n     type = \"l\",\n     col = primary_color,\n     xlab = \"n\",\n     ylab = expression(sqrt(n)*X[n]),\n     col.lab = fg_color,\n     col.main = fg_color,\n     col.axis = fg_color,\n     fg = fg_color,\n     panel.first = grid())"},{"path":"rodzaje-zbieżności-zmiennych-losowych.html","id":"zbieżność-według-prawdopodobieństwa","chapter":"18 Rodzaje zbieżności zmiennych losowych","heading":"18.2 Zbieżność według prawdopodobieństwa","text":"Drugim omawianym przez nas rodzajem zbieżności będzie znana z kursu miary całki zbieżność według miary.\nZ racji, że operujemy wyłącznie na miarach probabilistycznych dostosujemy nieznacznie terminologię.Definicja 18.3  Niech \\(\\{\\vec{X}_n\\}_{n \\\\mathbb{N}}\\) będzie ciągiem wektorów losowych.\nPowiemy, że \\(\\vec{X}_n\\) zbiega \\(\\vec{X}\\) według prawdopodobieństwa,\njeżeli dla każdego \\(\\varepsilon >0\\),\n\\[\\begin{equation*}\n\\lim_{n\\\\infty}\n\\mathbb{P}\\left[ \\left\\{\\omega: \\left\\| \\vec{X}_n(\\omega) - \\vec{X}(\\omega) \\right\\| > \\varepsilon\\right\\} \\right]\n= \\lim_{n\\\\infty}\\mathbb{P}\\left[\\left\\| \\vec{X}_n - \\vec{X}\\right\\| > \\varepsilon \\right] = 0.\n\\end{equation*}\\]\nPiszemy wówczas\n\\(\\vec{X}_n \\overset{\\mathbb{P}}{\\} \\vec{X}\\).Zbieżność według prawdopodobieństwa mówi, że z dużym prawdopodobieństwem \\(\\vec{X}_n\\) jest bliski\n\\(\\vec{X}\\).Przykład 18.3  Niech \\(\\{X_n\\}_{n \\\\mathbb{N}}\\) będzie ciągiem zmiennych losowych takich,\nże \\(X_n\\) ma rozkład \\(\\mathrm{Exp}(n)\\). Wówczas\n\\[\\begin{equation}\n    \\lim_{n \\\\infty}\\sqrt{n}X_n  =0\n\\end{equation}\\]\nwedług prawdopodobieństwa. Rzeczywiście, dla \\(\\epsilon>0\\),\n\\[\\begin{equation*}\n\\mathbb{P}\\left[\\sqrt{n}X_n>\\epsilon \\right] =\\exp\\left\\{-\\epsilon \\sqrt{n}\\right\\}\\0.\n\\end{equation*}\\]Twierdzenie 18.2  Jeżeli \\(\\vec{X}_n \\overset{{\\rm p.n.}}{\\} \\vec{X}\\),\n\\(\\vec{X}_n \\overset{\\mathbb{P}}{\\} \\vec{X}\\).Proof. Bez zmieszania ogólności załóżmy, że rozważane wektory losowe są jednowymiarowe.\nZałóżmy, że \\(X_n \\overset{{\\rm p.n.}}{\\} X\\). Ustalmy \\(\\varepsilon >0\\).\nPrzypomnijmy, że zbieżność punktowa oznacza, że dla prawie każdej \\(\\omega\\) zachodzi\n\\[\n  \\exists_m \\ \\forall_{ n\\ge m} \\ \\ |X_n(\\omega) - X(\\omega)| < \\varepsilon,\n  \\]\nwięc w terminach zbiorów\n\\[\n  \\mathbb{P}\\left[ \\bigcup_{m=1}^\\infty \\bigcap_{n=m}^\\infty \\left\\{ \\omega: \\left| X_n(\\omega) - X(\\omega)  \\right|<\\varepsilon \\right\\}\n  \\right] = 1.\n  \\]\nCiąg zdarzeń\n\\[\n    A_m =  \\bigcap_{n=m}^\\infty \\left\\{ \\omega: \\left| X_n(\\omega) - X(\\omega)  \\right|<\\varepsilon \\right\\}\n  \\]\njest wstępujący. Zatem\n\\[\n    1=   \\mathbb{P} \\left[  \\bigcup_{m=1}^\\infty A_m \\right] = \\lim_{m\\\\infty} \\mathbb{P}[A_m].\n  \\]\nZauważmy, że \\(A_m\\subset \\{ |X_m-X| < \\varepsilon  \\}\\), więc\n\\[\n  \\lim_{m\\\\infty} \\mathbb{P}\\left[|X_m-X|<\\varepsilon  \\right] = 1\n  \\]\nzatem\n\\[    \n   \\lim_{m\\\\infty} \\mathbb{P}\\left[|X_m-X|\\ge\\varepsilon  \\right] = 0\n  \\]\nco pokazuje zbieżność \\(X_m\\) \\(X\\) według prawdopodobieństwa.\n□Implikacja odwrotna nie jest prawdziwa.Przykład 18.4  Na przestrzeni probabilistycznej\n\\(([0,1], \\mathcal{B}([0,1]),{\\rm Leb})\\)\ndefiniujemy ciąg\nzmiennych losowych:Kolejne zmienne losowe definiujemy w analogiczny sposób:\njeżeli \\(2^n \\le k < 2^{n+1}\\), \n\\[ X_k = {\\bf 1}_{[ \\frac{k-2^n}{2^n}, \\frac{k-2^n+1}{2^n} )}\n\\]\nPowyższy ciąg zmiennych losowych zbiega \\(0\\) według prawdopodobieństw\n(bo miara zbioru na którym \\(X_k\\) jest niezerowe zbiega zera),\nale nie zbiega prawie na pewno\n(bo dla każdego punktu \\(\\omega\\[0,1]\\) nieskończenie elementów \\(X_k(\\omega)=1\\)).Przykład 18.5  Rozważmy jeszcze jeden przykład. Niech \\(\\{p_n\\}_{n \\\\mathbb{N}}\\) będzie dowolnym ciągiem liczb\nz przedziału \\([0,1]\\). Niech \\(\\{X_n\\}_{n \\\\mathbb{N}}\\) będzie nieskończonym\nciągiem niezależnych zmiennych losowych takich, że\n\\[\\begin{equation*}\n\\mathbb{P}[X_n=1] = 1-\\mathbb{P}[X_n=0]=p_n.\n\\end{equation*}\\]\nZbadamy zbieżność \\(\\{X_n\\}_{n \\\\mathbb{N}}\\) w zależności od zachowania ciągu \\(\\{p_n\\}_{n \\\\mathbb{N}}\\).\nZauważmy, że dla \\(\\epsilon\\(0,1)\\) mamy\n\\[\\begin{equation*}\n    \\mathbb{P}[|X_n|>\\epsilon] = \\mathbb{P}[X_n=1]=p_n.\n\\end{equation*}\\]\nW szczególności \\(X_n \\^\\mathbb{P} 0\\) wtedy tylko wtedy, gdy \\(p_n \\0\\).\nJeżeli więc \\(\\sum_{n}p_n <\\infty\\),\ndla prawie każdej \\(\\omega\\) w ciągu \\(\\{X_n(\\omega)\\}_{n\\\\mathbb{N}}\\) będzie skończenie wiele jedynek. Oznacza , że\n\\(X_n \\0\\) p.n.\nJeśli \\(\\sum_n p_n =\\infty\\), dla prawie każdej \\(\\omega\\) w ciągu\n\\(\\{X_n(\\omega)\\}_{n\\\\mathbb{N}}\\) będzie nieskończenie wiele jedynek.\nOznacza , że jeżeli \\(p_n \\0\\) ale \\(\\sum_n p_n=\\infty\\), \n\\(X_n \\^\\mathbb{P} 0\\) ale zbieżność nie będzie zachodziła prawie na pewno.\nWarto w tym momencie rzucić okiem na symulacje aby zobaczyć jak wygląda ciąg\n\\(\\{X_n(\\omega)\\}_{n\\\\mathbb{N}}\\) w różnych przypadkach.Jeżeli \\(\\sum_{n}p_n<\\infty\\), jak ma miejsce dla \\(p_n = n^{-11/10}\\), \nod pewnego miejsca \\(\\{X_n(\\omega)\\}_{n\\\\mathbb{N}}\\) jest ciągiem stale równym zero.\nJeżeli \\(\\sum_n p_n =\\infty\\) ale \\(p_n \\0\\), jak ma miejsce dla \\(p_n = n^{-9/10}\\), \nw ciągu \\(\\{X_n(\\omega)\\}_{n\\\\mathbb{N}}\\) obserwować będziemy nieskończenie wiele jedynek, lecz\nodległości między kolejnymi jedynkami będą stawały się coraz większe. Innymi słowy częstotliwość jedynek będzie spadała\nzera.\nZałóżmy dla ustalenia uwagi, że \\(p_n=n^{-9/10}\\).\nZauważmy, że jeżeli rozważmy deterministyczny podciąg \\(\\{X_{n^2}\\}_{n \\\\mathbb{N}}\\), \n\\(p_{n^2} = n^{-18/10}\\). Z powyższej dyskusji wynika, że \\(X_{n^2} \\0\\) p.n.\nPowyższa symulacja tłumaczy co się dzieje. W pełnym ciągu \\(\\{X_n(\\omega)\\}_{n}\\) widzimy nieskończenie wiele jedynek.\nPodciąg złożony w kwadratów liczb naturalnych jest rozłożony na tyle rzadko, że prawdopodobieństwo,\nże jedynka trafi się akurat w kwadracie liczby naturalnej jest bardzo małe. Ostatecznie w odpowiednio rzadkim\npodciągu \\(\\{X_{n^2}(\\omega)\\}_n\\) widzimy tylko skończenie wiele jedynek.Zjawisko z poprzedniego przykładu można uogólnić.Twierdzenie 18.3  (Twierdzenie Riesza) Jeżeli \\(X_n \\overset{\\mathbb{P}}{\\} X\\),\nistnieje podciąg \\(\\{X_{n_k}\\}_{k\\\\mathbb{N}}\\) taki, że\n\\(X_{n_k} \\overset{{\\rm p.n}}{\\} X\\).Proof. Z definicji zbieżności według prawdopodobieństwa dla każdego \\(k\\) istnieje \\(n_k\\) takie, że\n\\[\n  \\mathbb{P}\\left[ |X_n-X| > 2^{-k} \\right] \\le 2^{-k}\\qquad \\forall n\\ge n_k.\n  \\]\nMożemy ponadto założyć, że ciąg \\(n_k\\) jest rosnący. Zatem\n\\[\n  \\sum_{k=1}^{\\infty} \\mathbb{P}\\left[ |X_{n_k} - X| > 2^{-k} \\right] <\\infty.\n  \\]\nMożemy więc skorzystać z lematu Borela-Cantellego\n\\[\n  \\mathbb{P}\\left[ |X_{n_k} - X| > 2^{-k},\\ {\\rm .o.} \\right] = 0.\n  \\]\nWynika więc stąd, że \\(|X_{n_k}(\\omega) - X(\\omega)| > 2^{-k}\\)\nzachodzi jedynie skończenie wiele razy dla prawie każdej \\(\\omega\\). Zatem\n\\[\n  |X_{n_k}(\\omega) - X(\\omega)| \\le  2^{-k} \\qquad \\mbox{ dla } k>k(\\omega)\n  \\] stąd wynika \\(X_{n_k} \\overset{{\\rm p.n.}}{\\} X\\).","code":"\nn_vals <- 1:10000\np_n <- 1 / n_vals^(11/10)\n\nset.seed(123)\nX_n <- rbinom(length(n_vals), size = 1, prob = p_n)\n\n# Kolory Solarized\nbg_color <- \"#002b36\"\nfg_color <- \"#eee8d5\"\nprimary_color <- \"#2aa198\"\n\n# Ustawienia tła\npar(bg = bg_color)\n\n# Wykres punktowy (większość to zera!)\nplot(n_vals, X_n,\n     type = \"p\",\n     pch = 16,\n     col = primary_color,\n     xlab = \"n\",\n     ylab = expression(X[n]),\n     main = expression(paste(\"Symulacja X[n] ~ Bernoulli(1/n^(11/10))\")),\n     col.lab = fg_color,\n     col.main = fg_color,\n     col.axis = fg_color)"},{"path":"rodzaje-zbieżności-zmiennych-losowych.html","id":"zbieżność-w-lp","chapter":"18 Rodzaje zbieżności zmiennych losowych","heading":"18.3 Zbieżność w \\(L^p\\)","text":"Zauważmy, że ze zbieżności według prawdopodobieństwa lub prawie na pewno \\(X_n\\) \\(X\\) nie wynika, że\n\\[\\begin{equation*}\n    \\lim_{n\\\\infty}\\mathbb{E}\\left[ |X_n|^p \\right] =\\mathbb{E} \\left[|X|^p \\right].\n\\end{equation*}\\]\nDzieje się tak dlatego, że zbieżność prawie na pewno czy według prawdopodobieństwa mówią tylko, że\nz dużym prawdopodobieństwem \\(X_n\\) jest bliskie \\(X\\). Nie jest jasne co się dzieje na zbiorze o małym\nprawdopodobieństwie, gdzie \\(X_n\\) jest dalekie od \\(X\\). Odległość ta może być bardzo duża.Definicja 18.4  Niech \\(p\\geq 1\\).\nPowiemy, żę \\(X_n\\) zbiega \\(X\\) w \\(L^p\\),\njeżeli \\(X_n\\L^p\\) (tzn. \\(\\|X_n\\|_p =\\mathbb{E} [|X_n|^p]^{1/p}<\\infty\\))\noraz\n\\[\n  \\lim_{n\\\\infty}\\|X_n-X\\|_p =  \\lim_{n\\\\infty}\\mathbb{E}\\left[|X_n-X|^p\\right]^{1/p} = 0.\n  \\]\nPiszemy wówczas \\(X_n \\overset{L^p}{\\} X\\).Przykład 18.6  Rozważmy przestrzeń probabilistyczną \\(([0,1], \\mathcal{B}([0,1]), \\mathrm{Leb})\\).\nRozważmy zmienne losowe \\(X_n = n^{\\alpha} \\mathbf{1}_{[0,1/n]}\\). Wówczas \\(X_n \\0\\) p.n.\nDla \\(p\\geq 1\\) mamy\n\\[\\begin{equation*}\n\\|X_n-0\\|_p^p = \\mathbb{E}[|X_n|^p] = n^{\\alpha p-1}.\n\\end{equation*}\\]\nzbieżność zachodzi więc dla \\(p<1/\\alpha\\).Twierdzenie 18.4  Jeżeli \\(X_n \\overset{L^p}{\\} X\\), \\(X_n \\overset{\\mathbb{P}}{\\} X\\). Implikacja odwrotna nie jest prawdziwa.Proof. Z nierówności Czebyszewa wynika, że\n\\[\n  \\mathbb{P}\\big[ |X_n - X| > \\varepsilon  \\big] \\le \\frac{\\mathbb{E}|X_n-X|^p}{\\varepsilon^p} \\0.\n  \\]","code":""},{"path":"rodzaje-zbieżności-zmiennych-losowych.html","id":"słabe-prawo-wielkich-liczb","chapter":"18 Rodzaje zbieżności zmiennych losowych","heading":"18.4 Słabe prawo wielkich liczb","text":"Twierdzenie 18.5  (Słabe Prawo Wielkich Liczb (SPWL)) Jeżeli ciąg zmiennych losowych \\(X_1,X_2,\\ldots\\) spełnia \\(\\mathbb{E}[ X_i^2]<\\infty\\),\nzmienne losowe są nieskorelowane oraz mają wspólnie\nograniczoną wariancję:\n\\[\\begin{equation*}\n    \\sup_{n \\\\mathbb{N}} \\mathbb{V}ar[X_n] <\\infty\n  \\end{equation*}\\]\n\n\\[\n  \\frac{X_1+\\ldots + X_n - \\mathbb{E}[X_1+\\ldots + X_n]}{n} \\overset{L^2}{\\} 0.\n  \\]\nW szczególności jeżeli wszystkie zmienne losowe \\(X_i\\) mają taką samą wartość oczekiwaną, \n\\[\n  \\frac{X_1+\\ldots + X_n}{n} \\overset{\\mathbb{P}}{\\} \\mathbb{E} [X_1].\n  \\]Proof. Mamy\\[\\begin{align*}\n    &\\left\\| \\frac{X_1+\\ldots + X_n - \\mathbb{E}[X_1+\\ldots + X_n]}{n} \\right\\|_2^2 =\\\\\n     &\\mathbb{V}ar\\bigg( \\frac{X_1+\\ldots + X_n}{n} \\bigg)\n    = \\frac{1}{n^2} \\mathbb{V}ar(X_1+\\ldots + X_n)\\\\\n    &= \\frac{1}{n^2}\\sum_{k=1}^n \\mathbb{V}ar[ X_k]\n    \\leq \\frac{C}{n}\\0.\n  \\end{align*}\\]Wkrótce pokażemy znacznie mocniejszy wynik: Mocne Prawo Wielkich Liczb (MPWL), które mówi o zbieżności p.n.Przykład 18.7  Niech \\(X_1,X_2,\\ldots\\) będą niezależnymi zmiennymi losowymi o rozkładzie \\(U([-1,1])\\). Wówczas zmienne losowe\n\\(X^2_1,X^2_2,\\ldots\\) są niezależne oraz\n\\[\n  \\mathbb{E} X_i^2 =\\int_{-1}^1 x^2 \\frac{dx}2 = \\frac 13, \\qquad \\mathbb{V}ar X_i^2 \\le \\mathbb{E} X_i^4 \\le 1.\n  \\]\nMożemy więc skorzystać z SPWL:\n\\[  \\frac{X_1^2+\\ldots + X_n^2}{n} \\overset{\\mathbb{P}}{\\} \\frac 13.\\]\nUstalmy \\(\\varepsilon\\(0,1/2)\\) zdefiniujmy\n\\[A_{n,\\varepsilon} =\\bigg\\{ x\\\\mathbb{R}^n:\\; (1-\\varepsilon) \\sqrt{\\frac n3} \\le \\|x\\| \\le (1+\\varepsilon) \\sqrt{\\frac n3}  \\bigg\\}\n\\]Wówczas\n\\[\\begin{align*}\n  \\frac 1{2^n}\\cdot {\\rm Leb(A_{n,\\varepsilon} \\cap [-1,1]^n)} & = \\mathbb{P}\\big[ (X_1,\\ldots, X_n) \\A_{n,\\varepsilon} \\big]\\\\\n  &= \\mathbb{P}\\bigg[  (1-\\varepsilon) \\sqrt{\\frac n3} \\le \\sqrt{X_1^2+\\ldots + X_n^2} \\le (1+\\varepsilon) \\sqrt{\\frac n3}\n  \\bigg]\\\\\n  &= \\mathbb{P}\\bigg[\\frac 13  (1-2\\varepsilon+\\varepsilon^2) \\le \\frac{X_1^2+\\ldots + X_n^2}n \\le \\frac 13 (1+2\\varepsilon + \\varepsilon^2)\n  \\bigg]\\\\\n&\\ge \\mathbb{P}\\bigg[ \\bigg| \\frac 1n \\sum_{=1}^n X_i^2 - \\frac 13  \\bigg| \\le \\frac 13(2\\varepsilon - \\varepsilon^2)\n\\bigg]  \\1.\n\\end{align*}\\]\nTen rachunek pokazuje, że niemal cała masa kostki \\([-1,1]^n\\) pochodzi z \\(A_{n,\\varepsilon}\\), czyli ‘pogrubionego’ brzegu sfery o środku 0 promieniu \\(\\sqrt{n/3}\\).","code":""},{"path":"prawo-0-1-kołmogorowa.html","id":"prawo-0-1-kołmogorowa","chapter":"19 Prawo \\(0-1\\) Kołmogorowa","heading":"19 Prawo \\(0-1\\) Kołmogorowa","text":"Wiemy już, że jeżeli\nrzucamy wielokrotnie kością, \\(S_n\\) będące liczbą otrzymanych szóstek w pierwszych \\(n \\\\mathbb{N}\\)\nspełnia\n\\[\\begin{equation}\n\\lim_{n \\\\infty}S _n/n = 1/6\n\\tag{19.1}\n\\end{equation}\\]\nwedług prawdopodobieństwa. Oznacza , że z dużym prawdopodobieństwem \\(S_n/n\\approx 1/6\\) dla dużych \\(n\\).\nSama zbieżność według prawdopodobieństwa nie wyklucza, że wykonując wielokrotnie \\(n\\) rzutów monetą kiedyś\nzdarzy się, że \\(S_n/n\\) będzie odległe od \\(1/6\\). Chcemy pokazać, że dla dużych \\(n\\), \\(S_n/n\\) będzie zawsze\nbliskie \\(1/6\\). Oznacza , że zbieżność (19.1) powinna zachodzić prawie na pewno.W ogólnej sytuacji będziemy badać zbieżność sum postaci\n\\[\n\\frac{X_1+\\ldots + X_n- a_n}{b_n}.\n\\]\nZbieżność tego typu sum pozwala badać zbieżności szeregów. Przykładowo, z podstawowego kursu\nanalizy wiemy, że rozbieżny jest szereg\n\\[\n\\sum_{n=1}^\\infty \\frac 1n=\\infty.\n\\]\nJednakże w wersji naprzemiennej jest już zbieżny\n\\[\n\\sum_{n=1}^\\infty \\frac{(-1)^n}n=-\\log(2).\n\\]\nCo się jednak dzieje, jeżeli znaki w tym szeregu postawimy w sposób losowy? Jeżeli\nrozważymy ciąg \\(\\{X_n\\}_{n\\\\mathbb{N}}\\) niezależnych zmiennych o jednakowym rozkładzie\n\\[\\begin{equation*}\n    \\mathbb{P}\\left[X_n=1 \\right]\n=\\mathbb{P}\\left[X_n=-1 \\right] =1/2,\n\\end{equation*}\\]\nco możemy powiedzieć o zbieżności szeregu\n\\[\\begin{equation}\n    \\sum_{n=1}^\\infty \\frac{X_n}{n}?\n    \\tag{19.2}\n\\end{equation}\\]","code":""},{"path":"prawo-0-1-kołmogorowa.html","id":"sigma-ciało-ogonowe","chapter":"19 Prawo \\(0-1\\) Kołmogorowa","heading":"19.1 \\(\\sigma\\)-ciało ogonowe","text":"Pierwszym krokiem w kierunku udowodnienia (19.1) w wersji prawie na pewno będzie\nwykształcenie narzędzi, które pozwolą stwierdzić, że szeregi typu (19.2) jest zbieżny\nz prawdopodobieństwem jeden bądź zero. Okazuje się, że tego typu szeregi nie mogą być z bieżne\nz prawdopodobieństwem \\(p \\(0,1)\\).Definicja 19.1  Niech \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) będzie przestrzenią probabilistyczną\nniech \\(\\{\\mathcal{F}_n \\}_{n \\\\mathbb{N}}\\), będzie ciągiem \\(\\sigma\\)-ciał zawartych w \\(\\mathcal{F}\\).\nZdefiniujmy\n\\[\n  \\mathcal{F}_{n,\\infty} =\\sigma \\left(\\bigcup_{k=n}^\\infty \\mathcal{F}_k\\right),\n  \\]\noraz\n\\[\n  \\mathcal{F}_\\infty = \\bigcap_{n=1}^\\infty \\mathcal{F}_{n,\\infty}.\n  \\]\n\\(\\mathcal{F}_\\infty\\) nazywamy \\(\\sigma\\)-ciałem ogonowym.Przykład 19.1  Niech \\(\\Omega = \\mathbb{N}^\\mathbb{N}\\) będzie zbiorem wszystkich ciągów \\(\\{a_n\\}_{n\\\\mathbb{N}}\\)\no wyrazach naturalnych.\nNiech\n\\[\\begin{equation*}\n\\mathcal{F}_n = \\left\\{ \\mathbb{N}^{n-1}\\times \\times \\mathbb{N}^\\mathbb{N} \\: : \\: \\subseteq \\mathbb{N} \\right\\}.\n\\end{equation*}\\]\nWówczas każdy \\(\\\\mathcal{F}_n\\) jest postaci\n\\[\\begin{equation*}\n= \\mathbb{N}^{n-1}\\times B \\times \\mathbb{N}^\\mathbb{N}\n= \\{ \\{ a_j\\}_{j \\\\mathbb{N}}  \\: : \\: a_n \\B\\},\n\\end{equation*}\\]\ndla pewnego \\(B \\subseteq \\mathbb{N}\\).\nZdarzenia z \\(\\mathcal{F}_n\\) kodują więc informacje o \\(n\\)-tym\nwyrazie ciągu. \\(\\sigma\\)-ciało \\(\\mathcal{F}_{n, \\infty}\\) zawiera zdarzenia (prosto postaci\n\\[\\begin{equation*}\nB = \\mathbb{N}^{n-1}\\times B_n \\times B_{n+1} \\times \\cdots =\n\\{ \\{ a_j\\}_{j \\\\mathbb{N}}  \\: : \\: a_j \\B_j, \\: j \\geq n\\},\n\\end{equation*}\\]\ndla dowolnych \\(B_n, B_{n+1}, \\ldots \\subseteq \\mathbb{N}\\).\nW ogólności każdy zbiór \\(\\\\mathcal{F}_{n,\\infty}\\) jest postaci\n\\[\\begin{equation*}\n    = \\mathbb{N}^{n-1}\\times B,\n\\end{equation*}\\]\ndla pewnego \\(B \\\\mathcal{F}_{1, \\infty}\\)\nW ogólności jeżeli \\(\\\\mathcal{F}_{n, \\infty}\\),\ndla dowolnych ciągów\n\\(\\{a_j\\}_{j\\\\mathbb{N}}\\) oraz \\(\\{b_j\\}_{j\\\\mathbb{N}}\\) takich, że \\(a_j=b_j\\) dla \\(j\\geq n\\),\n\\[\\begin{equation*}\n    \\{a_j\\}_{j\\\\mathbb{N}} \\\\iff  \\{b_j\\}_{j\\\\mathbb{N}} \\.\n\\end{equation*}\\]\nInnymi słowy , czy dany ciąg należy \\(\\) zależy tylko od jego wyrazów o indeksach nie mniejszych niż \\(n\\).\nIstnieje jeszcze jedna charakteryzacja \\(\\mathcal{F}_{n,\\infty}\\). Rozważmy odwzorowanie \\(\\pi \\colon \\Omega \\\\Omega\\)\nzadane przez\n\\[\\begin{equation*}\n\\pi \\left(\\{a_j\\}_{j \\\\mathbb{N}} \\right) = \\{a_{j+1}\\}_{j\\\\mathbb{N}}.\n\\end{equation*}\\]\nWówczas dla \\(\\\\mathcal{F}_{1,\\infty}\\) mamy\n\\[\\begin{equation*}\n    \\pi^{-1}[] = \\mathbb{N}\\times .\n\\end{equation*}\\]\nInnymi słowy\n\\[\\begin{equation*}\n    \\left\\{ \\pi^{-1}[] \\: : \\: \\\\mathcal{F}_{1, \\infty} \\right\\} = \\mathcal{F}_{2,\\infty}.\n\\end{equation*}\\]\nPodobnie skoro\n\\[\\begin{equation*}\n    \\pi^{-n}[] = \\mathbb{N}^{n}\\times .\n\\end{equation*}\\]\n(tutaj \\(\\pi^{-n}\\) oznacza przeciwobraz \\(n\\)-krotnego złożenia \\(\\pi\\)), \n\\[\\begin{equation*}\n    \\left\\{ \\pi^{-n}[] \\: : \\: \\\\mathcal{F}_{1, \\infty} \\right\\} = \\mathcal{F}_{n+1,\\infty}.\n\\end{equation*}\\]\nStąd\n\\[\\begin{equation*}\n\\\\mathcal{F}_\\infty \\iff \\forall n \\: \\pi^{-n}[] = .\n\\end{equation*}\\]\n\\(\\sigma\\)-ciało ogonowe składa się ze zdarzeń niezmienniczych na dowolne skończone przesunięcie.\nPrzykładami takich zdarzeń są\n\\[\\begin{equation*}\n    \\left\\{ \\{a_n\\}_{n \\\\mathbb{N}} \\: : \\: \\lim_{n \\\\infty}a_n=17 \\right\\}\n\\end{equation*}\\]\noraz\n\\[\\begin{equation*}\n    \\left\\{ \\{a_n\\}_{n \\\\mathbb{N}} \\: : \\: \\sum_{n=1}^\\infty |a_n|<\\infty \\right\\}.\n\\end{equation*}\\]\nZauważmy wreszcie, że\n\\[\\begin{equation*}\n\\left\\{ \\{a_n\\}_{n \\\\mathbb{N}} \\: : \\: \\sum_{n=1}^\\infty |a_n|>1 \\right\\} \\notin \\mathcal{F}_\\infty.\n\\end{equation*}\\]Remark. Załóżmy, że \\(\\mathcal{F}_n\\) jest \\(\\sigma\\)-ciałem generowanym przez zmienną losową \\(X_n\\).\nWówczas\\(\\mathcal{F}_n\\) oznacza wiedzę o doświadczeniu w czasie \\(n\\);\\(\\mathcal{F}_{n,\\infty}\\) - wiedza o przyszłości od chwili \\(n\\), \\(\\mathcal{F}_{n,\\infty}\\supset \\mathcal{F}_{n+1,\\infty}\\);\\(\\mathcal{F}_\\infty\\) - wiedza o nieskończenie odległej przyszłościZdarzenie \\(\\\\mathcal{F}_\\infty\\), gdy dla każdego \\(n\\),\n\\(\\\\mathcal{F}_{n,\\infty}\\).Przykład 19.2  Niech \\(\\{\\mathcal{F}_n\\}_{n \\\\mathbb{N}}\\) będzie dowolnym ciągiem \\(\\sigma\\)-ciał.\nJeżeli \\(A_n\\\\mathcal{F}_n\\), \n\\[\n    =\\limsup_n A_n = \\bigcap_{k\\ge 1}\\bigcup_{\\ge k}A_i\\\\mathcal{F}_{\\infty},\n  \\]\nbo dla każdego \\(n\\)\n\\[\n  = \\bigcap_{k\\ge n}\\bigcup_{\\ge k}A_i\\\\mathcal{F}_{n,\\infty}.\n\\]Załóżmy, że \\(\\{X_n\\}_{n\\\\mathbb{N}}\\) jest ciągiem zmiennych losowych. Niech \\(\\mathcal{F}_n = \\sigma(X_n)\\).\nZdarzenie\n\\[\nB = \\big\\{ \\omega:\\; \\mbox{ ciąg } \\{ X_n(\\omega) \\} \\mbox{ jest zbieżny }\\big\\}\n\\]\nnależy \\(\\sigma\\)-ciała ogonowego.\nIstotnie, przypomnijmy definicję Cauchy’ego\nzbieżności ciągu:\n\\[\n\\forall_{\\varepsilon > 0}\\ \\exists_k \\ \\forall_{j,m \\ge k} \\  |X_j(\\omega) - X_m(\\omega)| < \\varepsilon.\n\\] W tym przypadku należy rozważać przeliczalną rodzinę parametrów korzystać z równoważnej definicji\n\\[\n\\forall_{N\\\\mathbb{N}}\\ \\exists_k \\ \\forall_{j,m \\ge k} \\  |X_j(\\omega) - X_m(\\omega)| < 1/N,\n\\]\nktóra pociąga\n\\[\\begin{align*}\nB &= \\bigcap_{N=1} \\bigcup_{k=1}^\\infty \\bigcap_{j,m\\ge k} \\big\\{ |X_j(\\omega) - X_m(\\omega)| < 1/N  \\big\\}\\\\\n&= \\bigcap_{N=n} \\bigcup_{k=N}^\\infty \\bigcap_{j,m\\ge k} \\big\\{ |X_j(\\omega) - X_m(\\omega)| < 1/N  \\big\\} \\\\mathcal{F}_{n,\\infty}.\n\\end{align*}\\]\nPokazaliśmy więc, że dla każdego \\(n\\),\n%{ :; { X_n() } }\n\\(B \\\\mathcal{F}_{n,\\infty}\\), z kolei implikuje\n\\(B \\\\mathcal{F}_\\infty\\).\nPodobnie pokazujemy, że zdarzenia\n\\[\n\\left\\{ \\sup_n |X_n|<\\infty \\right\\} \\qquad \\mbox{ } \\qquad \\left\\{ \\sum_{n=1}^\\infty X_n \\mbox{ jest zbieżny}  \\right\\}\n\\] należą \\(\\sigma\\)-ciała ogonowego.\nZ kolei zdarzenie\n\\[\n\\left\\{ \\limsup_{n\\\\infty} (X_1+X_2+\\ldots+ X_n) > 0\n\\right\\}\n\\]\nnie należy \\(\\sigma\\)-ciała ogonowego, ponieważ pierwsze składniki mają wpływ na granicę.Twierdzenie 19.1  (Prawo 0-1 Kołmogorowa) Jeżeli \\(\\sigma\\)-ciała \\(\\mathcal{F}_n\\) są niezależne, dla każdego zdarzenia \\(\\\\mathcal{F}_{\\infty}\\) zachodzi\n\\[\n  \\mathbb{P}[]=0\\qquad \\mbox{lub}\\qquad \\mathbb{P}[]=1.\n  \\]Lemma 19.1  Jeżeli rodziny zbiorów \\({\\mathcal }_1,{\\mathcal }_2\\) są niezależne obie tworzą \\(\\pi\\)-układ, \\(\\sigma\\)-ciała\nprzez nie generowane \\(\\sigma({\\mathcal }_1)\\) \\(\\sigma({\\mathcal }_2)\\) są niezależne.Proof. Skorzystamy z twierdzenia Dynkina o \\(\\pi-\\lambda\\) układach. Ustalmy zbiór\n\\(A_2\\{\\mathcal }_2\\) niech\n\\[\\mathcal{L} = \\Big\\{ :  \\mathbb{P}[\\cap A_2] = \\mathbb{P}[]\\mathbb{P}[A_2] \\Big\\}.\\]\nOczywiście \\({\\mathcal }_1\\subset \\mathcal{L}\\). Ponadto\n\\(\\mathcal{L}\\) jest \\(\\lambda\\)-układem:\\(\\Omega\\\\mathcal{L}\\);jeżeli \\(,B\\\\mathcal{L}\\) oraz \\(\\subset B\\), \n\\[ \\mathbb{P}\\big[(B\\setminus )\\cap A_2\\big] =\n  \\mathbb{P}\\big[B\\cap A_2\\big] -\n  \\mathbb{P}\\big[\\cap A_2\\big]\n  = \\mathbb{P}[B]\\mathbb{P}[A_2]- \\mathbb{P}[]\\mathbb{P}[A_2]\n  = \\mathbb{P}[B\\setminus ]\\mathbb{P}[A_2],\\]\nzatem \\(B\\setminus \\\\mathcal{L}\\)jeżeli \\(\\{B_k\\}\\) jest wstępującą rodziną zbiorów zawartych w \\(\\mathcal{L}\\), wówczas z ciągłości miary wiemy, że\n\\[\\mathbb{P}[B_k] \\\\mathbb{P}[B]\\] dla $ B=_{k=1}^{}B_k$. Podobnie \\(\\{B_k\\cap A_2\\}\\) jest rodziną wstępującą, więc\n\\[\\mathbb{P}[B_k\\cap A_2] \\\\mathbb{P}[B\\cap A_2].\n  \\]\nWynika stąd (odwołując się lematu o ciągłości miary)\n\\[ \\mathbb{P}[B\\cap A_2] \\overset{k\\\\infty}{\\leftarrow}\n  \\mathbb{P}[B_k\\cap A_2]  =  \\mathbb{P}[B_k]\\mathbb{P}[ A_2]\n  \\overset{k\\\\infty}{\\} \\mathbb{P}[B]\\mathbb{P}[ A_2]\n\\]\nwięc \\(B\\\\mathcal{L}\\).\\(\\mathcal{L}\\) jest więc \\(\\lambda\\)-układem zawierającym \\(\\pi\\)-układ \\({\\mathcal }_1\\). Zatem z twierdzenia Dynkina\n\\[\n\\sigma(  {\\mathcal }_1 ) \\subset \\mathcal{L},\n  \\] z kolei pokazuje, że \\(\\sigma({\\mathcal }_1)\\) \\({\\mathcal }_2\\) są niezależne.Następnie ustalamy zbiór\n\\(A_1\\\\sigma({\\mathcal }_1)\\) \ndefiniujemy\n\\[\\mathcal{L}_2 = \\Big\\{ :  \\mathbb{P}[A_1 \\cap ] = \\mathbb{P}[A_1]\\mathbb{P}[] \\Big\\}.\\]\npowtarzamy powyższe rozumowanie otrzymujemy tezę.Proof (Dowód twierdzenia 0-1 Kołmogorowa). W dowodzie pokażemy, że \\(\\sigma\\)-ciała \\(\\mathcal{F}_{\\infty}\\) oraz \\(\\sigma(\\mathcal{F}_1,\\mathcal{F}_2,\\ldots)\\) są niezależne.\nZauważmy, że wówczas zbiór \\(\\) należy obu powyższych \\(\\sigma\\)-ciał:\n\\[\n\\\\mathcal{F}_{\\infty} \\subset \\sigma(\\mathcal{F}_1,\\mathcal{F}_2,\\ldots).\\]\nZatem zbiór \\(\\) jest niezależny od samego siebie, tzn.\n\\[\n  \\mathbb{P}[\\cap ] = \\mathbb{P}[]\\mathbb{P}[].\n  \\] Powyższe równanie jest równoważne \\(\\mathbb{P}[] = \\mathbb{P}[]^2\\), jedynymi rozwiązaniami tego właśnie równania są: \\(\\mathbb{P}[]=0\\) lub \\(\\mathbb{P}[]=1\\).Krok 1. Pokażemy, że dla dowolnego \\(k\\), \\(\\sigma\\)-ciała \\(\\sigma(\\mathcal{F}_1,\\ldots,\\mathcal{F}_k)\\) \\(\\sigma(\\mathcal{F}_{k+1},\\mathcal{F}_{k+2},\\ldots)\\) są niezależne.Zauważmy, że\n\\[\n  \\sigma(\\mathcal{F}_{k+1},\\mathcal{F}_{k+2},\\ldots) = \\sigma \\bigg(\n  \\bigcup_{j\\ge 1}\\sigma(\\mathcal{F}_{k+1},\\mathcal{F}_{k+2},\\ldots, \\mathcal{F}_{k+j })\n  \\bigg)\n  \\] powyższa suma jest \\(\\pi\\)-układem. Korzystając więc z lematu, dla każdego \\(k\\), \\(\\sigma\\)-ciała\n\\[ \\sigma(\\mathcal{F}_1,\\ldots,\\mathcal{F}_k),  \\sigma(\\mathcal{F}_{k+1},\\mathcal{F}_{k+2},\\ldots)\\] są niezależne.\\[1ex]\nKrok 2. Zbiory\n\\[\n\\bigcup_k \\sigma(\\mathcal{F}_1,\\ldots,\\mathcal{F}_k), \\qquad \\mathcal{F}_{\\infty}\n\\] z punktu 1 są niezależne (bo \\(\\mathcal{F}_{\\infty} \\subset \\sigma(\\mathcal{F}_{k+1},\\mathcal{F}_{k+2},\\ldots)\\), wiemy\njuż, że \\(\\sigma(\\mathcal{F}_1,\\ldots,\\mathcal{F}_k)\\) \\(\\sigma(\\mathcal{F}_{k+1},\\mathcal{F}_{k+2},\\ldots)\\) są niezależne), ponadto\nsą \\(\\pi\\)-układami, więc z lematu \\(\\sigma\\)-ciała przez nie generowane:\n\\[\n\\sigma(\\mathcal{F}_1,\\mathcal{F}_2,\\ldots), \\quad  \\mathcal{F}_{\\infty}\\]\nsą niezależne.Wniosek 19.1  Jeżeli \\(\\{X_n\\}\\) jest ciągiem niezależnych zmiennych losowych, togranica \\(\\lim_{n\\\\infty} X_n\\) istnieje z prawdopodobieństwem \\(0\\) lub \\(1\\).szereg \\(\\sum_{n=1}^\\infty X_n\\) jest zbieżny z prawdopodobieństwem \\(0\\) lub \\(1\\).\\(\\mathbb{P}[\\{\\limsup X_n = \\infty\\}]= 0\\) lub \\(1\\).\\(\\mathbb{P}[\\{\\lim_{n\\\\infty} (X_1+X_2+\\ldots + X_n)/n < \\infty\\}]= 0\\) lub \\(1\\).","code":""},{"path":"mocne-prawo-wielkich-liczb.html","id":"mocne-prawo-wielkich-liczb","chapter":"20 Mocne prawo wielkich liczb","heading":"20 Mocne prawo wielkich liczb","text":"Celem wykładu jest pokazanie jednego z kluczowych wyników w rachunku prawdopodobieństwa:\nMocnego Prawa Wielkich Liczb. Mówi ono, że\njeżeli danych jest ciąg zmiennych losowych \\(\\{X_n\\}_{n\\\\mathbb{N}}\\)\no takim samym rozkładzie takich, że \\(\\mathbb{E}|X_n|<\\infty\\),\n\\[\n\\frac{X_1 + \\ldots + X_n}{n} \\\\mathbb{E} [X_1],\\qquad p.n.\n\\]","code":""},{"path":"mocne-prawo-wielkich-liczb.html","id":"twierdzenie-o-dwóch-szeregach","chapter":"20 Mocne prawo wielkich liczb","heading":"20.1 Twierdzenie o dwóch szeregach","text":"Twierdzenie 20.1  (Nierówność Kołmogorowa) Niech \\(X_1,\\ldots, X_n\\) będą niezależnymi zmiennymi losowymi takimi, że\n\\(\\mathbb{E} [X_i] = 0\\) \\(\\mathbb{E}\\left[ X_i^2\\right]<\\infty\\). Wtedy dla dowolnego \\(t>0\\)\n\\[\n  \\mathbb{P}\\left[ \\max_{1\\le k\\le n} |X_1+\\ldots + X_k| \\ge t\n  \\right] \\le \\frac 1{t^2} \\mathbb{V}ar(X_1+\\ldots + X_n).\n  \\]\nAndrey Nikolaevich Kolmogorov\nProof. Oznaczmy \\(S_0 = 0\\), \\(S_k = X_1+\\ldots + X_k\\). Zdefiniujmy zdarzenia\n\\[ A_k = \\big\\{ |S_j| < t \\mbox{ dla } j<k, |S_k|\\ge t\n  \\big\\}.\n  \\] Są zbiory tych trajektorii, które dokładnie w \\(k\\)-tym kroku przekraczają poziom \\(t\\). Wówczas \\(A_k\\\\sigma\\{X_1,\\ldots, X_k\\}\\). Ponadto zbiory \\(A_k\\) są rozłączne parami oraz\n\\[\\begin{equation}\\label{eq:abk}\n  \\bigcup_{k=1}^n A_k = B = \\left\\{ \\max_{1\\le k \\le n}  |S_k| \\ge t \\right\\}.\n  \\end{equation}\\]\nWtedy\n\\[\\begin{align*}\n    \\mathbb{V}ar [S_n] & =\\mathbb{E}\\left[ S_n^2\\right] = \\int _{\\Omega} S_n^2 \\mathrm{d}\\mathbb{P} \\ge \\int_B S_n^2 \\mathrm{d}\\mathbb{P}\\\\\n    &= \\sum_{k=1}^n \\int_{A_k} S_n^2 \\mathrm{d}\\mathbb{P} = \\sum_{k=1}^n \\int_{A_k} (S_k+S_n-S_k)^2 \\mathrm{d}\\mathbb{P}\\\\\n    &=\\sum_{k=1}^n \\bigg( \\int_{A_k} S_k^2 \\mathrm{d}\\mathbb{P} + 2\\int_{A_k}(S_n-S_k)S_k \\mathrm{d}\\mathbb{P} + \\int_{A_k}(S_n-S_k)^2 \\mathrm{d}\\mathbb{P}\\bigg)\\\\\n    &\\ge \\sum_{k=1}^n \\bigg(  \\int_{A_k} S_k^2 \\mathrm{d}\\mathbb{P} + 2\\int (S_n-S_k) S_k {\\bf 1}_{A_k}\\mathrm{d}\\mathbb{P}\n    \\bigg).\n  \\end{align*}\\]\nZmienne losowe \\(S_n-S_k\\) oraz \\(S_k{\\bf 1}_{A_k}\\) są niezależne zatem\n\\[\\begin{multline*}\n  \\int (S_n-S_k) S_k {\\bf 1}_{A_k}d\\mathbb{P} = \\mathbb{E}\\big[ (S_n-S_k)S_k{\\bf 1}_{A_k} \\big]\\\\\n  = \\mathbb{E}\\big[ S_n-S_k\\big]\\mathbb{E}\\big[S_k{\\bf 1}_{A_k} \\big] = \\mathbb{E}\\big[S_k{\\bf 1}_{A_k} \\big] \\cdot \\sum_{j=k+1}^n \\mathbb{E} X_j = 0.\n  \\end{multline*}\\]\nstąd wynika\n\\[\n  \\mathbb{V}ar[ S_n] \\ge \\sum_{k=1}^n \\int_{A_k} S_k^2d\\mathbb{P} \\ge \\sum_{k=1}^n \\int_{A_k} t^2 d\\mathbb{P}\n  \\ge t^2 \\int_B d\\mathbb{P} = t^2 \\mathbb{P}[B]\n  \\]Twierdzenie 20.2  (Twierdzenie Kołmogorowa o dwóch szeregach) Załóżmy, że \\(X_1,X_2,...\\) jest ciągiem niezależnych zmiennych losowych takich, że \\(\\mathbb{E}\\left[ X_i^2\\right]<\\infty\\).\nJeżeli\n\\[  \\sum_{=1}^\\infty \\mathbb{E} [X_i] <\\infty \\quad \\mbox{oraz}  \\quad \\sum_{=1}^\\infty \\mathbb{V}ar[ X_i]<\\infty,\\] \n\\[\\sum_{=1}^\\infty X_i <\\infty \\quad \\mbox{p.n.}\\]Przykład 20.1  Niech \\(\\{X_n\\}_{n \\\\mathbb{N}}\\) będzie ciągiem niezależnych zmiennych losowych takich, że dla każdego \\(n\\) zmienna \\(X_n\\)\nma rozkład \\(\\mathcal{N}(n^{-}, n^{-b})\\) dla pewnych \\(, b \\(0,2)\\). Wówczas szereg \\(\\sum_nX_n\\) jest zbieżny wtedy tylko wtedy, gdy \\(,b>1\\).\nPoniżej widzimy realizację ciągu sum częściowych tego szeregu.Co się dzieje dla małych wartości \\(\\)? Co się dzieje dla małych wartości \\(b\\)?Remark. W założeniach Twierdzenia o dwóch szeregach warunek\n\\[  \n   \\sum_{=1}^\\infty \\mathbb{E} [X_i] <\\infty\n\\]\nkontroluje trend ciągu sum częściowych (który musi być ograniczony, aby suma nie była \\(\\pm \\infty\\)).\nNatomiast założenie\n\\[\n\\sum_{=1}^\\infty \\mathbb{V}ar[ X_i]<\\infty\n\\]\nkontroluje oscylacje ciągu sum częściowych.Przykład 20.2  Czy szereg \\(\\sum \\frac {U_n}n\\) jest zbieżny, gdzie \\(U_n\\) jest\nciągiem niezależnych zmiennych losowych takich, że \\(U_n = \\pm 1\\) z prawdopodobieństwem \\(1/2\\)?\nZdefiniujmy \\(X_n = \\frac{U_n}{n}\\). Wówczas\n\\(\\mathbb{E} [X_n] = 0\\), \\(\\mathbb{V}ar[ X_n] = \\frac 1{n^2}\\).\nZ powyższego twierdzenia wnioskujemy więc, że szereg \\(\\sum  \\frac {U_n}n\\) jest zbieżny p.n.Proof. Możemy założyć, że \\(\\mathbb{E} [X_i] = 0\\),\nbo przy powyższych założeniach \\(\\sum X_i\\) jest zbieżny p.n. wtedy tylko wtedy, gdy \\(\\sum_i (X_i - \\mathbb{E} [X_i])\\) jest zbieżny p.n.Niech \\(S_N = \\sum_{n=1}^N X_n\\). Chcemy pokazać, że \\(\\{S_N\\}_N\\) jest ciągiem Cauchy’ego.\nZ nierówności Kołmogorowa\n\\[\n  \\mathbb{P}\\left[ \\max_{M\\le m \\le N} |S_m-S_M| >\\varepsilon\n  \\right] \\le \\frac 1{\\varepsilon^2} \\mathbb{V}ar(S_N - S_M) = \\frac{1}{\\varepsilon^2} \\sum_{n=M+1}^N\\mathbb{V}ar (X_n).\n  \\]\nPrzechodząc z \\(N\\) nieskończoności korzystając z ciągłości miary, otrzymujemy\n\\[\n  \\mathbb{P}\\left[ \\sup _{m\\ge M} |S_m-S_M| >\\varepsilon \\right] \\le \\frac 1{\\varepsilon^2}\\sum_{n=M+1}^\\infty \\mathbb{V}ar(X_n)\n  \\]\nPrzypomnijmy, że \\(\\sum_n \\mathbb{V}ar [X_n]<\\infty\\), więc przechodząc z \\(M\\\\infty\\),\ndla każdego \\(\\varepsilon > 0\\)\n\\[\\begin{align*}\n&\\mathbb{P}\\left[ \\sup _{n,m\\ge M} |S_m-S_n| > 2 \\varepsilon \\right] \\le \\\\\n& \\mathbb{P}\\left[ \\sup _{n\\ge M} |S_m-S_M| >  \\varepsilon \\right] +\\\\\n&\\mathbb{P}\\left[ \\sup _{m\\ge M} |S_n-S_M| >  \\varepsilon \\right]\n\\overset{M\\\\infty}{\\} 0.\n  \\end{align*}\\]\nPowyższy warunek przypomina definicję Cauchy’ego zbieżności ciągów, ale otrzymaliśmy zbieżność według prawdopodobieństwa,\nnie punktową.\nOznaczmy \\[W_M = \\sup_{m,n \\ge M} |S_n - S_m|.\\] Wtedy \\(W_M \\overset{\\mathbb{P}}{\\} 0\\).\nSkoro \\(W_M\\) jest niemalejący, jest zbieżny p.n. Z jedyności granica musi być równa \\(0\\) p.n.\npokazuje, że \\(S_n\\) jest ciągiem Cauchy’ego p.n., więc ciąg \\(S_n\\) jest zbieżny p.n.\n□Lemma 20.1  (Lemat Kroneckera) Załóżmy, że \\(a_n\\) jest ciągiem liczbowym takim, że szereg \\(\\sum_{n=1}^\\infty \\frac{a_n}n\\) jest zbieżny. Wtedy\n\\[\n   \\lim_{n\\\\infty} \\frac{a_1+\\ldots+a_n}{n} = 0.\n   \\]\nLeopold Kronecker (1823-1891)\nProof. Niech \\(S_n = \\sum_{k=1}^n \\frac{a_k}k\\), wtedy \\(a_n = n(S_n - S_{n-1})\\). Skorzystamy z lematu (zadanie z analizy ) mówiącego, że\njeżeli \\(S_n\\S\\), również średnie arytmetyczne zbiegają \\(S\\):\n\\[\n   \\frac{S_1 +\\ldots + S_n}{n} \\S.\n   \\]\nWówczas\n\\[\\begin{align*}\n     \\frac{a_1+\\ldots+a_n}{n} & = \\frac{S_1 + 2(S_2-S_1) + \\ldots + n(S_n - S_{n-1})}{n}\\\\\n     &= \\frac{nS_n - S_1 - S_2 -\\ldots - S_{n-1}}{n} \\0\n   \\end{align*}\\]\n□Twierdzenie 20.3  (Mocne Prawo Wielkich Liczb Kołmogorowa) Załóżmy, że \\(\\{X_n\\}\\) jest ciągiem niezależnych zmiennych losowych o tym samym rozkładzie.Jeżeli \\(\\mathbb{E} [|X_1|]<\\infty\\) \\(m = \\mathbb{E} [X_1]\\), \n\\[\n     \\lim_{n\\\\infty} \\frac{X_1+\\ldots +X_n}n = m,\\qquad \\mbox{p.n.}\n     \\]Jeżeli \\(\\mathbb{E} [|X_1|]=\\infty\\), \n\\[\n     \\mathbb{P}\\bigg[ \\limsup_{n\\\\infty} \\bigg| \\frac{X_1+\\ldots + X_n}{n} \\bigg| = \\infty\n     \\bigg] = 1.\n     \\]Proof. przy dodatkowym założeniu \\(\\mathbb{E} X_1^2<\\infty\\). Przy silniejszym warunku MPWL wynika natychmiast z poprzedniego twierdzenia Kołmogorowa o dwóch szeregach. Mianowicie zauważmy, że\n\\[\n\\sum_n  \\mathbb{V}ar\\bigg(\\frac{X_n-m}n\\bigg) = \\sum_n \\frac{\\mathbb{V}ar X_n}{n^2} = \\mathbb{V}ar X_1 \\sum_n \\frac{1}{n^2} <\\infty\n  \\]\noraz\n\\[\n\\sum_n \\mathbb{E}\\bigg[\\frac{X_n-m}n\\bigg]=0.\n   \\] Zatem szereg \\(\\sum_{n=1}^\\infty \\frac{X_n-m}n\\) jest zbieżny p.n. Korzystając następnie z lematu Kroneckera otrzymujemy\n\\[ 0 \\leftarrow \\frac{X_1+\\ldots + X_n - nm}n = \\frac{X_1 + \\ldots + X_n}n - m \\qquad \\mbox{p.n.}\n   \\]\ndowodzi MPWL w tym szczególnym przypadku.\n□","code":""},{"path":"mocne-prawo-wielkich-liczb.html","id":"dowód-mpwl-w-ogólnym-przypadku","chapter":"20 Mocne prawo wielkich liczb","heading":"20.2 Dowód MPWL w ogólnym przypadku","text":"Zdefiniujmy\n\\[\nX_n' = {\\bf 1}_{(-n,n)}(X_n) = \\left\\{\n\\begin{array}{cc}\n  X_n & \\ \\mbox{jeżeli } |X_n| < n \\\\\n  0 &  \\ \\mbox{jeżeli } |X_n| \\ge  n \\\\\n\\end{array}\n\\right.\n\\] Zmienne losowe \\(X_n'\\) są niezależne oraz ograniczone, więc również \\(\\mathbb{E} (X_n')^2<\\infty\\). Mamy\n\\[\\begin{multline*}\n  \\frac{X_1+\\ldots + X_n}n  - m\n=   \\frac{(X_1+\\ldots + X_n) - (X'_1+\\ldots + X'_n)}{n}\\\\\n+   \\frac{(X'_1+\\ldots + X'_n) - ( \\mathbb{E} X'_1+\\ldots + \\mathbb{E} X'_n)}{n}\n+  \\frac{\\mathbb{E} X'_1+\\ldots \\mathbb{E} X'_n}n  - m \\\\\n  = I_n + II_n + III_n.\n\\end{multline*}\\]\nPokażemy, że wszystkie powyższe wyrażenia zbiegają \\(0\\) p.n.Z twierdzenia Lebesgue’o zbieżności zdominowanej\n\\[\\mathbb{E} X_n' = \\mathbb{E} \\big[ X_n {\\bf 1}_{(-n,n)}(X_n)  \\big] = \\mathbb{E} \\big[ X_1 {\\bf 1}_{(-n,n)} (X_n) \\big] \\\\mathbb{E} X_1 = m,\\]\nzatem \\(III_n\\0\\) (ponownie korzystamy z lematu mówiącego, że jeżeli ciąg jest zbieżny, średnia arytmetyczna zbiega tej samej granicy).Badamy teraz \\(I_n\\). Pokażemy, że ciągi \\(\\{X_n\\}\\) oraz \\(\\{X'_n\\}\\) od pewnego miejsca się zgadzają.\nW tym celu skorzystamy z lematu Borela-Cantellego. Zauważmy najpierw\n\\[\n  \\sum_{n=1}^\\infty\\mathbb{P}[ X_n\\= X_n' ]  = \\sum_{n=1}^\\infty \\mathbb{P}[|X_n|\\ge n] = \\sum_{n=1}^\\infty \\mathbb{P}[|X_1|\\ge n]\n\\] Przypomnijmy, że jeżeli zmienna losowa przyjmuje dyskretne wartości, powyższa suma jest równa \\(\\mathbb{E} |X_1|\\). W ogólnej sytuacji nie zachodzi równość, ale obie wartości są ze sobą porównywalne, więc założenie \\(\\mathbb{E} |X_1|<\\infty\\) powinno implikować zbieżność powyższego szeregu. Poniższe rachunki pokazują, że rzeczywiście zachodzi:\n\\[\\begin{align*}\n  \\sum_{n=1}^\\infty\\mathbb{P}[ X_n\\= X_n' ] & = \\sum_{n=1}^\\infty \\mathbb{P}[|X_n|\\ge n]\n  = \\sum_{n=1}^\\infty \\mathbb{P}[|X_1|\\ge n]\\\\\n  &= \\sum_{n=1}^\\infty \\sum_{k=n}^{\\infty} \\mathbb{P}[k\\le |X_1| < k+1]\n  = \\sum_{k=1}^\\infty \\sum_{n=1}^k \\mathbb{P}[k\\le |X_1| < k+1]\\\\\n  & = \\sum_{k=1}^\\infty k \\mathbb{P}[k\\le |X_1| < k+1] \\le \\sum_{k=1}^\\infty \\int_{\\{k\\le |X_1|< k+1\\}} k d\\mathbb{P}\\\\\n  &  \\le \\sum_{k=1}^\\infty \\int_{\\{k\\le |X_1|< k+1\\}} |X_1| d\\mathbb{P}\\le \\mathbb{E} |X_1| <\\infty.\n\\end{align*}\\] Zatem z lematu Borela-Cantellego z prawdopodobieństwem \\(1\\) zachodzi tylko skończenie wiele zdarzeń \\(\\{X_n\\= X_n'\\}\\), więc\np.n. od pewnego miejsca \\(X_n = X'_n\\). Zatem \\[I_n\n= \\frac{(X_1-X'_1)+\\ldots + (X_n -X'_n)}{n} \\0\\quad \\mbox{p.n.}\\]Pozostaje składnik \\[II_n =\n\\frac{(X'_1 - \\mathbb{E} X'_1) +\\ldots + (X'_n- \\mathbb{E} X'_n)}{n}\n\\] Z lematu Kroneckera wystarczy pokazać, że suma \\[\\sum_{n=1}^\\infty \\frac{X_n' - \\mathbb{E} X_n'}{n}\\] jest zbieżna p.n. Skorzystamy z poprzedniego\ntwierdzenia Kołmogorowa o dwóch szeregach, które mówi, że wystarczy pokazać, że szereg wariancji jest zbieżny:\n\\[\\begin{align*}\n  \\sum_{n=1}^\\infty \\mathbb{V}ar\\bigg( \\frac{X_n' - \\mathbb{E} X_n'}n \\bigg) & = \\sum_{n=1}^\\infty \\mathbb{V}ar \\bigg( \\frac{X_n'}n\\bigg)\\\\\n  &= \\sum_{n=1}^\\infty \\frac 1{n^2} \\big( \\mathbb{E} (X_n')^2 - (\\mathbb{E} X_n')^2  \\big) \\le \\sum_{n=1}^\\infty \\frac 1{n^2}\\mathbb{E} (X_n')^2\\\\\n  &= \\sum_{n=1}^\\infty \\frac 1{n^2} \\sum_{k=1}^\\infty \\int_{\\{ k-1 \\le |X_n'|<k \\}} (X_n')^2 d\\mathbb{P}\\\\\n  &= \\sum_{n=1}^\\infty \\frac 1{n^2} \\sum_{k=1}^n \\int_{\\{ k-1 \\le |X_1|<k \\}} (X_1)^2 d\\mathbb{P}\n  \\le  \\sum_{n=1}^\\infty \\frac k{n^2} \\sum_{k=1}^n \\int_{\\{ k-1 \\le |X_1|<k \\}} |X_1| d\\mathbb{P}\\\\\n  &= \\sum_{k=1}^\\infty \\sum_{n=k}^{\\infty} \\frac k{n^2} \\mathbb{E} \\big[|X_1|{\\bf 1}_{[k-1,k)}(|X_1|)  \\big]\\\\\n  &\\le \\sum_{k=1}^\\infty \\bigg( \\sum_{n=k}^{\\infty}  \\frac 1{n^2} \\bigg) k \\mathbb{E} \\big[|X_1|{\\bf 1}_{[k-1,k)}(|X_1|)  \\big]\\\\\n  & \\le 2 \\sum_{k=1}^\\infty  \\mathbb{E} \\big[|X_1|{\\bf 1}_{[k-1,k)}(|X_1|)  \\big] = 2\\mathbb{E}|X_1| <\\infty.\n\\end{align*}\\]\nW przedostatniej nierówności skorzystaliśmy z nierówności znanej z analizy :\n\\[\n\\sum_{n=k}^{\\infty}  \\frac 1{n^2} \\le \\frac 2k.\n\\]\nDowód pkt 1. jest więc kompletny.Dowód pkt. 2 Załóżmy, że \\(\\mathbb{E} |X_1|=\\infty\\). Chcemy pokazać, że ciąg \\(\\{\\big|\\frac{S_n}n\\big|\\}\\) nie może być ograniczony.\nPokażemy najpierw, że ciąg \\(X_n\\) nieskończenie wiele razy przyjmuje duże wartości, większe niż \\(na\\) dla\ndowolnego \\(>0\\). W tym celu piszemy\n\\[\\begin{align*}\n  \\sum_{n}\\mathbb{P}[|X_n| > na] &  =  \\sum_{n}\\mathbb{P}[|X_1| > na]\\\\\n  &  = \\sum_{n=1}^\\infty \\sum_{k=n}^\\infty \\mathbb{P}[ka  < |X_1| \\le (k+1) ]\\\\\n  &  = \\sum_{k=1}^\\infty \\sum_{n\\le k} \\mathbb{P}[ka  < |X_1| \\le (k+1) ]\\\\\n  &  = \\frac 1a \\sum_{k=1}^\\infty ka   \\mathbb{P}[ka  < |X_1| \\le (k+1) ]\\\\\n  &  = \\frac 1a \\sum_{k=1}^\\infty \\int_{\\{ ak< |X_1| \\le (k+1) \\}} ka d\\mathbb{P} \\\\\n  &  \\ge \\frac 1a \\sum_{k=1}^\\infty  \\int_{\\{ ak< |X_1| \\le (k+1) \\}} (|X_1|-) d\\mathbb{P} \\\\\n  &\\ge \\frac 1a ( \\mathbb{E}|X_1|-2a) = \\infty.\n\\end{align*}\\]\nZatem korzystając z lematu Borela - Cantelliego\n\\[\n\\mathbb{P}[|X_n|>na\\ \\mbox{.o}] = 1,\n\\] więc z em 1, \\(|X_n|>na\\) dla nieskończenie wielu indeksów \\(n\\). Wówczas \\[\\mathbb{P}\\big[ |S_n-S_{n-1}| > na \\ \\ {\\mbox .o}\\big]  =1,\\] zatem\n\\[\n\\mathbb{P}\\bigg[ \\bigg| \\frac{S_n}n\\bigg| > \\frac a2\\ \\ \\ \\mbox{ lub }\\ \\ \\  \\bigg|\\frac{S_{n-1}}n\\bigg| > \\frac a2 \\ \\ \\ \\mbox{.o.}\\bigg] = 1\n\\] z kolei implikuje, że\n\\[\n\\mathbb{P}[|S_n/n| > /3\\ \\mbox{.o}] = 1.\n\\]\nZ dowolności \\(\\) wynika, że ciąg \\(\\{S_n/n\\}\\) niekończenie wiele razy przekracza dowolnie dużą wartość, więc\n\\[\n\\mathbb{P}\\bigg[\\limsup_{n\\\\infty} \\bigg|\\frac{S_n}n\\bigg| = \\infty \\bigg] = 1.\n\\]","code":""},{"path":"zastosowania-mpwl.html","id":"zastosowania-mpwl","chapter":"21 Zastosowania MPWL","heading":"21 Zastosowania MPWL","text":"","code":""},{"path":"zastosowania-mpwl.html","id":"metoda-monte-carlo","chapter":"21 Zastosowania MPWL","heading":"21.1 Metoda Monte Carlo","text":"Jednym z twórców tej metody był Stanisław Ulam. Problem jest następujący. Chcemy obliczyć całkę \\(\\int_0^1 f(x)dx\\), ale jest trudne, np. nie potrafimy znaleźć funkcji pierwotnej.Niech \\(X_i\\) będą niezależnymi zmiennymi losowymi o wartościach w \\((0,1)\\) rozkładzie z zadaną gęstością \\(g\\)\n(można przyjąć \\(g=1\\)). Wówczas z MPWL\n\\[\nS_n:= \\frac 1n\\sum_{=1}^n \\frac {f(X_i)}{g(X_i)} \\\\mathbb{E} \\frac{f(X_1)}{g(X_1)} = \\int_0^1 \\frac{f(x)}{g(x)}\\; g(x)dx = \\int_0^1 f(x)dx.\n\\]\nDla przykładu, jeżeli \\(g=1\\), możemy wygenerować ciąg niezależnych zmiennych losowych o rozkładzie jednostajnym na odcinku \\((0,1)\\) obliczyć\n\\(S_n\\) dla dużej wartości \\(n\\) aproksymując w ten sposób wartość całki.W praktyce warto używać funkcji \\(g\\), ponieważ dobierając ją kształtu funkcji \\(f\\) można przyśpieszyć zbieżność. W jednym wymiarze metody numeryczne są efektywniejsze (chyba, że funkcja \\(f\\) jest nieregularna). Jednak w wyższych wymiarach metoda Monte Carlo jest skuteczniejsza, gdyż tempo zbieżności zawsze jest rzędu \\(c/\\sqrt n\\) (wynika z kolei z Centralnego Twierdzenia Granicznego), podczas gdy metody numeryczne w wyższych wymiarach są wolniejsze.Przykład 21.1  Chcemy poznać przybliżoną wartość całki\n\\[\\begin{equation*}\n    \\int_{-1}^1 e^{-x^2} \\mathrm{d}x.\n\\end{equation*}\\]\nW tym celu rozważmy ciąg \\(\\{U_{n}\\}_{n \\\\mathbb{N}}\\) niezależnych zmiennych losowych o rozkładzie jednostajnym na przedziale \\([-1,1]\\).\nWówczas\n\\[\\begin{equation*}\n    \\mathbb{E} \\left[ e^{-U_1^2} \\right] =\n    \\frac 12\\int_{-1}^1 e^{-x^2} \\mathrm{d}x.\n\\end{equation*}\\]\nZ mocnego prawa wielkich liczb\n\\[\\begin{equation*}\n    \\frac 2n \\sum_{j=1}^n e^{-U_j^2} \\\n    \\int_{-1}^1 e^{-x^2} \\mathrm{d}x.\n\\end{equation*}\\]Przybliżona wartość całki: 1.494209Dokładna wartość całki: 1.493648Przykład 21.2  (Igła Buffona) Igłę o długości \\(l\\) rzucamy na podłogę z desek o szerokości \\(\\) (\\(l\\le \\)). Wiemy, że\no, przecięcia przez igłę krawędzi deski wynosi \\(p = \\frac{2l}{\\pi}\\). Tej formuły użyto wyznaczenia liczby \\(\\pi\\). Mianowicie wykonujemy \\(n\\) rzutów igłą dla bardzo dużego \\(n\\) \ndefiniujemy ciąg zmiennych losowych\n\\[\nX_i = \\left\\{ \\begin{array}{cc}\n                1 & \\mbox{ jeżeli w $$-tym rzucie igła przetnie krawędź} \\\\\n                0 & \\mbox{w przeciwnym razie.}\n              \\end{array}\n\\right.\n\\] Wówczas zmienne losowe \\(X_i\\) są niezależne mają ten sam rozkład. Ponadto \\(\\mathbb{E} X_i = \\mathbb{P}[X_i=1] =  p\\). Z~MPWL mamy\n\\[\n\\frac kn = \\frac{\\mbox{liczba trafień w krawędź}}{n} = \\frac{X_1+\\ldots+X_n}{n} \\\\mathbb{E} X_1 = p = \\frac{2l}{\\pi},\n\\] stąd wynika, że\n\\[\n\\pi \\approx \\frac {2l}{} \\cdot \\frac nk\n\\] W 1901 Mario Lazzarini wykonał 3408 rzutów otrzymał\n\\[\n\\pi \\approx  \\frac{355}{113} = 3,141592{\\bf 9203}.\n\\] Przypomnijmy \\(\\pi = 3,141592{\\bf 65359}\\).Przykład 21.3  Rozważmy następujące zadanie: oblicz\n\\[\n\\lim_{n\\\\infty} \\int_0^1\\ldots \\int_0^1 \\frac{x_1^3+\\ldots+ x_n^3}{x_1+\\ldots+ x_n}dx_1\\ldots dx_n.\n\\]\nPowyższą granicę można obliczyć przy pomocy MPWL. Niech \\(\\{X_n\\}\\) będzie ciągiem niezależnych zmiennych losowych o rozkładzie \\(U(0,1)\\). Wówczas\n\\[\n\\int_0^1\\ldots \\int_0^1 \\frac{x_1^3+\\ldots+ x_n^3}{x_1+\\ldots+ x_n}dx_1\\ldots dx_n\n= \\mathbb{E} \\bigg[\\frac{X_1^3+\\ldots+ X_n^3}{X_1+\\ldots+ X_n}\\bigg].\n\\] Z MPWL\n\\[\n\\frac{X_1^3+\\ldots+ X_n^3}{X_1+\\ldots+ X_n} = \\frac{\\frac{X_1^3+\\ldots+ X_n^3}n}{\\frac{X_1+\\ldots+ X_n}n} \\\\frac{\\mathbb{E} X_1^3}{\\mathbb{E} X_1} = \\frac 12.\n\\] Podsumowując:\n\\[\\begin{multline*}\n\\lim_{n\\\\infty} \\int_0^1\\ldots \\int_0^1 \\frac{x_1^3+\\ldots+ x_n^3}{x_1+\\ldots+ x_n}dx_1\\ldots dx_n  \\\\\n= \\lim_{n\\\\infty} \\mathbb{E} \\bigg[\\frac{X_1^3+\\ldots+ X_n^3}{X_1+\\ldots+ X_n}\\bigg]\n\\overset{{\\mbox{tw. Leb.}}}{=} \\mathbb{E} \\bigg[\\lim_{n\\\\infty}  \\frac{X_1^3+\\ldots+ X_n^3}{X_1+\\ldots+ X_n}\\bigg] = \\frac 12.\n\\end{multline*}\\]\nPowyżej skorzystaliśmy z twierdzenia Lebesgue’o zbieżności zdominowanej. Zauważmy, że mogliśmy zrobić, gdyż funkcja podcałkowa jest mniejsza od 1.","code":"\n# Ustawienie liczby prób\nn <- 10000\n\n# Wygenerowanie n losowych wartości z rozkładu jednostajnego na [-1, 1]\nU <- runif(n, min = -1, max = 1)\n\n# Obliczenie wartości funkcji e^(-x^2) dla każdej wartości U\nf_values <- exp(-U^2)\n\n# Średnia z tych wartości to przybliżenie całki\n# Mnożymy przez długość przedziału (2)\nmonte_carlo_estimate <- mean(f_values) * 2  \n\n# Wynik\ncat('Przybliżona wartość całki:', monte_carlo_estimate)\n# Dla porównania: dokładna wartość (przybliżona numerycznie)\nexact_value <- integrate(function(x) exp(-x^2), -1, 1)$value\ncat(\"Dokładna wartość całki:\", exact_value, \"\\n\")"},{"path":"zastosowania-mpwl.html","id":"dystrybuanta-empiryczna","chapter":"21 Zastosowania MPWL","heading":"21.2 Dystrybuanta empiryczna","text":"Powtarzamy wielokrotnie pewne doświadczenie o nieznanym rozkładzie. Na podstawie otrzymanych wyników \\(X_1,\\ldots, X_n\\) chcemy wyznaczyć ich dystrybuantę \\(F\\).\nW tym celu definiujemy\n\\[F_n(x) = \\frac 1n \\sum_{=1}^n {\\bf 1}_{\\{X_n\\le x\\}}.\\] Powyższa funkcja \\(F_n(x) = F_n(x,\\omega)\\) nazywa się dystrybuantą empiryczną.\nZauważmy, że jest naturalna definicja, dla danego \\(x\\) zliczamy liczbę próbek mniejszych niż \\(x\\) uśredniamy wynik.\nPoniższe twierdzenie jest jednym z podstawowych wyników używanych w statystyce:Twierdzenie 21.1  (Gliwienko - Cantelli) Zachodzi\n\\[\n  \\sup_{x\\\\mathbb{R}} |F_n(x) - F(x)| \\0,\\qquad \\mbox{p.n.}\n  \\]\nValery Glivenko\n\nFrancesco Paolo Cantelli\nDowód twierdzenia pominiemy, zauważmy jednak, że dla zmiennej losowej \\(Y_k = {\\bf 1}_{\\{X_n\\le x\\}}\\) zachodzi\n\\[\\mathbb{E} Y_k = \\mathbb{P}[X_k \\le x] = F(x),\\] z MPWL\n\\[\nF_n(x) = \\frac 1n \\sum_{k=1}^{n} {\\bf 1}_{\\{ X_k \\le x\\}} = \\frac{Y_1+\\ldots + Y_n}{n} \\\\mathbb{E} Y_1 = F(x) \\quad \\mbox{p.n.}\n\\]","code":""},{"path":"zastosowania-mpwl.html","id":"liczby-normalne","chapter":"21 Zastosowania MPWL","heading":"21.3 Liczby normalne","text":"Liczbę \\(\\[0,1]\\) nazywamy normalną przy podstawie \\(d\\) (\\(d\\\\{2,3,\\ldots \\}\\)), jeżeli ma ona przedstawienie\n\\[\n= \\sum_{n=1}^{\\infty} \\frac{\\varepsilon_n}{d^n},\n\\] dla \\(\\varepsilon_n\\\\{0,1,\\ldots,d-1\\}\\) takie, że\n\\[\n\\frac{\\# \\{: \\varepsilon_i = k, \\ \\le n \\}}{n} \\\\frac 1d\n\\] dla każdego \\(k=\\{0,1,\\ldots, d-1\\}\\).Dosyć łatwo jest wskazać liczby normalne o ustalonej podstawie \\(d\\) (pozostawiamy jako zadanie). Jednak wskazanie liczby (obliczalnej), która jest normalna przy każdej podstawie \\(d\\) jest problemem otwartym. Tymczasem okazuje się, że prawie każda liczba ma tę własność:Twierdzenie 21.2  (Borel) Prawie wszystkie liczby (względem miary Lebesgue’) z przedziału \\([0,1]\\) są normalne względem każdej podstawy.\nÉmile Borel (1871-1956)\nProof. Niech\n\\[\\begin{align*}\n    A_d & = \\big\\{ x\\[0,1]: x \\mbox{ jest normalna przy podstawie $d$}\\big\\},\\\\\n    & =  \\bigcap_{d=2}^{\\infty} A_d =  \\big\\{ x\\[0,1]: x \\mbox{ jest normalna przy każdej podstawie $d$}\\big\\}.\n  \\end{align*}\\]\nRozważmy przestrzeń probabilistyczną \\(([0,1], \\mathcal{B}([0,1]), {\\rm Leb})\\).\nWystarczy pokazać, że dla każdego \\(d\\), \\(\\mathbb{P}[A_d]=1\\). Ustalmy \\(d\\). Każdą liczbę \\(x\\[0,1]\\) można zapisać w postaci\n\\[\nx = \\sum_{n=1}^{\\infty} \\frac{\\varepsilon_n(x)}{d^n},\n\\] dla \\(\\varepsilon_n(x)\\\\{0,1,\\ldots,d-1\\}\\).\nWtedy \\(\\varepsilon_n(x)\\) jest ciągiem niezależnych zmiennych losowych o rozkładzie\njednostajnym na \\(\\{ 0,1,\\ldots,d-1 \\}\\) (zadanie!). Ustalmy \\(k\\\\{0,1,\\ldots d-1\\}\\). Niech\n\\[X_n(x) = \\left\\{\n\\begin{array}{cc}\n  1, & \\mbox{ jeżeli } \\varepsilon_n(x)=k \\\\\n  0, & \\mbox{w przeciwnym razie.}\n\\end{array}\n\\right.\n\\] Wówczas \\(\\mathbb{E}[X_n] =\\mathbb{P}[X_n=1] = 1/d\\) z MPWL\n\\[\n\\frac{\\{ \\le n:\\; \\varepsilon_i(x) = k \\}}{n} = \\frac{X_1+\\ldots + X_n}{n} \\\\mathbb{E} X_1  = \\frac 1d\\qquad\\mbox{p.n.}\n\\] zatem \\(\\mathbb{P}[A_d]=1\\) oraz \\(\\mathbb{P}[]=1\\).","code":""},{"path":"zastosowania-mpwl.html","id":"wokół-mpwl","chapter":"21 Zastosowania MPWL","heading":"21.4 Wokół MPWL","text":"Twierdzenie 21.3  Jeżeli \\(X_n\\) jest ciągiem niezależnych zmiennych losowych o takim samym rozkładzie oraz istnieje stała \\(c\\) taka, że\n\\[\n  \\mathbb{P}\\bigg[ \\lim_n \\frac{X_1+\\ldots+X_n}{n} = c\n  \\bigg] >0,\n  \\] \\(\\mathbb{E}|X_1|<\\infty\\) oraz \\(c=\\mathbb{E} X_1\\).Proof. Z prawa 0-1 Kołmogorowa\n\\[\n  \\mathbb{P}\\bigg[ \\lim_n \\frac{X_1+\\ldots+X_n}{n} = c\n  \\bigg] = 1,\n  \\] stąd\n\\[\n  \\frac{X_n}{n} = \\frac{S_n}n - \\frac{S_{n-1}}{n-1}\\cdot \\frac{n-1}{n} \\0 \\qquad \\mbox{p.n.}\n  \\]\nZatem z em 1 zajdzie jedynie skończenie wiele zdarzeń \\(\\{|X_n| > n\\}\\). Lemat Borela-Cantelliego implikuje więc\n\\[\n  \\sum_{n=1}^\\infty \\mathbb{P}[|X_1|>n] = \\sum_{n=1}^\\infty \\mathbb{P}[|X_n|>n] <\\infty,\n  \\] stąd wynika, że\n\\[\\mathbb{E} |X_1| \\le   \\sum_{n=1}^\\infty \\mathbb{P}[|X_1|>n] <\\infty.\\]\nZ MPWL wynika więc\n\\[\n  \\frac{X_1+\\ldots+ X_n}{n} \\\\mathbb{E} X_1\\qquad \\mbox{p.n.}\n  \\]Twierdzenie 21.4  (MPWL, Etemadi) Niech \\(\\{X_n\\}\\) będzie ciągiem zmiennych losowych, które są {} niezależne mają jednakowy rozkład. Jeżeli \\(\\mathbb{E}|X_1|<\\infty\\), \n\\[\n  \\frac{X_1+\\ldots+ X_n}{n} \\\\mathbb{E} X_1\\qquad \\mbox{p.n.}\n  \\]Dla \\(c>0\\) niech \\(X^{(c)}\\) oznacza obcięcie zmiennej losowej \\(X\\):\n\\[\nX^{(c)} = \\left\\{\n\\begin{array}{cc}\n  X & \\mbox{ dla } |X|\\le c \\\\\n  0 & \\mbox{ dla } |X|>c\n\\end{array}\n\\right.\n\\]Twierdzenie 21.5  (Twierdzenie Kołmogorowa o 3 szeregach) Ustalmy \\(c>0\\). Szereg \\(\\sum_n X_n\\) niezależnych zmiennych losowych jest zbieżny p.n. wtedy tylko, gdy następujące 3 szeregi\n\\[\n  \\sum_{n=1}^\\infty \\mathbb{E} X_n^{(c)}, \\quad \\sum_{n=1}^{\\infty} \\var X_n^{(c)}, \\quad \\sum_{n=1}^{\\infty} \\mathbb{P}[|X_n|>c]\n  \\] są zbieżne.Remark. Jeżeli powyższe twierdzenie zachodzi dla pewnego \\(c_0\\), zachodzi również dla każdej innej wartości \\(c>0\\).Proof. Załóżmy, że wszystkie 3 powyższe szeregi są zbieżne. Wtedy z twierdzenie Kołmogorowa o 2 szeregach szereg\n$ _n {X_n^{(c)}}$ jest zbieżny p.n. Z warunku \\(\\sum_{n=1}^{\\infty} \\mathbb{P}[|X_n|>c]<\\infty\\) lematu Borela-Cantelliego\notrzymujemy\n\\[\n  \\mathbb{P}\\big[ |X_n| >c \\ \\mbox{.o}\\big] = 0.\n  \\] Zatem \\(|X_n|>c\\) tylko skończenie wiele razy, więc dla dostatecznie dużych indeksów \\(X_n = X_n^{(c)}\\). Zatem\n\\(\\sum X_n<\\infty\\) p.n.Dowód odwrotnej implikacji pomijamy.","code":""},{"path":"dygresja-kompresja.html","id":"dygresja-kompresja","chapter":"22 Dygresja: kompresja","heading":"22 Dygresja: kompresja","text":"","code":""},{"path":"dygresja-kompresja.html","id":"wprowadzenie-1","chapter":"22 Dygresja: kompresja","heading":"22.1 Wprowadzenie","text":"Załóżmy, że dysponujemy następującym słownikiem\n\\[\\begin{equation*}\n    \\mathcal{} = \\{,b,c,d,e,f,g,h\\}.\n\\end{equation*}\\]\nWówczas każdą wiadomość złożoną z jednego słowa możemy zakodować za pomocą trzech bitów.Podobnie, każdą wiadomość składającą się z \\(N \\\\mathbb{N}\\) słów możemy zakodować za pomocą \\(3N\\) bitów.\nZałóżmy, że każde słowo pojawia się z pewnym prawdopodobieństwemWówczas\n\\[\\begin{equation*}\n    \\mathbb{P}[,b,c,d]=15/16.\n\\end{equation*}\\]\nJeśli jesteśmy gotowi zaryzykować, możemy zakodować słowa za pomocą dwóch bitówWówczas z prawdopodobieństwem \\(15/16\\) jesteśmy w stanie poprawnie zakodować wiadomość.\nPrawdopodobieństwo, że wiadomość składająca się z jednego słowa nie zostanie odczytana wynosi \\(1/16\\).","code":""},{"path":"dygresja-kompresja.html","id":"długie-wiadomości","chapter":"22 Dygresja: kompresja","heading":"22.2 Długie wiadomości","text":"Chcemy teraz kodować długie wiadomości. Niech \\(M \\\\mathbb{N}\\). Zakładać będziemy, że nasz słownik jest postaci\n\\(\\mathcal{} = \\{1,2, \\ldots , M\\} = [M]\\). Dla \\(j \\\\mathbb{N}\\) niech \\(X_j\\) będzie \\(j\\)-tym słowem losowej wiadomości.\nZakładać będziemy, że zmienne \\(\\{X_j\\}_{j \\\\mathbb{N}}\\) są niezależne\nz rozkładem\n\\[\\begin{equation*}\n    \\mathbb{P}[X_j=k] = p_k\n\\end{equation*}\\]\ndla pewnych liczb \\(p_k \\[0,1]\\) takich, że \\(\\sum_{k=1}^Mp_k =1\\).\nNiech \\(N \\\\mathbb{N}\\). \\(N\\)-wymiarowy wektor losowy \\(\\vec{X} = (X_1, X_2, \\ldots, X_N)\\) jest internującą\nnas wiadomością, którą chcemy zakodować (zapisać) za pomocą możliwie małej liczby bitów.Jeżeli \\(\\vec{} = (a_1, \\ldots, a_N)\\) jest typową wiadomością, spodziewamy się, że\n\\[\\begin{equation*}\n\\mathrm{P}(\\vec{}) = \\mathbb{P}\\left[\\vec{X}=\\vec{}\\right] = p_{a_1}p_{a_2}\\cdots p_{a_N} \\sim p_1^{p_1N} \\cdot p_2^{p_2N} \\cdots p_M^{p_MN}\n\\end{equation*}\\]\nZatem\n\\[\\begin{equation*}\n    \\log_2\\left(1/\\mathrm{P}(\\vec{}) \\right) \\sim N \\sum_{k=1}^M p_k\\log_2(1/p_k).\n\\end{equation*}\\]\nLiczbę\n\\[\\begin{equation*}\n    H= \\sum_{k=1}^M p_k\\log(1/p_k)\n\\end{equation*}\\]\nnazywamy entropią rozkładu \\((p_1, \\ldots, p_M)\\). Dla \\(\\epsilon>0\\) rozważmy zbiór\n\\[\\begin{equation*}\n    T_{N,\\epsilon} = \\left\\{ \\vec{} \\\\mathcal{}^N \\: :\\: \\left| \\frac 1N \\log_2 (1/\\mathrm{P}(\\vec{})) -H \\right| <\\epsilon \\right\\}\n\\end{equation*}\\]\ntych słów długości \\(N\\), których prawdopodobieństwo jest między \\(2^{-N(H+\\epsilon)}\\) \\(2^{-N(H-\\epsilon)}\\).\nOkazuje się, że typowe wiadomości są w \\(T_{N,\\epsilon}\\). Rzeczywiście \\(\\vec{X} \\T_{N,\\epsilon}\\) wtedy tylko wtedy, gdy\n\\[\\begin{equation*}\n    \\left| \\frac 1N \\log_2 (1/\\mathrm{P}(\\vec{X})) -H \\right| <\\epsilon.\n\\end{equation*}\\]\nZauważmy, że\n\\[\\begin{equation*}\n    \\frac 1N \\log_2 (1/\\mathrm{P}(\\vec{X})) = \\frac 1N \\log_2 \\left( \\frac 1{p_{X_1}p_{X_2}\\cdots p_{X_N} } \\right)\n= \\frac 1N\\sum_{k=1}^N \\log_2(1/p_{X_j}).\n\\end{equation*}\\]\nKorzystając z Mocnego prawa wielkich liczb\n\\[\\begin{equation*}\n   \\frac 1N\\sum_{k=1}^N \\log_2(1/p_{X_j}) \\\\mathbb{E}\\left[ \\log_2(1/p_{X_1}) \\right]\n\\end{equation*}\\]\nprawie na pewno. Mamy\n\\[\\begin{equation*}\n    \\mathbb{E}\\left[ \\log_2(1/p_{X_1}) \\right] = \\sum_{k=1}^M \\mathbb{P}[X_1=k] \\log_2(1/p_{k}) = \\sum_{k=1}^M p_k\\log_2(1/p_k) = H.\n\\end{equation*}\\]\nPodsumowując, z prawdopodobieństwem jeden\n\\[\\begin{equation*}\n    \\frac 1N \\log_2 (1/\\mathrm{P}(\\vec{X})) \\H.\n\\end{equation*}\\]\nZauważmy, że\n\\[\\begin{equation*}\n1 \\geq \\mathbb{P}\\left[ \\vec{X} \\T_{N, \\epsilon} \\right] = \\sum_{\\vec{} \\T_{N,\\epsilon}} \\mathrm{P}(\\vec{})\n\\geq 2^{-N(H+\\epsilon)} \\left|T_{N,\\epsilon}\\right|.\n\\end{equation*}\\]\nStąd\n\\[\\begin{equation*}\n    \\left| T_{N,\\epsilon} \\right| \\leq 2^{N(H+\\epsilon)}.\n\\end{equation*}\\]\nzakodowania typowej wiadomości potrzebujemy więc \\(N(H+\\epsilon)\\) bitów. Zauważmy, że z nierówności Jensena\n\\[\\begin{equation*}\n    H = \\sum_{k=1}^M p_k \\log_2(1/p_k) \\leq \\log_2 \\left(\\sum_{k=1}^M 1 \\right) = \\log_2(M).\n\\end{equation*}\\]\nPrzy czym równość zachodzi jedynie w przypadku \\(p_k=1/M\\).\nJeżeli więc rozkład \\((p_1, \\ldots , p_M)\\) nie jest jednostajny, \n\\(H<\\log_2(M)\\). Wówczas dla dostatecznie małego \\(\\epsilon>0\\), \\(H+\\epsilon< \\log_2(M)\\). Wtedy\n\\[2^{N(H+\\epsilon)} \\ll 2^{N\\log_2(M)} = \\left|\\mathcal{}^N \\right|. \\]\nZ prawdopodobieństwem bliskim jeden, każdą wiadomość długości \\(N\\) jesteśmy w stanie zapisać za pomocą\n\\(N(H+\\epsilon)\\) bitów. Skoro \\(H+\\epsilon<\\log_2(M)\\) oznacza , że jesteśmy w stanie skompresować dane.Można pokazać, że \\(H\\) jest optymalne. Dokładniej nie jest możliwa kompresja \\(N(H-\\epsilon)\\) bitów. Wówczas\nprawdopodobieństwo poprawnego odczytania wiadomości jest oddalone od jeden.","code":""},{"path":"twierdzenie-de-moivrea-laplacea.html","id":"twierdzenie-de-moivrea-laplacea","chapter":"23 Twierdzenie de Moivre’a-Laplace’a","heading":"23 Twierdzenie de Moivre’a-Laplace’a","text":"Z MPWL wielkich liczb wynika, że\njeżeli zmienne \\(\\{X_k\\}_{k\\\\mathbb{N}}\\) są niezależne mają ten sam rozkład, dla \\(S_n = \\sum_{k=1}^nX_k\\) zachodzi\n\\(S_n/n \\\\mathbb{E} [X]\\) p.n. Chcemy teraz zrozumieć jak szybka jest ta zbieżność.\nZauważmy, że powyższe mówi, że z prawdopodobieństwem jeden, dla każdego \\(\\epsilon>0\\),\n\\[\\begin{equation*}\n    n(\\mathbb{E}[X_1] -\\epsilon ) \\leq S_n \\leq n(\\mathbb{E}[X_1] +\\epsilon).\n\\end{equation*}\\]\nCzyli wykres ciągu \\(\\{S_n\\}_{n \\\\mathbb{N}}\\) od pewnego miejsca będzie znajdował się między prostymi o nachyleniach\n\\(\\mathbb{E}[X_1] \\pm \\epsilon\\).Przykład 23.1  Jeżeli \\(X_j\\) mają rozkład jednostajny na przedziale \\([-1,3]\\), \\(\\mathbb{E}[X_j]=1\\).\nZatem dla dostatecznie dużego \\(n\\),\n\\[\\begin{equation*}\n0.9 n \\leq S_n \\leq 1.1n.\n\\end{equation*}\\]\nNa symulacji widzimy, że \\(S_n\\) oscyluje wokół prostej o nachyleniu \\(1 =\\mathbb{E}[X_1]\\).Przesuwając zmienne losowe, tzn. przyjmując \\(X' = X - \\mathbb{E} [X]\\), możemy założyć, że\n\\(\\mathbb{E} X = 0\\), więc\n\\(S_n/n \\0\\). Oznacza w szczególności, że \\(|S_n| \\le \\varepsilon n\\) od pewnego miejsca dla każdego \\(\\varepsilon>0\\).\nChcielibyśmy lepiej zrozumieć zachowanie \\(S_n\\) opisać dokładniej strukturę tego ciągu.\nInteresują nas podstawowe pytania. Jak szybko ciąg \\(S_n/n\\) zbiega zera, tzn. jak należy znormalizować \\(S_n\\),\naby otrzymać nietrywialną granicę? Jaka jest typowa odległość \\(S_n\\) od zera?\nNa ostatnie pytanie już teraz możemy odpowiedzieć już teraz.\nObliczenie \\(\\mathbb{E} |S_n|\\) jest kłopotliwe, ale policzmy drugi moment:\n\\[\n\\mathbb{E} [S_n^2] = \\mathbb{V}ar [S_n] = n \\mathbb{V}ar X_1 = n\\mathbb{E} \\left[X_1^2\\right],\n\\]\nwięc \\(S_n^2\\approx n\\), czyli właściwą normalizacją jest \\(|S_n|\\approx \\sqrt n\\).\nPowstaje więc kolejne pytanie. Czy ciąg \\(S_n/\\sqrt n\\) ma jakąś strukturę, którą można opisać.Rozważmy najprostszą możliwą sytuację. Naszym celem będzie teraz udowodnienie szczególnej wersji centralnego twierdzenia granicznego (CTG)\ndla niezależnych zmiennych losowych \\(\\{X_n\\}\\) o rozkładzie \\(X_n = \\pm 1\\) z em \\(1/2\\).\nPo łatwym przesunięciu (\\(Y_n = 2 X_n - 1\\)) twierdzenie pozwala analizować liczbę orłów w wielokrotnym rzucie monetą.\nOznaczmy przez \\(S_n = X_1+\\ldots + X_n\\).Przypomnijmy doświadczenie z deską GaltonaPokazuje ono, że każdym razem histogramy otrzymanych wyników układają się w krzywą Gaussa.\nBadany model jest stosunkowo łatwy. Przy pomocy wzorów kombinatorycznych potrafimy obliczyć \\(\\mathbb{P}[S_n = k]\\);\nprzypomnijmy, że \\((S_n+n)/2\\) ma rozkład \\({\\rm Bin}(n,1/2)\\).Chcemy więc pokazać, że przy odpowiedniej normalizacji prawdopodobieństwa te będą zbiegać funkcji Gaussa.\nJest treścią poniższego twierdzenia, pochodzącego z pierwszej połowy XVIII wieku:Twierdzenie 23.1  (de Moivre'- Laplace') Niech \\(\\{X_n\\}_{n \\\\mathbb{N}}\\) będzie ciągiem niezależnych zmiennych losowych takich,\nże \\(\\mathbb{P}[X_1=1] = \\mathbb{P}[X_1 = -1] = 1/2\\) niech\n\\(S_n = X_1+\\ldots + X_n\\). Wówczas dla dowolnych \\(<b\\)\n\\[\n  \\lim_{n \\\\infty}\\mathbb{P}\\bigg[ \\le \\frac{S_n}{\\sqrt n} \\le b \\bigg] =\n  \\frac 1{\\sqrt{2\\pi}}\\int_a^b e^{-\\frac{x^2}2}\\mathrm{d}x.\n  \\]\nAbraham de Moivre\n\nPierre-Simon Laplace\nProof. Przedstawiony poniżej dowód nie jest w pełni precyzyjny zawiera drobne luki\n(przejścia graniczne wymagają dokładniejszej wersji wzoru Stirlinga dokładniejszych szacowań).\nPokazuje jednak istotę dowodu, pozostałe uzupełnienia dosyć techniczne szczegóły można znaleźć w literaturze.Zacznijmy od oczywistej obserwacji, że w parzystych krokach spacer losowy \\(S_n\\) jest w parzystych punktach.\nNależy więc rozpatrzyć dwa podciągi po \\(n\\) parzystych oraz nieparzystych pokazać, że obie granice są identyczne.\nZałóżmy \\(n=2m\\), wtedy dla liczb nieparzystych mamy\n\\[\n  \\mathbb{P}[S_{2m} = 2k+1]=0.\n  \\]\nChcemy więc oszacować wartość\n\\[\\begin{equation*}%\\label{eq:ma1}\n    \\mathbb{P}\\bigg[ \\le \\frac{S_{2m}}{\\sqrt{2m}} \\le b \\bigg] =\n  \\sum_{x\\[,b]\\cap \\frac{2\\mathbb{Z}}{\\sqrt{2m}}} \\mathbb{P}\\big[ S_{2m} = x\\sqrt{2m} \\big],\n\\end{equation*}\\]\ngdzie w powyższej sumie \\(x\\) jest postaci \\(x = \\frac{2k}{\\sqrt{2m}}\\) ponadto \\(k\\) spełniają \\(|k|\\le m\\).\nPowinniśmy zatem opisać asymptotyczne zachowanie wyrażeń\n\\[\n  \\mathbb{P}\\big[ S_{2m} = 2k \\big],\n  \\] gdy \\(2k = x\\sqrt{2m}\\), \\(x\\) jest ustalone, \\(m\\) zbiega \\(\\infty\\) W dowodzie będziemy korzystać z formuły Stirlinga:\n\\[\n  n! \\sim \\sqrt{2\\pi n} \\cdot \\frac{n^n}{e^n}, \\quad n\\\\infty.\n  \\]Dla liczb parzystych postaci \\(2k\\) \\(|k|\\le m\\), proces \\(S_{2m}\\) znajduje się w punkcie \\(2k\\),\ngdy w \\(2m\\) próbach wylosowano dokładnie \\(m+k\\) jedynek.\nZatem korzystając z definicji rozkładu dwumianowego\n\\[\\begin{align*}\n    \\mathbb{P}[S_{2m}=2k] & = {2m \\choose m+k} \\frac 1{2^{2m}} = \\frac{(2m)!}{(m+k)! (m-k)! 2^{2m}}\\\\\n    &\\overset{{\\rm \\small Stirling}}{\\sim} \\frac{e^{m+k}e^{m-k}}{e^{2m}}\\frac{(2m)^{2m}}{(m+k)^{m+k}(m-k)^{m-k}}\\cdot \\frac{\\sqrt{ 4\\pi m}}{\\sqrt{2\\pi(m+k)}\\sqrt{2\\pi (m-k)}}\\cdot \\frac 1{2^{2m}}\\\\\n    &= \\bigg(\\frac {m}{m+k}\\bigg)^{m+k}\\bigg( \\frac{m}{m-k}\\bigg)^{m-k} \\cdot \\sqrt{\\frac{m}{(m+k)(m-k)}} \\cdot \\frac 1{\\sqrt \\pi}\\\\\n    & = \\bigg( 1+\\frac km \\bigg)^{-(m+k)}\\bigg( 1-\\frac km \\bigg)^{-(m-k)} \\cdot \\frac 1{\\sqrt{\\pi m}}\\cdot \\frac{1}{\\sqrt{1+\\frac km}\\sqrt{1-\\frac km}}\\\\\n    & = \\bigg( 1-\\frac {k^2}{m^2} \\bigg)^{-m}\\bigg( 1+\\frac km \\bigg)^{-k} \\bigg( 1-\\frac {k}{m} \\bigg)^k\n    \\cdot \\frac 1{\\sqrt{\\pi m}}\\cdot \\frac{1}{\\sqrt{1+\\frac km}\\sqrt{1-\\frac km}}\\\\\n  \\end{align*}\\]\nPrzypomnijmy, że interesuje nas sytuacja, gdy\n\\(x\\\\mathbb{R}\\) jest ustalone oraz \\(\\frac{k^2}{m} = \\frac{x^2}{2}\\). Wówczas, gdy \\(m\\) zbiega \\(\\infty\\) (więc również \\(k \\\\infty\\)),\nz własności liczby \\(e\\) otrzymujemy\n\\[\\begin{align*}\n  \\bigg( 1-\\frac {k^2}{m^2} \\bigg)^{-m} &= \\bigg( 1-\\frac {k^2}{m^2} \\bigg)^{- \\frac {m^2}{k^2} \\cdot \\frac{x^2}2 }  \\e^{\\frac{x^2}{2}},\\\\\n  \\bigg( 1+\\frac km \\bigg)^{-k} &=  \\bigg( 1+\\frac km \\bigg)^{-\\frac mk \\cdot \\frac{x^2}{2}}  \\e^{-\\frac{x^2}2},\\\\\n   \\bigg( 1-\\frac {k}{m} \\bigg)^k&\\e^{-\\frac{x^2}{2}}.\n  \\end{align*}\\]\nStąd wynika \\[\\mathbb{P}[S_{2m} = 2k] \\sim e^{-x^2/2}\\cdot \\frac 1 {\\sqrt{\\pi m}}.\\]\nDalej\n\\[\\begin{multline*}\n  \\mathbb{P}\\bigg[ \\le \\frac{S_{2m}}{\\sqrt{2m}} \\le b \\bigg] =\n  \\sum_{x\\[,b]\\cap \\frac{2\\mathbb{Z}}{\\sqrt{2m}}} \\mathbb{P}\\big[ S_{2m} = x\\sqrt{2m} \\big] \\\\\n  \\sim \\sum_{x\\[,b]\\cap \\frac{2\\mathbb{Z}}{\\sqrt{2m}}} \\frac 1{\\sqrt{2\\pi}} \\sqrt{\\frac 2m} e^{-x^2/2}\n  \\sim \\int_a^b \\frac 1{\\sqrt{2\\pi}} e^{-x^2/2}dx,\n  \\end{multline*}\\]\ngdzie ostatnia implikacja wynika z definicji całki Riemanna.Przykład 23.2  Jakie jest prawdopodobieństwo, że w 100 rzutach kostką otrzymamy co najmniej 60 orłów?\nSformułujmy zadanie w terminach zmiennych losowych, które były używane w powyższym twierdzeniu. Niech \\(X_i = 1\\), gdy wyrzucono w \\(\\)-tym rzucie orła \\(X_i=-1\\) w przeciwnym razie. Oznaczmy przez \\(S_n = X_1+\\ldots+X_n\\). Wówczas 60 orłów 40 reszek daje \\(S_{100} = 20\\). Z powyższego twierdzenia mamy więc\n\\[\n\\mathbb{P}[S_{100}\\ge 20] = \\mathbb{P}[S_{100}/10 \\ge 2] \\approx 1- \\mathbb{P}hi(2) \\approx 0,02.\n\\]\nAnalogicznie można obliczyć o wyrzucenia co najmniej 540 orłów w 1000 rzutach:\n\\[\n\\mathbb{P}[S_{1000}\\ge 80] \\approx 0,006.\n\\]Twierdzenie 23.2  Niech \\(S_n\\) będzie liczbą sukcesów w \\(n\\) próbach przy ie sukcesu \\(p\\) (\\(S_n\\sim {\\rm Bin}(n,p)\\)). Wówczas\n\\[\n  \\lim_{n \\\\infty}\\mathbb{P}\\bigg(\\leq \\frac{S_n-np}{\\sqrt{n pq}} \\leq b \\bigg) \\\\frac{1}{\\sqrt{2\\pi}} \\int_a^b e^{-s^2/2}\\mathrm{d}s.\n  \\]Przypomnijmy, że \\(\\mathbb{E} S_n = np\\), \\(\\mathbb{V}ar S_n = npq\\). Więc powyższy wynik można zapisać w postaci\n\\[\n  \\lim_{n \\\\infty}\\mathbb{P}\\bigg(\\leq \\frac{S_n-\\mathbb{E}[S_n]}{\\sqrt{\\mathbb{V}ar[S_n]}} \\leq b \\bigg)\n\\\\frac{1}{\\sqrt{2\\pi}} \\int_a^b e^{-s^2/2}\\mathrm{d}s.\n  \\]\nPodczas kolejnych wykładów pokażemy, że analogiczny wynik zachodzi w znacznie większej ogólności.","code":"\n# Ustawienie liczby obserwacji\nn_vals <- 1:200\n\n# Spacer losowy o średniej 1 (np. suma zmiennych U(0.5, 1.5))\n#set.seed(123)  # dla powtarzalności\nsteps <- runif(length(n_vals), min = -1, max = 3)\nwalk <- cumsum(steps)\n\n# Proste referencyjne\nline_1.1 <- 1.1 * n_vals\nline_0.9 <- 0.9 * n_vals\n\n# Ustawienia kolorystyczne (Solarized)\nbg_color <- \"#002b36\"\nfg_color <- \"#eee8d5\"\nprimary_color <- \"#2aa198\"\nref_line_color <- \"#586e75\"\npar(bg = bg_color)\n\n# Rysowanie wykresu\nplot(n_vals, walk,\n     type = \"l\",\n     col = primary_color,\n     xlab = \"n\",\n     ylab = \"S_n\",\n     col.lab = fg_color,\n     col.main = fg_color,\n     col.axis = fg_color,\n     fg = fg_color,\n     panel.first = grid(col = fg_color))\n\n# Dodanie prostych o nachyleniu 1.1 i 0.9\nlines(n_vals, line_1.1, col = ref_line_color, lty = 2)\nlines(n_vals, line_0.9, col = ref_line_color, lty = 2)\nlines(n_vals, n_vals, col = fg_color, lty = 1)\n\n# Dodanie legendy\nlegend(\"topleft\", legend = c(\"S_n\", \"Nachylenie 1\",  \"Nachylenie 1.1\", \"Nachylenie 0.9\"),\n       col = c(primary_color, fg_color, ref_line_color, ref_line_color),\n       lty = c(1, 1, 2, 2), text.col = fg_color, bg = bg_color)"},{"path":"zbieżność-według-rozkładu.html","id":"zbieżność-według-rozkładu","chapter":"24 Zbieżność według rozkładu","heading":"24 Zbieżność według rozkładu","text":"Twierdzenie de Moivre’- Laplace’mówi, że odpowiednio znormalizowany rozkład\n\\({\\rm Bin(n,1/2)}\\) zbiega w pewnym sensie rozkładu normalnego.\nPodczas zajęć pojawiły się również podobne przykłady, np. Twierdzenie Poissona mówiło,\nże rozkład \\({\\rm Bin}(n,p_n)\\) zbiega rozkładu\n\\({\\rm Pois}(\\lambda)\\), gdy \\(np_n\\\\lambda\\).Chcemy zdefiniować pojęcie zbieżności rozkładów zmiennych losowych.\nInnymi słowy, ponieważ rozkłady są miarami probabilistycznymi na \\(\\mathbb{R}\\),\nchcemy zrozumieć co oznacza, że ciąg miar \\(\\mu_n\\) zbiega \\(\\mu\\).\nCo oznacza, że dwie miary są blisko siebie?\nNajbardziej naturalnie byłoby powiedzieć, że \\(\\mu_n\\\\mu\\), gdy dla każdego zbioru borelowskiego \\(\\\\mathcal{B}(\\mathbb{R})\\),\n\\(\\mu_n()\\\\mu()\\). Okazuje się jednak, że takie podejście jest niewłaściwe tak wprowadzona zbieżność\nnie ma dobrych własności (np. jest trudna sprawdzenia operowania).Przykład 24.1  Niech \\(\\{X_n\\}_{n \\\\mathbb{N}}\\) będzie ciągiem niezależnych zmiennych losowych takich, że\n\\[\\begin{equation*}\n    \\mathbb{P}[X_j=1]=\\mathbb{P}[X_j=-1] =1/2.\n\\end{equation*}\\]\nWówczas dla \\(S_n = \\sum_{j=1}^nX_j\\) mamy\n\\[\\begin{equation*}\n    \\lim_{n \\\\infty} \\mathbb{P}\\left[S_n/\\sqrt{n} \\[,b] \\right] = \\int_a^b \\frac 1{\\sqrt{2\\pi}} e^{-x^2/2} \\mathrm{d}x.\n\\end{equation*}\\]\nJednak dla przeliczalnego zbioru\n\\[\\begin{equation*}\n    = \\bigcup_{k\\geq 1} \\mathbb{Z}/\\sqrt{k}\n\\end{equation*}\\]\nmamy\n\\[\\begin{equation*}\n    \\mathbb{P}\\left[S_n/\\sqrt{n} \\\\right] = 1\n\\end{equation*}\\]\nale\n\\[\\begin{equation*}\n\\int_A \\frac 1{\\sqrt{2\\pi}} e^{-x^2/2} \\mathrm{d}x=0.\n\\end{equation*}\\]Widzimy więc, że zagadnienia zbieżności miar należy podejść ostrożnie.\nZrobimy więc krok w tył zastanowimy się najpierw w jaki sposób możemy rozróżnić miarę.","code":""},{"path":"zbieżność-według-rozkładu.html","id":"rozróżnianie-miar","chapter":"24 Zbieżność według rozkładu","heading":"24.1 Rozróżnianie miar","text":"Definicja 24.1  Niech \\(\\vec{X}\\) \\(\\vec{Y}\\) będą \\(d\\)-wymiarowymi wektorami losowymi.\nPowiemy, że \\(\\vec{X}\\) \\(\\vec{Y}\\) maja ten sam rozkład, jeżeli \\(\\mu_{\\vec{X}} = \\mu_{\\vec{Y}}\\).\nPiszemy wtedy \\(\\vec{X} \\stackrel{d}{=} \\vec{Y}\\).Jeżeli \\(\\mu\\) oraz \\(\\nu\\) są rozkładami prawdopodobieństwa na \\(\\mathbb{R}^d\\) powiemy, że są one równe,\njeżeli są równe jako odwzorowania \\(\\mathcal{B}(\\mathbb{R}^d) \\[0,1]\\). Dokładniej \\(\\mu = \\nu\\)\nwtedy gdy\n\\[\\begin{equation*}\n    \\nu() = \\mu()\n\\end{equation*}\\]\ndla każdego borelowskiego \\(\\subseteq \\mathbb{R}^d\\).\nKlasa \\(\\mathcal{B}(\\mathbb{R}^d)\\) jest stosunkowo duża w wielu przypadkach obliczenie \\(\\nu()\\)\njest kłopotliwe. Chcemy więc znaleźć prostszą klasę \\(\\mathcal{C} \\subset \\mathcal{B}(\\mathbb{R}^d)\\)\nna której możemy testować równość miar.Lemma 24.1  Załóżmy, że \\(\\mathcal{C}\\) jest \\(\\pi\\)-układem generującym \\(\\mathcal{B}(\\mathbb{R}^d)\\). Jeżeli\n\\(\\nu(C) = \\mu(C)\\) dla każdego \\(C \\\\mathcal{C}\\), \\(\\nu=\\mu\\).Proof. Dowód przebiega tak samo jak dowód Twierdzenia o jednoznaczności. Przypomnijmy, że wystarczy sprawdzić, że\n\\[\\begin{equation*}\n    \\mathcal{L} = \\left\\{ \\\\mathcal{B}(\\mathbb{R}^d)\n    \\: : \\: \\nu() = \\mu() \\right\\}\n\\end{equation*}\\]\njest \\(\\lambda\\)-układem.\nSkoro \\(\\mathcal{C} \\subseteq \\mathcal{L}\\), \\(\\mathcal{B}(\\mathbb{R}^d) = \\mathcal{L}\\).□Z powyższego lematu płynie prosty wniosek.Wniosek 24.1  Jeżeli \\(\\mu\\) \\(\\nu\\) są rozkładami na \\(\\mathbb{R}\\) takimi, że dla każdych\n\\(<b\\), \\(\\nu((,b)) = \\mu((,b))\\), \\(\\nu=\\mu\\).Podobny wniosek można napisać o przedziałach domkniętych.\nZ rozkładami związana jest też operacja całkowania. Dla każdej ograniczonej borelowskiej\n\\(f \\colon \\mathbb{R}^d \\\\mathbb{R}\\) całka\n\\[\\begin{equation*}\n    \\int_{\\mathbb{R}^d} f(x) \\nu(\\mathrm{d}x)\n\\end{equation*}\\]\njest zbieżna. Okazuje się, że znajomość wartości całki dla dostatecznie bogatej rodziny funkcji wystarcza\nrozróżnienia miaryLemma 24.2  Załóżmy, że\n\\[\\begin{equation*}\n    \\int_{\\mathbb{R}^d} f(x) \\nu(\\mathrm{d}x) = \\int_{\\mathbb{R}^d} f(x) \\mu(\\mathrm{d}x)\n\\end{equation*}\\]\ndla każdej ograniczonej borelowskiej funkcji \\(f \\colon \\mathbb{R}^d \\\\mathbb{R}\\).\nWówczas\n\\(\\nu = \\mu\\).Proof. Zauważmy, że dla \\(\\\\mathcal{B}(\\mathbb{R}^d)\\) funkcja \\(f(x) = \\mathbf{1}_A(x)\\) jest borelowska ograniczona\noraz\n\\[\\begin{equation*}\n\\nu()=\\int_{\\mathbb{R}^d} f(x) \\nu(\\mathrm{d}x) =\n    \\int_{\\mathbb{R}^d} f(x) \\mu(\\mathrm{d}x) = \\mu().\n\\end{equation*}\\]\n□Klasa wszystkich ograniczonych funkcji borelowskich, podobnie jak samych zbiorów borelowskich,\njest niepraktyczna. Wygodniej jest pracować z ciągłymi funkcjami ograniczonymi.\nOznaczmy\n\\[\n  C_b\\left(\\mathbb{R}^d\\right) =\n  \\left\\{ f:\\mathbb{R}^d\\\\mathbb{R}, f \\mbox{ jest ciągła ograniczona}  \\right\\}.\n\\]Lemma 24.3  Załóżmy, że\n\\[\\begin{equation*}\n    \\int_{\\mathbb{R}^d} f(x) \\nu(\\mathrm{d}x) = \\int_{\\mathbb{R}^d} f(x) \\mu(\\mathrm{d}x)\n\\end{equation*}\\]\ndla każdej \\(f \\C_b(\\mathbb{R}^d)\\).\nWówczas\n\\(\\nu = \\mu\\).Proof. Pokażemy, że \\(\\nu(F) = \\mu(F)\\) dla każdego domkniętego \\(F \\subseteq \\mathbb{R}^d\\). Ustalmy takie \\(F\\).\nDla \\(x \\\\mathbb{R}^d\\) niech\n\\[\\begin{equation*}\n    \\mathrm{dist}(x, F) = \\inf_{f \\F} \\|x-f\\|.\n\\end{equation*}\\]\nSkoro \\(F\\) jest domknięty, \\(x \\F\\) wtedy tylko wtedy, gdy \\(\\mathrm{dist}(x, F)=0\\).\nNiech\n\\[\\begin{equation*}\n    f_n(x) = ( 1-n\\mathrm{dist}(x,F))_+.\n\\end{equation*}\\]\nWówczas \\(f_n(x) \\\\mathbf{1}_F(x)\\). Zauważmy, że korzystamy tutaj z tego, że \\(F\\) jest domknięty.\nZ założenia\n\\[\\begin{equation*}\n    \\int_{\\mathbb{R}^d} f_n(x) \\nu(\\mathrm{d}x) = \\int_{\\mathbb{R}^d} f_n(x) \\mu(\\mathrm{d}x)\n\\end{equation*}\\]\ndla każdego \\(n \\\\mathbb{N}\\). Przechodząc granicy korzystając z twierdzenia o zbieżności ograniczonej\n\\[\\begin{equation*}\n\\nu(F)=\\int_{\\mathbb{R}^d} \\mathbf{1}_F(x) \\nu(\\mathrm{d}x) =\n    \\int_{\\mathbb{R}^d} \\mathbf{1}_F(x) \\mu(\\mathrm{d}x) = \\mu().\n\\end{equation*}\\]\n□","code":""},{"path":"zbieżność-według-rozkładu.html","id":"słaba-zbieżność","chapter":"24 Zbieżność według rozkładu","heading":"24.2 Słaba zbieżność","text":"Zastanówmy się przez chwilę, jaki test na zbieżność miar byłby najwygodniejszy.\nJedyny warunek brzegowy jaki stawiamy jest że ciąg miar \\(\\delta_{(-1)^n/n}\\) skupionych w\npunktach \\((-1)^n/n\\) dla \\(n \\\\mathbb{N}\\) powinien być zbieżny miary \\(\\delta_0\\) skupionej w \\(0\\).Sprawdźmy najpierw jak zachowuje się ten ciąg przy całkowaniu zbiorów otwartych.\nZauważmy, że\n\\[\\begin{align*}\n    &\\delta_{(-1)^n/n}(0,1) = (1+(-1)^n)/2 & \\delta_0(0,1) =0\\\\\n         &\\delta_{(-1)^n/n}(\\{0\\}) = 0 & \\delta_0(\\{0\\}) =1.\n\\end{align*}\\]\nOkazuje się, że ciągi \\(\\delta_{(-1)^n/n}()\\) nie muszą być zbieżne. Jeżeli są zbieżne, \ngranica nie musi być równa \\(\\delta_0()\\). Okazuje się jednak, że sytuacja nie jest aż tak dramatyczna\nmożliwe jest opisanie asymptotycznego zachowania \\(\\delta_{(-1)^n/n}()\\) w terminach \\(\\delta_0()\\). Opis ten\nwymaga jednak informacji o topologicznych własnościach \\(\\).W przypadku funkcji ciągłych sprawa jest o wiele prostsza. Dla dowolnej \\(f \\C_n(\\mathbb{R})\\)\nmamy bowiem\n\\[\\begin{equation*}\n    \\int_{\\mathbb{R}} f(x) \\delta_{(-1)^n/n} (\\mathrm{d}x) = f((-1)^n/n) \\f(0) = \\int_\\mathbb{R} f(x) \\delta_0(\\mathrm{d}x).\n\\end{equation*}\\]\nFunkcje ciągłe są zatem niewrażliwe na małe wahania miar. Są dobrą klasą testowania zbieżności miar.Definicja 24.2  Niech \\(\\{\\mu_n\\}_{n \\\\mathbb{N}}\\) będzie ciągiem miar\nprobabilistycznych na \\(\\mathcal{B}(\\mathbb{R}^d)\\).\nMówimy, że \\(\\mu_n\\) zbiegają słabo miary probabilistycznej \\(\\mu\\)\njeżeli dla każdej funkcji \\(f\\C_b(\\mathbb{R}^d)\\)\n\\[\n    \\int_{\\mathbb{R}^d} f(x)\\mu_n(\\mathrm{d}x) \\\\int_{\\mathbb{R}^d} f(x)\\mu(\\mathrm{d}x).\n    \\]\nPiszemy wtedy \\(\\mu_n\\Rightarrow \\mu\\).Przykład 24.2  Niech \\(a_n\\\\\\mathbb{R}\\).\nWtedy \\(\\delta_{a_n}\\Rightarrow \\delta_a\\), bo dla każdej funkcji \\(f\\C_b(\\mathbb{R})\\)\n\\[\n  \\int_\\mathbb{R} f(x) \\delta_{a_n}(\\mathrm{d}x) = f(a_n)\\f() = \\int_\\mathbb{R} f(x)\\delta_a(\\mathrm{d}x).\n  \\]Przykład 24.3  Zdefiniujmy ciąg miar probabilistycznych\n\\[\n  \\mu_n\\bigg(\\bigg\\{ \\frac kn \\bigg\\}\\bigg) = \\frac 1n, \\qquad k\\\\{1,2,\\ldots,n\\}.\n  \\] Wtedy dla \\(f\\C(\\mathbb{R})\\)\n\\[\n  \\int_\\mathbb{R} f(x)\\mu_n(dx) = \\sum_{k=1}^n f\\bigg(\\frac kn \\bigg) \\cdot \\frac 1n \\\\int_0^1 f(x)dx,\n  \\]\ngdzie zbieżność wynika z własności całki Riemanna. Zatem \\(\\mu_n\\) zbiegają słabo miary Lebesgue’na \\([0,1]\\).Przykład 24.4  Niech \\(n \\\\mathbb{N}\\). Z liczb \\(\\{1, \\ldots n\\}\\) losujemy bez zwracania dwie liczby.\nNiech \\(X_n\\) będzie większą z nich. Wówczas\n\\[\\begin{equation*}\n    \\mathbb{P}[X_n=k] = \\frac{2(k-1)}{n(n-1)}.\n\\end{equation*}\\]\nNiech \\(\\mu_n\\) będzie rozkładem zmiennej \\(X_n/n\\). Wówczas dla każdej \\(f \\C_b(\\mathbb{R})\\),\n\\[\\begin{align*}\n    \\int_\\mathbb{R} f(x) \\mu_n(\\mathrm{d}x) & = \\sum_{k} f(k/n) \\mathbb{P}[X_n/n=k/n] \\\\\n    & = \\sum_{k=2}^n f(k/n) \\frac{2(k-1)}{n(n-1)} \\\\int_0^1 f(x) 2x \\mathrm{d}x.\n\\end{align*}\\]\nMiary \\(\\mu_n\\) zbiegają zatem słabo miary \\(\\mu\\) o gęstości \\(2x \\mathbf{1}_{[0,1]}(x)\\).Twierdzenie 24.1  (Portmanteau) Niech \\(\\mu_n,\\mu\\) będą miarami probabilistycznymi na\n\\((\\mathbb{R}^d,\\mathcal{B}(\\mathbb{R}^d))\\).\nNastępujące warunki są równoważne\\(\\mu_n \\Rightarrow\\mu\\);dla każdej funkcji \\(f\\C_b(\\mathbb{R}^d)\\) jednostajnie ciągłej:\n\\[\\int_\\mathbb{R} f(x)\\mu_n(dx) \\\\int_{\\mathbb{R}^d} f(x)\\mu(dx);\\]dla każdego domkniętego zbioru \\(F\\subset \\mathbb{R}^d\\):\n\\[\\limsup_{n\\\\infty} \\mu_n(F) \\le \\mu(F);\\]dla każdego otwartego zbioru \\(G\\subset \\mathbb{R}^d\\):\n\\[\\liminf_{n\\\\infty} \\mu_n(G) \\ge \\mu(G);\\]dla każdego zbioru \\(\\\\mathcal{B}(\\mathbb{R}^d)\\) takiego, że \\(\\mu(\\partial )=0\\):\n\\[\\lim_{n\\\\infty} \\mu_n() = \\mu().\\]Remark. Przypomnijmy, że zbiór \\(G \\subseteq \\mathbb{R}^d\\) nazywamy otwartym jeżeli dla każdego jest punktu\n\\(x \\G\\) istnieje kula o dostatecznie małym promieniu \\(\\epsilon>0\\) środku w punkcie \\(x\\),\n\\[\\begin{equation*}\n    B(x, \\epsilon) = \\left\\{ y \\\\mathbb{R}^d \\: : \\: \\|x-y\\| < \\epsilon \\right\\}\n\\end{equation*}\\]\ntaka, że \\(B(x,\\epsilon) \\subseteq G\\). Dla zbioru \\(\\subseteq \\mathbb{R}^d\\)\nprzez \\(^\\circ = \\mathrm{int}()\\) oznaczamy największy zbiór otwarty zawarty w \\(\\).Zbiór \\(F \\subseteq \\mathbb{R}^d\\) nazywamy domkniętym, jeżeli jego dopełnienie\n\\(F^c = \\mathbb{R}^d\\setminus F\\) jest otwarte. Równoważnie \\(F\\) jest domknięty, jeżeli dla każdego\npunktu spoza \\(F\\), \\(x \\notin F\\) istnieje kula o środku w punkcie \\(x\\) promieniu \\(\\epsilon\\) rozłączna\nz \\(F\\), \\(B(x, \\epsilon) \\cap F =\\emptyset\\).\nJeżeli \\(\\subseteq \\mathbb{R}^d\\), przez \\(\\overline{} = \\mathrm{cl}()\\)\noznaczamy najmniejszy zbiór domknięty zawierający \\(\\).\nDodatkowo \\(\\overline{}\\) jest zbiorem\nwszystkich granic dla ciągów zawartych w \\(\\).Wreszcie dla \\(\\subseteq \\mathbb{R}^d\\) określamy jego brzeg przez\n\\[\\begin{equation*}\n    \\partial() = \\mathrm{fr}() = \\mathrm{bd}() = \\mathrm{cl}() \\setminus \\mathrm{int}().\n\\end{equation*}\\]\nZbiór \\(\\partial \\) składa się z punktów, które są granicami ciągu zawartego w \\(\\) ciągu zawartego\nw \\(^c\\).Remark. Potrmanteau zapożyczone z francuskiego określenie skórzanej walizki podróżnej na ubrania (porter (nosić) + manteau płaszcz).Proof. \\(1\\rightarrow 2\\). Oczywiste.\\(2\\rightarrow 3\\).\nUstalmy dowolny zbiór domknięty \\(F\\).\nDla \\(\\varepsilon>0\\) oznaczmy przez\n\\[\nF_{\\varepsilon} = \\big\\{ x\\\\mathbb{R} : d(x,F) < \\varepsilon\\big\\}\n\\]\n\\(\\varepsilon\\)-otoczkę zbioru \\(F\\).Ustalmy \\(\\delta > 0\\) weźmy \\(\\varepsilon>0\\) taki, że\n\\[\n\\mu(F_\\varepsilon) <\\mu(F) + \\delta.\n\\]\nTaki \\(\\varepsilon\\) musi istnieć, gdyż z domkniętości zbioru \\(F\\):\n\\(F = \\bigcap_{n=1}^{\\infty} F_{\\frac 1n}\\), z lematu o ciągłości miary\n\\(\\mu(F_{\\frac 1n})\\\\mu(F)\\).\nNiech \\(f\\) będzie jednostajnie ciągłą funkcją taką, że\n\\[\n{\\bf 1}_F \\le f \\le {\\bf 1}_{F_{\\varepsilon}}\n\\]\nmożna np. przyjąć\n\\[\nf(x) = (1-d(x,F)/\\varepsilon))_+.\n\\]\nWówczas odwołując się punktu \\(2.\\)\n\\[\n\\limsup \\mu_n(F) \\le \\limsup \\int f d\\mu_n = \\int f d\\mu \\le \\mu(F_{\\varepsilon}) \\le \\mu(F)+\\delta.\n\\] Z dowolności \\(\\delta\\) otrzymujemy tezę.\\(3\\leftrightarrow 4\\). Dowód równoważności wynika z wzięcia dopełnienia zbioru.\\(4,3\\rightarrow 5\\)\nNiech \\(\\\\mathcal{B}(\\mathbb{R}^d)\\) będzie taki, że \\(\\mu(\\partial())=0\\).\nMamy\n\\[\\begin{align*}\n    \\mu(\\mathrm{cl}()) & \\geq \\limsup_{n \\\\infty} \\mu_n(\\mathrm{cl}()) \\\\\n    & \\geq \\limsup_{n \\\\infty} \\mu_n() \\\\\n    & \\geq \\liminf_{n \\\\infty} \\mu_n(\\mathrm{int}()) \\\\\n    & \\geq \\mu(\\mathrm{int}()).\n\\end{align*}\\]\nZauważmy jednak, że\n\\(\\mu(\\mathrm{cl}()) =\n\\mu(\\partial ()) + \\mu(\\mathrm{int}())\n= \\mu(\\mathrm{int}())\\). Zatem wszystkie granice powyżej są sobie równe.\\(5 \\rightarrow 1\\)\nWybierzmy \\(f \\C_b(\\mathbb{R}^d)\\).\nZ liniowości całki bez zmniejszania ogólności możemy założyć, że \\(f\\geq 0\\).\nWówczas dla dowolnego \\(x \\\\mathbb{R}^d\\),\n\\[\\begin{equation*}\nf(x) = \\int_0^{f(x)} \\mathrm{d}t = \\int_0^\\infty \\mathbf{1}_{\\{ f(x) >t\\}}(t) \\mathrm{d}t.\n\\end{equation*}\\]\nStąd\n\\[\\begin{equation*}\n  \\int_{\\mathbb{R}^d} f(x) \\mu(\\mathrm{d} x) =\n  \\int_0^1 \\mu[x \\: : \\: f(x)>t] \\mathrm{d} t.\n\\end{equation*}\\]\nNiech\n\\[\\begin{equation*}\nB_t = \\left\\{ x\\\\mathbb{R}^d \\: : \\: f(x) >t \\right\\}.\n\\end{equation*}\\]\nZauważmy, że z ciągłości \\(f\\),\n\\[\\begin{equation*}\n  {\\rm cl} \\{ x \\: : \\: f(x)>t \\} \\subseteq \\{ x \\: : \\: f(x)\\geq t \\}  \n\\end{equation*}\\]\noraz, skoro przeciwobrazy przez \\(f\\) zbiorów otwartych są otwarte\n\\[\\begin{equation*}\n  {\\rm int} \\{ x \\: : \\: f(x)>t \\} = \\{ x \\: : \\: f(x)> t \\} .\n  \\end{equation*}\\]\nStąd\n\\[\\begin{equation*}\n    \\partial B_t=\\partial \\{ x \\: : \\: f(x)>t \\} \\subseteq \\{ x \\: : \\: f(x)= t \\}.  \n  \\end{equation*}\\]\nZauważmy, że zbiory \\(A_t = \\{ x \\: : \\: f(x)= t \\}\\)\nsą parami rozłączne wypełniają\ncałe \\(\\mathbb{R}^d\\).\nWówczas \\(\\mu(A_t)>0\\) tylko dla przeliczalnie wielu \\(t \\\\mathbb{R}\\)\n(jest co najwyżej \\(n\\\\mathbb{N}\\) zbiorów postaci \\(A_t\\) dla których \\(\\mu(A_t) > 1/n\\)).\nOznacza , że \\(\\mu(\\partial B_t) =0\\) dla prawie wszystkich \\(t\\)\n(wszystkich poza przeliczalną ilością).\nZ punktu 4\n\\[\\begin{equation*}\n    \\mu_n(B_t) \\\\mu(B_t)\n  \\end{equation*}\\]\n\\(\\lambda_1\\) -p.w.\nStąd na mocy twierdzenia o zbieżności ograniczonej\n\\[\\begin{align*}\n    \\int f(x) \\mu_n(\\mathrm{d}x)  \n    & = \\int_0^\\infty \\mu_n (x \\: : \\: f(x) >t) \\mathrm{d} t\n    \\\\ & \\\\int_0^\\infty \\mu (x \\: : \\: f(x) >t) \\mathrm{d} t \\\\\n    & = \\int f(x) \\mu(\\mathrm{d}x).\n  \\end{align*}\\]\n□Twierdzenie 24.2  Niech \\(\\{\\mu_n\\}_n,\\mu\\) będą rozkładami prawdopodobieństwa na \\(\\mathbb{R}\\) o dystrybuantach\nodpowiednio \\(\\{F_n\\}_n,F\\).\nWówczas \\(\\mu_n\\Rightarrow \\mu\\) wtedy tylko wtedy, gdy\n\\(F_n(x)\\F(x)\\) dla każdego punktu \\(x\\), w którym \\(F\\) jest ciągła.Proof. Załóżmy, że \\(\\mu_n\\Rightarrow \\mu\\).\nNiech \\(x\\) będzie punktem ciągłości \\(F\\).\nZdefiniujmy \\(= (-\\infty,x]\\), wówczas \\(\\partial = \\{x\\}\\) \n\\(\\mu(\\partial ) = 0\\). Zatem\n\\[\n  F_n(x) = \\mu_n() \\\\mu() = F(x).\n\\]Załóżmy teraz, że \\(F_n(x)\\F(x)\\) dla każdego punktu \\(x\\), w którym \\(F\\) jest ciągła.\nNiech \\(G\\subset \\mathbb{R}\\) będzie dowolnym otwartym zbiorem.\nWówczas możemy \\(G\\) przedstawić w postaci\n\\[\n  G = \\bigcup_{k=1}^\\infty I_k,\n  \\]\ngdzie \\(I_k = (a_k,b_k)\\) są rozłącznymi przedziałami (suma może też być skończona).\nUstalmy \\(\\varepsilon>0\\) wybierzmy punkty ciągłości\n\\(a_k', b_k'\\(a_k,b_k)\\) tak, aby\n\\[\n  F(b_k') - F(a_k') = \\mu(a_k',b_k') \\ge \\mu(I_k) - \\varepsilon 2^{-k}.\n  \\]\nTakie punkty istnieją z lematu o ciągłości miary oraz obserwacji,\nże \\(F\\) ma co najwyżej przeliczalnie wiele punktów nieciągłości.Wówczas dla ustalonego \\(m\\):\n\\[\\begin{align*}\n  \\liminf_{n\\\\infty} \\mu_n(G) & \\ge \\liminf_{n\\\\infty}\n  \\sum_{k=1}^m \\big( F_n(b_k') - F_n(a_k') \\big) \\\\ &\n  =  \\sum_{k=1}^m \\big( F(b_k') - F(a_k') \\big) \\\\\n  & \\ge \\mu\\bigg( \\bigcup_{k=1}^m  I_k \\bigg) - \\varepsilon.\n  \\end{align*}\\]\nZ dowolności \\(m\\) lematu o ciągłości miary:\n\\[\n  \\liminf_{n\\\\infty} \\mu_n(G) \\ge \\mu(G) -\\varepsilon.\n  \\] Następnie z dowolności \\(\\varepsilon\\), przechodząc z tym parametrem 0, otrzymujemy\n\\[\n  \\liminf_{n\\\\infty} \\mu_n(G) \\ge \\mu(G).\n  \\] Zatem z twierdzenia 24.1, \\(\\mu_n\\Rightarrow \\mu\\).Przykład 24.5  Niech \\(a_n\\\\\\mathbb{R}\\).\nWtedy \\(\\delta_{a_n}\\Rightarrow \\delta_a\\), bo dystrybuanta \\(F\\) miary \\(\\delta_a\\)\nma tylko jeden punkt nieciągłości: \\(\\).\nDla każdego \\(x\\= \\), \\(F_n(x)\\F(x)\\)Przykład 24.6  Niech\n\\[\n  \\mu_n\\bigg(\\bigg\\{ \\frac kn \\bigg\\}\\bigg) = \\frac 1n.\n  \\] \\(\\mu = {\\bf 1}_{[0,1]}(x)dx\\). Wykazana przez nas wcześniej zbieżność\n\\(\\mu_n\\Rightarrow \\mu\\) wynika z powyższego twierdzenia poniższego rysunku, na którym\nw kremowy beżu zaznaczono dystrybuantę \\(F\\), w kolorze morskiego turkusu dystrybuantę \\(\\mu_n\\).Przykład 24.7  Rozważmy zmienne losowe \\(\\{X_n\\}_{n \\\\mathbb{N}}\\) takie, że dla każdego \\(n \\\\mathbb{N}\\)\nma rozkład geometryczny z parametrem \\(1/n\\),\n\\[\\begin{equation*}\n    \\mathbb{P}\\left[ X_n=k \\right] = \\left( 1-1/n\\right)^{k-1}/n.\n\\end{equation*}\\]\nNiech \\(\\mu_n\\) będzie rozkładem zmiennej losowej \\(X_n/n\\). Wówczas\ndla każdego \\(t\\\\mathbb{R}\\),\n\\[\\begin{align*}\n    \\mu_n((t,+\\infty)) & = \\mathbb{P}[X_n> tn]  \\\\\n    & = \\mathbb{P}[X_n \\geq [tn]+1] \\\\\n    & = (1-1/n)^{[1n]+1} \\e^{-t}.\n\\end{align*}\\]\nPowyższe jest równoważne zbieżności dystrybuant. Oznacza , że \\(\\mu_n \\Rightarrow \\mathrm{Exp}(1)\\).\nZbieżność tę widać na rysunku poniżej.","code":""},{"path":"zbieżność-według-rozkładu.html","id":"zbieżność-według-rozkładu-1","chapter":"24 Zbieżność według rozkładu","heading":"24.3 Zbieżność według rozkładu","text":"Wiele naturalnych rozkładów najwygodniej jest zdefiniowac w terminach zmiennych losowych.\nAby uprościć notację w takich przypadkach, zbieżność ich rozkładów\nbędziemy wysławiali w terminach tych właśnie zmiennych.Definicja 24.3  Mówimy, że ciąg wektorów losowych \\(\\{\\vec{X}_n\\}_{n \\\\mathbb{N}}\\)\nzbiega według rozkładu wektora losowego \\(\\vec{X}\\), jeżeli\n\\(\\mu_{\\vec{X}_n} \\Rightarrow \\mu_\\vec{X}\\), czyli\n\\[\\begin{equation*}\n    \\mathbb{E} \\left[ f\\left(\\vec{X}_n\\right) \\right] \\\n    \\mathbb{E}\\left[ f\\left(\\vec{X}\\right) \\right]\n\\end{equation*}\\]\ndla każdej \\(f \\C_b(\\mathbb{R}^d)\\).W naturalny sposób Twierdzenie 24.1 można przepisać w terminach zmiennych losowych.Twierdzenie 24.3  Niech \\(\\{\\vec{X}_n\\}_{n \\\\mathbb{N}}\\), \\(\\vec{X}\\) będą \\(d\\)-wymiarowymi wektorami losowymi.\nNastępujące warunki są równoważne\\(\\vec{X}_n \\Rightarrow \\vec{X}\\);dla każdej funkcji \\(f\\C_b(\\mathbb{R}^d)\\) jednostajnie ciągłej:\n\\[\n\\lim_{n \\\\infty}\\mathbb{E} \\left[ f\\left(\\vec{X}_n\\right) \\right] =\n\\mathbb{E}\\left[ f\\left(\\vec{X}\\right) \\right]\n\\]dla każdego domkniętego zbioru \\(F\\subset \\mathbb{R}^d\\):\n\\[\\limsup_{n\\\\infty} \\mathbb{P}\\left[ \\vec{X}_n \\F \\right] \\le\n\\mathbb{P}\\left[ \\vec{X} \\F \\right];\\]dla każdego otwartego zbioru \\(G\\subset \\mathbb{R}^d\\):\n\\[\\liminf_{n\\\\infty} \\mathbb{P}\\left[ \\vec{X}_n \\G \\right] \\ge\n\\mathbb{P}\\left[ \\vec{X} \\G \\right];\\]dla każdego zbioru \\(\\\\mathcal{B}(\\mathbb{R}^d)\\) takiego,\nże \\(\\mathbb{P}\\left[ \\vec{X} \\\\partial \\right]=0\\):\n\\[\n\\lim_{n\\\\infty} \\mathbb{P}\\left[ \\vec{X}_n \\\\right] =\n\\mathbb{P}\\left[ \\vec{X} \\\\right];\\]Często będziemy używać nieco innej notacji np. pisać \\(X_n \\Rightarrow \\mu\\), co oznacza,\nże \\(X_n\\) zbiegają według rozkładu pewnej zmiennej losowej o rozkładzie \\(\\mu\\).Przykład 24.8  Niech \\(\\{X_n\\}\\) będzie ciągiem niezależnych zmiennych losowych takich, że\n\\(\\mathbb{P}[X_n = 1] = \\mathbb{P}[X_n=-1] = 1/2\\) niech \\(S_n = X_1+\\ldots+ X_n\\).\nWówczas ciąg zmiennych losowych \\(S_n/\\sqrt n\\) zbiega według rozkładu \\(\\mathcal{N}(0,1)\\),\nco wynika z twierdzenia de Moivre’- Laplace’aZbieżność według rozkładu jest najsłabszą z poznanych tej pory zbieżności.Lemma 24.4  Niech \\(\\{X_n\\}, \\{Y_n\\}\\) będą ciągiem zmiennych losowych. Wówczasjeżeli \\(X_n \\overset{\\mathbb{P}}{\\} X\\), \\(X_n \\overset{d}{\\} X\\);jeżeli \\(X_n \\overset{d}{\\} X\\) \\(\\mathbb{P}[X=c]=1\\), \\(X_n \\overset{\\mathbb{P}}{\\} c\\);jeżeli \\(X_n \\overset{d}{\\} X\\), \\(aX_n+b \\overset{d}{\\} aX+b\\), dla \\(,b\\\\mathbb{R}\\);jeżeli \\(X_n \\overset{d}{\\} X\\), \\(Y_n \\overset{d}{\\} c\\), \\(X_n +Y_n\\overset{d}{\\} X + c\\) oraz\n\\(X_n Y_n\\overset{d}{\\} cX\\).Dowód powyższego lematu pozostawiamy jako ćwiczenie.Zbieżność według rozkładu nie implikuje zbieżności według prawdopodobieństwa,\nczy też zbieżności p.n. Można zaobserwować na poniższym przykładzie.Przykład 24.9  Rozważmy przestrzeń probabilistyczną \\(([0,1], \\mathcal{B}([0,1]), {\\rm Leb})\\).\nZdefiniujmy ciąg zmiennych losowych \\(X_{2n-1} = {\\bf 1}_{[0,1/2]}\\), \\(X_{2n}= {\\bf 1}_{[1/2,1]}\\).\nWówczas ciąg \\(X_n\\) nie jest zbieżny ani punktowo, ani według prawdopodobieństwa,\nale dla każdego \\(n\\), \\(X_n\\) ma rozkład \\(\\frac 12 \\delta_0 + \\frac 12 \\delta _1\\), zatem\n\\[\n  X_n \\Rightarrow \\frac 12 \\delta_0 + \\frac 12 \\delta _1\n\\]Powyższy przykład pokazuje, że zbieżność według rozkładu nie implikuje zbieżności p.n.,\nale zachodzi następujący wynik:Twierdzenie 24.4  (Skorochoda) Załóżmy, że \\(\\{\\vec{X}_n\\}_{n \\\\mathbb{N}}\\) jest ciągiem wektorów losowych\ntakich, że \\(\\vec{X}_n \\Rightarrow \\vec{X}\\) dla pewnego wektora losowego \\(\\vec{X}\\).\nIstnieje przestrzeń probabilistyczna \\((\\Omega', \\mathcal{F}', \\mathbb{P}')\\) oraz\nokreślone na niej wektory losowe\n\\(\\{\\vec{X}'_n\\}_{n \\\\mathbb{N}}\\) oraz \\(X'\\)\ntakie, że \\(\\vec{X}_n \\overset{d}= \\vec{X}'_n\\), \\(\\vec{X} \\overset{d}= \\vec{X}'\\)\noraz\n\\[\n\\vec{X}'_n \\\\vec{X}'\\quad \\mbox{p.n.}\n\\]\nAnatolii Volodymyrovych Skorokhod (1930-2011)\nProof. Przedstawimy dowód dla \\(d=1\\).\nNiech \\(F_n\\) \\(F\\) będą dystrybuantami odpowiednio \\(X_n\\) oraz \\(X\\).\nSkoro \\(X_n \\Rightarrow X\\), \\(F_n(x) \\F(x)\\) dla \\(x \\\\mathbb{R} \\setminus \\mathrm{D}_F\\), gdzie\n\\[\\begin{equation*}\n\\mathrm{D}_F = \\{ x \\\\mathbb{R} \\: : \\: \\mbox{$F$ nie jest ciągła w $x$} \\}.\n\\end{equation*}\\]\nWiemy, że zbiór \\(\\mathrm{D}_F\\) jest co najwyżej przeliczalny. Stąd\n\\(F_n \\F\\) prawie wszędzie względem jednowymiarowej miary Lebesgue’.\nNaturalnymi kandydatami na zmienne \\(X_n'\\) oraz \\(X'\\) są zatem\n\\[\\begin{equation*}\nX_n'(\\omega) = F_n^{-1}(\\omega) = \\sup \\{ y \\\\mathbb{R} \\: : \\: F_n(y) <\\omega\\}\n\\end{equation*}\\]\noraz\n\\[\\begin{equation*}\nX'(\\omega) = F^{-1}(\\omega) = \\sup \\{ y \\\\mathbb{R} \\: : \\: F(y) <\\omega\\}.\n\\end{equation*}\\]\nNależy teraz sprawdzić, że \\(X_n'\\X'\\) p.w.Zaczynamy od identyfikacji zbioru na którym zachodzi zbieżność.\nNiech \\(a_x = \\sup\\{ y : F(y) < x \\}\\), \\(b_x = \\inf\\{ y : F(y) > x \\}\\),\noraz \\(\\Omega_0 = \\{ x : (a_x, b_x) = \\emptyset \\}\\).\n\\(\\Omega - \\Omega_0\\) jest zbiorem przeliczalnym, ponieważ przedziały \\((a_x, b_x)\\)\nsą rozłączne każdy niepusty przedział otwarty zawiera liczbę wymierną.\nJeśli \\(x \\\\Omega_0\\), \\(F(y) < x\\) dla \\(y < F^{-1}(x)\\) oraz \\(F(z) > x\\) dla \\(z > F^{-1}(x)\\).\nAby pokazać, że \\(F_n^{-1}(x) \\F^{-1}(x)\\) dla \\(x \\\\Omega_0\\), trzeba pokazać dwie rzeczy:\\(\\liminf_{n \\\\infty} F_n^{-1}(x) \\geq F^{-1}(x)\\)Dowód .\nNiech \\(y < F^{-1}(x)\\) będzie taki, że \\(F\\) jest ciągła w punkcie \\(y\\).\nPonieważ \\(x \\\\Omega_0\\), \\(F(y) < x\\) jeśli \\(n\\) jest dostatecznie duże, \\(F_n(y) < x\\),\nczyli \\(F_n^{-1}(x) \\geq y\\). Ponieważ zachodzi dla wszystkich \\(y\\) spełniających zadany warunek,\nwynik podpunktu () jest dowiedziony.\\(\\limsup_{n \\\\infty} F_n^{-1}(x) \\leq F^{-1}(x)\\)Dowód b.\nNiech \\(y > F^{-1}(x)\\) będzie taki, że \\(F\\) jest ciągła w punkcie \\(y\\).\nPonieważ \\(x \\\\Omega_0\\), \\(F(y) > x\\) dla dostatecznie dużego \\(n\\),\n\\(F_n(y) > x\\), czyli \\(F_n^{-1}(x) \\leq y\\).\nPonieważ zachodzi dla wszystkich \\(y\\) spełniających zadany warunek,\npociąga podpunkt b.□Niech \\(g \\colon \\mathbb{R}^d \\\\mathbb{R}^m\\) będzie mierzalną funkcją oraz niech\n\\[\\begin{equation*}\n  \\mathrm{D}_g = \\left\\{ x \\\\mathbb{R}^d : g \\text{ nie jest ciągła w } x \\right\\}.\n\\end{equation*}\\]\nWówczas \\(\\mathrm{D}_g\\) jest borelowskim podzbiorem \\(\\mathbb{R}^d\\).Twierdzenie 24.5  (Twierdzenie o odwzorowaniu ciągłym) Niech \\(g\\) będzie mierzalną funkcją.\nJeśli \\(X_n \\X_\\infty\\) oraz \\(P(X_\\infty \\D_g) = 0\\), wtedy \\(g(X_n) \\Rightarrow g(X)\\).Proof. Zastosujemy Twierdzenie Skorohoda.\nNiech \\(Y_n \\overset{d}{=} X_n\\) \\(Y_n \\Y_\\infty\\) prawie na pewno.\nJeśli \\(f\\) jest funkcją ciągłą, \\(\\mathrm{D}_{f \\circ g} \\subseteq \\mathrm{D}_g\\),\nwięc \\(\\mathbb{P}(Y_\\infty \\\\mathrm{D}_{f \\circ g}) = 0\\),\nzatem \\(f(g(Y_n)) \\f(g(Y_\\infty))\\) prawie na pewno.\nJeśli dodatkowo \\(f\\) jest ograniczona, z twierdzenia o zbieżności ograniczonej mamy:\n\\[\n\\mathbb{E} \\left[ f(g(Y_n)) \\right] \\\\mathbb{E} \\left[ f(g(Y_\\infty)) \\right].\n\\]\nPonieważ zachodzi dla wszystkich ograniczonych ciągłych funkcji \\(f\\),\n\\[\ng(X_n) \\Rightarrow g(X_\\infty).\n\\]□","code":""},{"path":"zbieżność-według-rozkładu.html","id":"ciasne-rodziny-miar","chapter":"24 Zbieżność według rozkładu","heading":"24.4 Ciasne rodziny miar","text":"Podczas pierwszego kursu z analizy matematycznej dowodzi się fakt, że z ograniczonego\nciągu liczb rzeczywistych zawsze można wybrać podciąg zbieżny.\nTen niepozorny fakt jest bardzo użyteczny przy analizie własności funkcji ciągłych.\nChcemy znaleźć analogon tej własności dla słabej zbieżności.Definicja 24.4  Dystrybuanta ułomna jest funkcja \\(F\\) spełniająca warunki:\\(F\\) jest niemalejąca;\\(0\\le F(t)\\le 1\\) dla każdego \\(t \\\\mathbb{R}\\);\\(F\\) jest prawostronnie ciągła;Z powyższej definicji wynika, że \\(\\lim_{t\\\\infty} F(t) - \\lim_{t\\-\\infty} F(t) = p\\le 1\\),\nw szczególności może się zdarzyć, że \\(p<1\\).Rozważamy zbiór dystrybuant ułomnych wraz ze słabą zbieżnością jak powyżej, tzn mówimy,\nże ciąg dystrybuant ułomnych \\(F_n\\) zbiega słabo dystrybuanty ułomnej \\(F\\),\njeżeli \\(F_n(x)\\F(x)\\) dla każdego \\(x\\) będącego punktem ciągłości \\(F\\).Twierdzenie 24.6  (Helly'ego o wyborze) Każdy ciąg dystrybuant (ułomnych) zawiera podciąg słabo zbieżny dystrybuanty ułomnej.\nEduard Helly (1884-1943)\nProof. Pokażemy, używając metody przekątniowej, że dowolny ciąg dystrybuant zawiera podciąg,\nktóry jest zbieżny na zbiorze liczb wymiernych,\nczyli na gęstym podzbiorze \\(\\mathbb{R}\\) pewnej funkcji \\(F_0\\).\nFunkcja ta może nie być uzupełniona dystrybuanty ułomnej liniowo.Ustawmy w ciąg liczby wymierne: \\(w_1,w_2,\\ldots\\) Ciąg \\(F_n(w_1)\\) jest ograniczony możemy więc wybrać z niego podciąg zbieżny:\n\\(F_{n_{1,1}}(w_1),\n  F_{n_{1,2}}(w_1), \\ldots\\):\n\\[\n  \\lim_{k\\\\infty} F_{n_{1,k}}(w_1) =: F_0(w_1)\n  \\]\nPatrzymy teraz na ciąg dystrybuant \\(\\{F_{n_{1,k}}\\}_k\\). Ponownie, ciąg \\(F_{n_{1,k}}(w_2)\\) jest ograniczony, więc zawiera podciąg zbieżny\n\\[\n  \\lim_{k\\\\infty} F_{n_{2,k}}(w_2) =: F_0(w_2).\n  \\] Kontynuując rozumowanie w \\(j\\)-tym kroku wybieramy podciąg taki, że\n\\[\n  \\lim_{k\\\\infty} F_{n_{j,k}}(w_j) =: F_0(w_j).\n  \\] Metodą przekątniową wybieramy ciąg dystrybuant \\(F_{n_{k,k}}\\). Jest ciąg, który jest zbieżny w każdym punkcie wymiernym \\(w\\), \\(F_0(w)\\), ponieważ z konstrukcji dla każdego \\(j\\) ciąg \\(\\{F_{k,k}\\}_{k\\ge j}\\) jest podciągiem \\(\\{F_{j,}\\}_i\\).\nFunkcja \\(F_0\\) jest więc zdefiniowana na zbiorze liczb wymiernych chcemy ją przedłużyć całej prostej. Zdefiniujmy\n\\[\n  F(t) =\\inf\\{ F_0(w): t<w\\{\\mathbb Q} \\}, \\quad t\\\\mathbb{R}.\n  \\] Wtedy \\(F\\) jest dystrybuantą ułomną (szczegóły pozostawiamy czytelnikowi). Pozostaje sprawdzenia, że \\(F_{n_{k,k}}\\) zbiega \\(F\\) wszystkich punktach ciągłości \\(F\\).Niech \\(x\\) będzie punktem ciągłości \\(F\\) niech \\(x_1,x_2, x_3\\{\\mathbb  Q}\\) będą takie, że \\(x_1< x_2 < x < x_3\\), wówczas:\n\\[\n  F_{n_{k,k}}(x_2) \\le F_{n_{k,k}}(x) \\le F_{n_{k,k}}(x_3),\n  \\] co implikuje\n\\[\\begin{multline*}\n  F(x_1) \\le F_0(x_2) \\le  \\liminf_{k\\\\infty} F_{n_{k,k}}(x_2) \\le \\liminf_{k\\\\infty} F_{n_{k,k}}(x)\\\\ \\le \\limsup_{k\\\\infty} F_{n_{k,k}}(x) \\le \\limsup_{k\\\\infty} F_{n_{k,k}}(x_3) = F_0(x_3) \\le F(x_3),\n  \\end{multline*}\\]\nPrzypomnijmy, że \\(x\\) jest punktem ciągłości \\(F\\), przechodząc więc z \\(x_1 \\nearrow x\\) oraz \\(x_3 \\searrow x\\), otrzymujemy\n\\[ \\lim_{k\\\\infty} F_{n_{k,k}}(x) = F(x).\n      \\]\n□Przykład 24.10  Niech\n\\[\n  F_n(x) = \\left\\{\n  \\begin{array}{cc}\n    0\\ & x < n  \\\\\n    1\\ & x\\ge n\n  \\end{array}\n  \\right.\n  \\] Wtedy \\(F_n(x)\\0\\), więc słabą granicą ciągu dystrybuant \\(F_n\\) jest dystrybuanta ułomna\ntożsamościowo równa \\(0\\).Nieznacznie modyfikując ten przykład definiując\n\\[ F_n(x) = \\left\\{\n  \\begin{array}{cc}\n    0\\ & x < -n  \\\\\n    1\\ & x\\ge -n\n  \\end{array}\n  \\right.\\]\notrzymujemy ciąg dystrybuant słabo zbieżny ułomnej dystrybuanty tożsamościowo równej 1.Definicja 24.5  Rodzina \\(\\{\\mu_t\\}_{t\\T}\\) rozkładów prawdopodobieństwa na\n\\((\\mathbb{R}^d,\\mathcal{B}(\\mathbb{R}^d))\\) nazywana jest ciasną jeżeli\ndla każdego \\(\\varepsilon >0\\) istnieje zbiór zwarty \\(K\\subseteq \\mathbb{R}^d\\) taki, że\n\\[\n  \\mu_t(K) > 1-\\varepsilon,\\quad \\forall t\\T.\n  \\]Powyższy warunek mówi, że większość mas miar \\(\\mu_t\\) jest skupiona w pobliżu siebie.Przykład 24.11  Niech \\(T = [1,+\\infty)\\) niech \\(\\mu_t\\) będzie rozkładem wykładniczym z parametrem \\(t\\).\nWówczas rodzina miar \\(\\{\\mu_t\\}_{t \\T}\\) jest ciasna. Dla \\(k\\\\mathbb{N}\\)\noraz \\(t \\T\\) mamy\n\\[\\begin{equation*}\n\\mu_t([0,k]) = 1-e^{-tk}\\geq 1-e^{-k} \\geq 1-\\varepsilon\n\\end{equation*}\\]\ngdzie ostatnia nierówność jest spełniona o ile \\(k \\geq \\log(1/\\varepsilon)\\).\nZbiór \\(K = [0,k]\\) jest więc świadkiem na , że \\(\\{ \\mu_t\\}_{t \\T}\\) jest istotnie ciasna.Rodziny miar, które nie są ciasne charakteryzują się tym, że rozkłady skupione są na zbiorach,\nktórych suma mnogościowa jest nieograniczonym zbiorem \\(\\mathbb{R}^d\\).Przykład 24.12  Niech \\(T=\\mathbb{N}\\) oraz niech \\(\\mu_n = \\mathcal{N}(n,1)\\).\nDla wygody oznaczmy przez \\(X_n\\) zmienną losową o rozkładzie \\(\\mu_n\\). Wówczas\n\\[\\begin{equation*}\n\\mu_n([n-2,n+2]) = \\mathbb{P}[|X_n-n|\\leq 2].\n\\end{equation*}\\]\nZ nierówności Czebyszewa\n\\[\\begin{equation*}\n\\mathbb{P}[|X_n-n|\\geq 2]\\leq 1/4.\n\\end{equation*}\\]\nczyli\n\\[\\begin{equation*}\n\\mu_n([n-2,n+2]) \\geq 3/4.\n\\end{equation*}\\]\nDla każdego zwartego \\(K \\subseteq \\mathbb{R}\\) znajdziemy zawsze takie \\(n \\\\mathbb{N}\\), że\n\\(K \\cap [n-2, n+2] = \\emptyset\\). Wówczas\n\\[\\begin{equation*}\n\\mu_n(K) \\leq 1/4.\n\\end{equation*}\\]\nRodzina \\(\\{\\mu_n\\}_{n \\\\mathbb{N}}\\) nie jest zatem ciasna.Przykład 24.13  Niech \\(\\{X_t\\}_{t\\T}\\) będzie rodziną zmiennych losowych takich, że istnieje \\(\\delta >0\\):\n\\[\n  \\sup_{t\\T}\\mathbb{E}|X_t|^\\delta = M < \\infty.\n  \\] Wtedy rodzina \\(\\{\\mu_{X_t}\\}\\) jest ciasna.Mianowicie, ustalmy \\(\\varepsilon>0\\), wówczas dla każdego \\(t\\T\\)\n\\[\n\\mu_t\\big( [-n,n]^c\\big) =  \\mathbb{P}[|X_t|>n] \\le \\mathbb{P}\\big[|X_t|^{\\delta}>n^{\\delta}\\big] \\le \\frac{\\mathbb{E} |X_t|^\\delta}{n^\\delta} \\le \\frac{M}{n^\\delta} < \\varepsilon\n  \\] dla dużych wartości \\(n\\).Twierdzenie 24.7  (Prochorowa) Rodzina rozkładów prawdopodobieństwa jest ciasna wtedy tylko wtedy,\ngdy z każdego ciągu elementów tej rodziny można wybrać podciąg słabo zbieżny\npewnego rozkładu prawdopodobieństwa.\nYuri Vasilyevich Prokhorov (1929-2013)\nProof. Niech \\(F_n\\) będzie ciasnym ciągiem dystrybuant. Z twierdzenia Helly’ego istnieje podciąg \\(F_{n_k}\\) zbieżny pewnej dystrybuanty \\(F\\) (być może ułomnej). Z założenia ciasności dla każdego \\(\\varepsilon > 0\\) istnieją punkty \\(M_{\\varepsilon}\\) \\(-M_{\\varepsilon}\\) takie, że\\(M_{\\varepsilon}\\) \\(-M_{\\varepsilon}\\) są punktami ciągłości \\(F\\);\\(F_{n_k}(M_{\\varepsilon}) - F_{n_k}(-M_{\\varepsilon})\\ge 1-\\varepsilon\\) dla każdego \\(n_k\\).Zatem przechodząc granicy\n\\[\\lim_{x\\\\infty} F(x) - \\lim_{x\\-\\infty} F(x) \\ge   F(M_{\\varepsilon}) - F(-M_{\\varepsilon}) \\ge 1-\\varepsilon.\n  \\] Z dowolności \\(\\varepsilon\\), \\(F\\) jest dystrybuantą (nie ułomną!) pewnego rozkładu .Załóżmy teraz, że z każdego ciągu elementów tej rodziny można wybrać podciąg słabo zbieżny pewnego rozkładu .\nPrzypuśćmy, że rodzina miar nie jest ciasna, więc istnieje \\(\\varepsilon > 0\\), ciąg miar \\(\\mu_n\\) oraz liczby \\(M_n\\nearrow \\infty\\) takie, że\n\\[ \\mu_n([-M_n,M_n]) <1-\\varepsilon.\n\\] Zgodnie z założeniem istnieje podciąg miar \\(\\mu_{n_k}\\) słabo zbieżny pewnej miary probabilistycznej \\(\\mu\\). Weźmy zbiór postaci\n\\((-N,N)\\) taki, że \\(\\mu(-N,N) > 1-\\varepsilon\\). Wówczas korzystając z twierdzenia Portmanteau:\n\\[\n1-\\varepsilon < \\mu(-N,N) \\le \\liminf \\mu_{n_k}(-N,N) \\le \\liminf \\mu_{n_k} ([-M_k, M_k]) \\le 1-\\varepsilon,\n\\] co doprowadza nas sprzeczności, zatem rodzina tych miar musi być ciasna.","code":""},{"path":"lista-1-rozgrzewka.html","id":"lista-1-rozgrzewka","chapter":"Lista 1: Rozgrzewka","heading":"Lista 1: Rozgrzewka","text":"Zadania na ćwiczenia: 2025-02-24","code":""},{"path":"lista-1-rozgrzewka.html","id":"zadania-do-samodzielnego-rozwiązania","chapter":"Lista 1: Rozgrzewka","heading":"Zadania do samodzielnego rozwiązania","text":"Na szachownicy o wymiarach \\(n\\times n\\) umieszczono \\(8\\) nierozróżnialnych wież, w taki sposób aby żadne dwie się nie biły.\nNa ile sposobów można zrobić? Jak zmieni się wynik, gdy wieże będą rozróżnialne?\n\nOdpowiedź\n\nJeżeli wieże nie są rozróżnialne \\({n \\choose 8}^28!\\)\njeżeli wieże są rozróżnialne \\({n \\choose 8}^2(8!)^2\\).\nNa ile sposobów można rozmieścić \\(n\\) kul w \\(k\\) urnach jeżeli: () kule są rozróżnialne, (b) kule są nierozróżnialne.\n\nOdpowiedź\n\n\n\n\\(n^k\\) b \\({n+k-1 \\choose k-1}\\)\n\n\n\\(n^k\\) b \\({n+k-1 \\choose k-1}\\)\nIle jest liczb mniejszych od \\(1000\\) podzielnych przez jedną z liczb \\(3\\), \\(5\\), \\(7\\)?\n\nOdpowiedź\n\n\n\n\\(542\\)\n\n\n\\(542\\)\nNa płaszczyznie danych jest pięć punktów kratowych (o obu współrzędnych całkowitych).\nWykazać, ze środek jednego z odcinków łączacych te punkty również jest kratowy\n\nOdpowiedź\n\n\n\nNależy pokazać, że istnieje para punktów na płaszczyźnie, której współrzędne mają tę samą parzystość.\nTeza wynika z zastosowania zasady szufladkowej Dirichleta.\n\n\nNależy pokazać, że istnieje para punktów na płaszczyźnie, której współrzędne mają tę samą parzystość.\nTeza wynika z zastosowania zasady szufladkowej Dirichleta.\nOblicz prawdopodobieństwo zdarzenia, że w potasowanej talii \\(52\\) kart wszystkie cztery asy znajdują się koło siebie.\n\nOdpowiedź\n\n\n\n\\(4!/(52\\cdot 51\\cdot 50)\\)\n\n\n\\(4!/(52\\cdot 51\\cdot 50)\\)\n","code":""},{"path":"lista-1-rozgrzewka.html","id":"zadania-na-ćwiczenia","chapter":"Lista 1: Rozgrzewka","heading":"Zadania na ćwiczenia","text":"Na ile sposobów można ustawić \\(7\\) krzeseł białych \\(3\\) czerwone przy okrągłym stole?Na ile sposobów można ustawić \\(7\\) krzeseł białych \\(3\\) czerwone przy okrągłym stole?Zsumuj\n\\[\\begin{equation*}\n\\sum_{k=0}^m{n+k \\choose j}\n\\end{equation*}\\]\ngdzie \\(m,n,j \\\\mathbb{N}\\) są liczbami naturalnymi.\nZapisz \\(k^2\\) jako \\(a_2{k \\choose 2 }+ a_1{ k \\choose 1} + a_0{k \\choose 0}\\).\nWykorzystaj policzenia \\(\\sum_{k=0}^n{k^2}\\)Zsumuj\n\\[\\begin{equation*}\n\\sum_{k=0}^m{n+k \\choose j}\n\\end{equation*}\\]\ngdzie \\(m,n,j \\\\mathbb{N}\\) są liczbami naturalnymi.\nZapisz \\(k^2\\) jako \\(a_2{k \\choose 2 }+ a_1{ k \\choose 1} + a_0{k \\choose 0}\\).\nWykorzystaj policzenia \\(\\sum_{k=0}^n{k^2}\\)Wachlarzem rzędu \\(n\\) nazywamy graf o wierzchołkach \\(\\{0, 1, \\ldots , n\\}\\) z \\(2n-1\\) krawędziami zdefiniowanymi\nnastępująco: wierzchołek \\(0\\) połączony jest krawędzią z każdym z pozostałych wierzchołków wierzchołek \\(k\\) połączony\njest krawędzią z wierzchołkiem \\(k+1\\) dla każdego \\(1 \\leq k<n\\). Dla przykładu wachlarz rzędu \\(5\\) wygląda następująco\n\n\n\nPrzyjmując \\(S_0=0\\), ile wynosi \\(S_n\\) liczba drzew rozpinających na takim grafie?\nDrzewem rozpinającym nazywamy spójny podgraf bez cykli zawierający wszystkie wierzchołki\n\nWskazówka\n\n\n\nZnajdź zależność rekurencyjną na \\(S_n\\).\n\nWachlarzem rzędu \\(n\\) nazywamy graf o wierzchołkach \\(\\{0, 1, \\ldots , n\\}\\) z \\(2n-1\\) krawędziami zdefiniowanymi\nnastępująco: wierzchołek \\(0\\) połączony jest krawędzią z każdym z pozostałych wierzchołków wierzchołek \\(k\\) połączony\njest krawędzią z wierzchołkiem \\(k+1\\) dla każdego \\(1 \\leq k<n\\). Dla przykładu wachlarz rzędu \\(5\\) wygląda następującoPrzyjmując \\(S_0=0\\), ile wynosi \\(S_n\\) liczba drzew rozpinających na takim grafie?\nDrzewem rozpinającym nazywamy spójny podgraf bez cykli zawierający wszystkie wierzchołki\n\nWskazówka\n\nZnajdź zależność rekurencyjną na \\(S_n\\).\nW klasie jest \\(15\\) uczniów. Na każdej lekcji odpytywany jest losowo jeden z nich.\nOblicz prawdopodobieństwo, że podczas \\(16\\) lekcji zostanie przepytany każdy z nich.W klasie jest \\(15\\) uczniów. Na każdej lekcji odpytywany jest losowo jeden z nich.\nOblicz prawdopodobieństwo, że podczas \\(16\\) lekcji zostanie przepytany każdy z nich.W Totolotku losuje się \\(6\\) z \\(49\\) liczb. Jakie jest prawdopodobieństwo, że żadne dwie nie będą dwiema\nkolejnymi liczbami naturalnymi?W Totolotku losuje się \\(6\\) z \\(49\\) liczb. Jakie jest prawdopodobieństwo, że żadne dwie nie będą dwiema\nkolejnymi liczbami naturalnymi?Stefan Banach w każdej z kieszeni trzymał po pudełku zapałek.\nPoczątkowo każde z nich zawierało \\(n\\) zapałek. Za każdym razem kiedy Banach potrzebował zapałki sięgał losowo jednej\nz kieszeni wyciągał jedną zapałkę.\nOblicz prawdopodobieństwo, że w momencie gdy sięgnął po puste pudełko, w drugim pozostało jeszcze \\(k\\) zapałek.Stefan Banach w każdej z kieszeni trzymał po pudełku zapałek.\nPoczątkowo każde z nich zawierało \\(n\\) zapałek. Za każdym razem kiedy Banach potrzebował zapałki sięgał losowo jednej\nz kieszeni wyciągał jedną zapałkę.\nOblicz prawdopodobieństwo, że w momencie gdy sięgnął po puste pudełko, w drugim pozostało jeszcze \\(k\\) zapałek.Oblicz\n\\[\\begin{equation*}\n\\int_{-\\infty}^{\\infty} e^{-x^2} \\: \\mathrm{d}x.\n   \\end{equation*}\\]\n\nWskazówka\n\n\n\nPodnieś całkę kwadratu zastosuj współrzędne biegunowe.\n\nOblicz\n\\[\\begin{equation*}\n\\int_{-\\infty}^{\\infty} e^{-x^2} \\: \\mathrm{d}x.\n   \\end{equation*}\\]\n\nWskazówka\n\nPodnieś całkę kwadratu zastosuj współrzędne biegunowe.\nGrupa składająca się z \\(2n\\) pań \\(2n\\) panów została podzielona na dwie równoliczne grupy.\nZnaleźć prawdopodobieństwo, że każda z tych grup składa się z takiej samej liczby pań panów.\nPrzybliżyć prawdopodobieństwo za pomocą wzoru Stirlinga.Grupa składająca się z \\(2n\\) pań \\(2n\\) panów została podzielona na dwie równoliczne grupy.\nZnaleźć prawdopodobieństwo, że każda z tych grup składa się z takiej samej liczby pań panów.\nPrzybliżyć prawdopodobieństwo za pomocą wzoru Stirlinga.","code":""},{"path":"lista-1-rozgrzewka.html","id":"zadania-dodatkowe","chapter":"Lista 1: Rozgrzewka","heading":"Zadania dodatkowe","text":"Pewien sułtan więził 100 osób. Pewnego dnia postanowił ich zgładzić.\nJako, że był znany ze swego miłosierdzia dał im ostatnią szansę.\nPostawił przed nimi następujące zadanie. Każdemu więźniowi przyporządkował liczbę.\nNastępnie w pokoju obok umieścił w rzędzie kolejno \\(100\\) pudełek każdego z nich włożył losową liczbę od \\(1\\) \\(100\\)\n(w każdym pudełku inną). Wieżniowie po kolei, pojedynczo, wchodzą pokoju z pudełkami.\nMogą otworzyć \\(50\\) pudełek, aby znaleźć swój numer, ale pokój muszą pozostawić dokładnie w takim samym stanie w jakim\ngo zastali. Następnie opuszczają pokój wychodząc innym wyjściem nie mają możliwości skontaktowania się z\npozostałymi osobami. Więźniowie zostaną ocaleni, jeżeli z nich każdy znajdzie swój numer.\nJeżeli każdy z nich otwiera losowe 50 pudełek, szanse ich przeżycia wynoszą\n\\(2^{-100} \\approx 7,8*10^{-31}\\). Czy mają oni lepszą strategię?","code":""},{"path":"lista-2-aksjomaty-rachunku-prawdopodobieństwa.html","id":"lista-2-aksjomaty-rachunku-prawdopodobieństwa","chapter":"Lista 2: Aksjomaty rachunku prawdopodobieństwa","heading":"Lista 2: Aksjomaty rachunku prawdopodobieństwa","text":"Zadania na ćwiczenia: 2025-03-03","code":""},{"path":"lista-2-aksjomaty-rachunku-prawdopodobieństwa.html","id":"zadania-do-samodzielnego-rozwiązania-1","chapter":"Lista 2: Aksjomaty rachunku prawdopodobieństwa","heading":"Zadania do samodzielnego rozwiązania","text":"Ze zbioru \\([100]=\\{1,  \\ldots  100 \\}\\) wylosowano ze zwracaniem dwie liczby \\(L\\) \\(M\\).\nZdefiniuj odpowiednią przestrzeń probabilistyczną.\nOblicz prawdopodobieństwo, że średnia arytmetyczna \\(L\\) \\(M\\) jest ściśle mniejsza niż \\(30\\).\n\nOdpowiedź\n\n\n\n\\(\\Omega = [100]^2\\), \\(\\mathcal{F}=2^\\Omega\\), \\(\\mathbb{P}[\\{(l,m)\\} ]=100^{-2}\\).\nSzukane prawdopodobieństwo \\(58\\cdot 59/20000\\).\n\n\n\\(\\Omega = [100]^2\\), \\(\\mathcal{F}=2^\\Omega\\), \\(\\mathbb{P}[\\{(l,m)\\} ]=100^{-2}\\).\nSzukane prawdopodobieństwo \\(58\\cdot 59/20000\\).\nWybrano losowy punkt \\((x,y)\\) z kwadratu \\([0,1]\\times [0,1]\\). Zdefiniuj odpowiednią przestrzeń probabilistyczną. Oblicz prawdopodobieństwo, że\n\\(x\\) jest liczbą wymierną;\nobie liczby \\(x\\) \\(y\\) są niewymierne;\nspełniona jest nierówność \\(x^2+y^2 < 1\\);\nspełniona jest równość \\(x^2+y^2 = 1\\).\n\nOdpowiedź\n\n\n\n\\(\\Omega = [0,1]^2\\), \\(\\mathcal{F} = \\mathcal{B}([0,1]^2)\\), \\(\\mathbb{P}\\) jest dwuwymiarową\nmiarą Lebesgue’. ) 0; b) 1; c) \\(\\pi/4\\); d) 0\n\n\n\\(x\\) jest liczbą wymierną;obie liczby \\(x\\) \\(y\\) są niewymierne;spełniona jest nierówność \\(x^2+y^2 < 1\\);spełniona jest równość \\(x^2+y^2 = 1\\).\n\nOdpowiedź\n\n\n\n\\(\\Omega = [0,1]^2\\), \\(\\mathcal{F} = \\mathcal{B}([0,1]^2)\\), \\(\\mathbb{P}\\) jest dwuwymiarową\nmiarą Lebesgue’. ) 0; b) 1; c) \\(\\pi/4\\); d) 0\n\n\n\\(\\Omega = [0,1]^2\\), \\(\\mathcal{F} = \\mathcal{B}([0,1]^2)\\), \\(\\mathbb{P}\\) jest dwuwymiarową\nmiarą Lebesgue’. ) 0; b) 1; c) \\(\\pi/4\\); d) 0\nW kwadracie \\([0,1] \\times [0,1]\\) wybrano losowo dwa punkty \\(\\) \\(B\\).\nZdefiniuj odpowiednią przestrzeń probabilistyczną. Oblicz prawdopodobieństwo, że\nodcinek \\(AB\\) przecina przekątną łączącą wierzchołki \\((0,0)\\) \\((1,1)\\);\nodległość punktu \\(\\) od \\((1,1)\\) jest mniejsza niż 1,\nodległość punktu \\(B\\) od \\((1,1)\\) jest większa niż 1;\noba punkty leżą pod parabolą \\(y = - x(x-1)\\).\n\nOdpowiedź\n\n\n\n\\(\\Omega = [0,1]^4\\), \\(\\mathcal{F} = \\mathcal{B}([0,1]^4)\\), \\(\\mathbb{P}\\) jest czterowymiarową\nmiarą Lebesgue’. . \\(1/2\\) b. \\(\\pi/4\\) c. \\(1-\\pi/4\\) d. \\(1/36\\).\n\n\nodcinek \\(AB\\) przecina przekątną łączącą wierzchołki \\((0,0)\\) \\((1,1)\\);odległość punktu \\(\\) od \\((1,1)\\) jest mniejsza niż 1,odległość punktu \\(B\\) od \\((1,1)\\) jest większa niż 1;oba punkty leżą pod parabolą \\(y = - x(x-1)\\).\n\nOdpowiedź\n\n\n\n\\(\\Omega = [0,1]^4\\), \\(\\mathcal{F} = \\mathcal{B}([0,1]^4)\\), \\(\\mathbb{P}\\) jest czterowymiarową\nmiarą Lebesgue’. . \\(1/2\\) b. \\(\\pi/4\\) c. \\(1-\\pi/4\\) d. \\(1/36\\).\n\n\n\\(\\Omega = [0,1]^4\\), \\(\\mathcal{F} = \\mathcal{B}([0,1]^4)\\), \\(\\mathbb{P}\\) jest czterowymiarową\nmiarą Lebesgue’. . \\(1/2\\) b. \\(\\pi/4\\) c. \\(1-\\pi/4\\) d. \\(1/36\\).\nNiech \\(\\cup B \\cup C = \\Omega\\), \\(\\mathbb{P}[B] = 2 \\mathbb{P}[]\\),\n\\(\\mathbb{P}[C] = 3 \\mathbb{P}[]\\), \\(\\mathbb{P}[\\cap B] = \\mathbb{P}[\\cap C] =\n\\mathbb{P}[B \\cap C]\\). Pokaż, że \\(1/6 \\leq \\mathbb{P}[] \\leq 1/4\\),\nprzy czym oba ograniczenia są osiągalne.\n\nOdpowiedź\n\n\n\nMamy \\(1 = \\mathbb{P}[\\Omega] \\leq \\mathbb{P}[]\\) \\(+\\mathbb{P}[B]+\\mathbb{P}[C] = 6 \\mathbb{P}[]\\).\nStąd \\(\\mathbb{P}[]\\geq 1/6\\). ograniczenie jest osiągnięte, jeżeli zbiory \\(\\), \\(B\\) \\(C\\) są\nrozłączne. Z drugiej strony\n\\(1 = \\mathbb{P}[\\Omega] = \\mathbb{P}[C] + \\mathbb{P}[B\\setminus C]\\)\n\\(= \\mathbb{P}[C] +\\mathbb{P}[B] - \\mathbb{P}[B\\cap C] \\geq 4\\mathbb{P}[]\\).\nStąd \\(\\mathbb{P}[]\\leq 1/4\\).\nOgraniczenie jest osiągnięte kiedy \\(= B \\cap C\\).\n\n\nMamy \\(1 = \\mathbb{P}[\\Omega] \\leq \\mathbb{P}[]\\) \\(+\\mathbb{P}[B]+\\mathbb{P}[C] = 6 \\mathbb{P}[]\\).\nStąd \\(\\mathbb{P}[]\\geq 1/6\\). ograniczenie jest osiągnięte, jeżeli zbiory \\(\\), \\(B\\) \\(C\\) są\nrozłączne. Z drugiej strony\n\\(1 = \\mathbb{P}[\\Omega] = \\mathbb{P}[C] + \\mathbb{P}[B\\setminus C]\\)\n\\(= \\mathbb{P}[C] +\\mathbb{P}[B] - \\mathbb{P}[B\\cap C] \\geq 4\\mathbb{P}[]\\).\nStąd \\(\\mathbb{P}[]\\leq 1/4\\).\nOgraniczenie jest osiągnięte kiedy \\(= B \\cap C\\).\nPokaż, że jeżeli \\(\\{A_i\\}_{\\ge 1}\\) jest rodziną zdarzeń takich, że\n\\(\\mathbb{P}[A_i]=1\\) dla \\(\\ge 1\\), \\(\\mathbb{P}\\left[\\bigcap_{=1}^{\\infty} A_i\\right]=1\\).\n\nOdpowiedź\n\n\n\nJest wniosek z zadania 6.\n\n\nJest wniosek z zadania 6.\n","code":""},{"path":"lista-2-aksjomaty-rachunku-prawdopodobieństwa.html","id":"zadania-na-ćwiczenia-1","chapter":"Lista 2: Aksjomaty rachunku prawdopodobieństwa","heading":"Zadania na ćwiczenia","text":"Udowodnij nierówność Boole’\n\\[\n\\mathbb{P}\\left[\\bigcup_{=1}^n A_i\\right] \\; \\leq \\; \\sum_{=1}^n \\mathbb{P}[A_i].\n\\]\nWywnioskuj, że\n\\[\n\\mathbb{P}\\left[\\bigcap_{=1}^n A_i\\right] \\; \\geq \\; 1-  \\sum_{=1}^n \\mathbb{P}\\left[A_i^c\\right].\n\\]Udowodnij nierówność Boole’\n\\[\n\\mathbb{P}\\left[\\bigcup_{=1}^n A_i\\right] \\; \\leq \\; \\sum_{=1}^n \\mathbb{P}[A_i].\n\\]\nWywnioskuj, że\n\\[\n\\mathbb{P}\\left[\\bigcap_{=1}^n A_i\\right] \\; \\geq \\; 1-  \\sum_{=1}^n \\mathbb{P}\\left[A_i^c\\right].\n\\]Udowodnij nierówność Bonferroniego\n\\[\\begin{equation*}\n     \\mathbb{P}\\left[\\bigcup_{=1}^n A_i\\right] \\; \\geq \\;\n     \\sum_{=1}^n \\mathbb{P}[A_i] - \\sum_{1 \\leq < j \\leq n} \\mathbb{P}[A_i \\cap A_j]\n\\end{equation*}\\]Udowodnij nierówność Bonferroniego\n\\[\\begin{equation*}\n     \\mathbb{P}\\left[\\bigcup_{=1}^n A_i\\right] \\; \\geq \\;\n     \\sum_{=1}^n \\mathbb{P}[A_i] - \\sum_{1 \\leq < j \\leq n} \\mathbb{P}[A_i \\cap A_j]\n\\end{equation*}\\]Udowodnij wzór włączeń wyłączeń\n\\[\\begin{multline*}\n  \\mathbb{P}[A_1\\cup A_2\\cup \\ldots \\cup A_n] = \\sum_{=1}^n\\mathbb{P}[A_i]+ \\\\\n-\\sum_{<j}\\mathbb{P}[A_i\\cap A_j] + \\sum_{<j<k}\\mathbb{P}[A_i\\cap A_j\\cap A_k]\n  \\\\ + \\ldots +(-1)^{n+1} \\mathbb{P}[A_1\\cap A_2\\cap \\ldots \\cap A_n]\n\\end{multline*}\\]Udowodnij wzór włączeń wyłączeń\n\\[\\begin{multline*}\n  \\mathbb{P}[A_1\\cup A_2\\cup \\ldots \\cup A_n] = \\sum_{=1}^n\\mathbb{P}[A_i]+ \\\\\n-\\sum_{<j}\\mathbb{P}[A_i\\cap A_j] + \\sum_{<j<k}\\mathbb{P}[A_i\\cap A_j\\cap A_k]\n  \\\\ + \\ldots +(-1)^{n+1} \\mathbb{P}[A_1\\cap A_2\\cap \\ldots \\cap A_n]\n\\end{multline*}\\]Dziesięć małżeństw usiadło losowo przy okrągłym stole.\nOblicz prawdopodobieństwo, że żaden mąż nie siedzi przy swojej żonie.Dziesięć małżeństw usiadło losowo przy okrągłym stole.\nOblicz prawdopodobieństwo, że żaden mąż nie siedzi przy swojej żonie.Podczas imprezy mikołajkowej wszystkie \\(n\\) prezentów pozbawiono karteczek z imieniem adresata\nlosowo rozdano uczestnikom. Niech \\(p_n\\) oznacza prawdopodobieństwo,\nże dokładnie jedna osoba dostanie własny prezent.\nOblicz \\(p_n\\) oraz \\(\\lim_{n \\\\infty} p_n\\).Podczas imprezy mikołajkowej wszystkie \\(n\\) prezentów pozbawiono karteczek z imieniem adresata\nlosowo rozdano uczestnikom. Niech \\(p_n\\) oznacza prawdopodobieństwo,\nże dokładnie jedna osoba dostanie własny prezent.\nOblicz \\(p_n\\) oraz \\(\\lim_{n \\\\infty} p_n\\).Rzucamy symetryczną monetą chwili otrzymania orła.\nZdefiniuj odpowiednią przestrzeń probabilistyczną.\nJaka jest szansa, że liczba rzutów będzie parzysta?\npodzielna przez \\(3\\)? podzielna przez \\(m\\)?Rzucamy symetryczną monetą chwili otrzymania orła.\nZdefiniuj odpowiednią przestrzeń probabilistyczną.\nJaka jest szansa, że liczba rzutów będzie parzysta?\npodzielna przez \\(3\\)? podzielna przez \\(m\\)?Z przedziału \\([0,1]\\) wybrano losowo dwa punkty, które podzieliły go na trzy odcinki.\nObliczyć prawdopodobieństwo, że z tych odcinków można skonstruować trójkąt.Z przedziału \\([0,1]\\) wybrano losowo dwa punkty, które podzieliły go na trzy odcinki.\nObliczyć prawdopodobieństwo, że z tych odcinków można skonstruować trójkąt.Na nieskończoną szachownicę o boku \\(\\) rzucamy monetę o średnicy \\(2r < \\).\nZnaleźć prawdopodobieństwo, że\nmoneta znajdzie się całkowicie wnętrzu jednego z pól;\nmoneta przetnie się z co najwyżej jednym bokiem pola na szachownicy.\nNa nieskończoną szachownicę o boku \\(\\) rzucamy monetę o średnicy \\(2r < \\).\nZnaleźć prawdopodobieństwo, żemoneta znajdzie się całkowicie wnętrzu jednego z pól;moneta przetnie się z co najwyżej jednym bokiem pola na szachownicy.Igłę o długości \\(l\\) rzucono na podłogę z desek o szerokości \\(\\geq l\\).\nZnajdź prawdopodobieństwo, że igła przetnie krawędź deski.Igłę o długości \\(l\\) rzucono na podłogę z desek o szerokości \\(\\geq l\\).\nZnajdź prawdopodobieństwo, że igła przetnie krawędź deski.","code":""},{"path":"lista-2-aksjomaty-rachunku-prawdopodobieństwa.html","id":"zadania-dodatkowe-1","chapter":"Lista 2: Aksjomaty rachunku prawdopodobieństwa","heading":"Zadania dodatkowe","text":"Niech \\((\\Omega, \\mathcal{F})\\) będzie przestrzenią mierzalną.\nUzasadnij, że \\(\\sigma\\)-ciało \\(\\mathcal{F}\\) nie może być nieskończoną przeliczalną rodziną zbiorów.Niech \\((\\Omega, \\mathcal{F})\\) będzie przestrzenią mierzalną.\nUzasadnij, że \\(\\sigma\\)-ciało \\(\\mathcal{F}\\) nie może być nieskończoną przeliczalną rodziną zbiorów.Oznaczmy przez \\(\\mathcal B_0\\) ciało składające się ze skończonych sum\nrozłącznych przedziałów \\((,b]\\) zawartych w odcinku \\((0,1]\\). Określmy na\n\\(\\mathcal B_0\\) funkcję \\(\\mathbb{P}\\) taką, że \\(\\mathbb{P}() = 1\\) lub \\(0\\) w zależności od tego,\nczy zbiór \\(\\) zawiera przedział postaci \\((1/2,1/2+\\varepsilon]\\) dla pewnego \\(\\varepsilon>0\\),\nczy też nie. Pokaż, że \\(\\mathbb{P}\\) jest miarą addytywną, ale nie przeliczalnie addytywną.Oznaczmy przez \\(\\mathcal B_0\\) ciało składające się ze skończonych sum\nrozłącznych przedziałów \\((,b]\\) zawartych w odcinku \\((0,1]\\). Określmy na\n\\(\\mathcal B_0\\) funkcję \\(\\mathbb{P}\\) taką, że \\(\\mathbb{P}() = 1\\) lub \\(0\\) w zależności od tego,\nczy zbiór \\(\\) zawiera przedział postaci \\((1/2,1/2+\\varepsilon]\\) dla pewnego \\(\\varepsilon>0\\),\nczy też nie. Pokaż, że \\(\\mathbb{P}\\) jest miarą addytywną, ale nie przeliczalnie addytywną.","code":""},{"path":"lista-3-prawdopodobieństwo-warunkowe.html","id":"lista-3-prawdopodobieństwo-warunkowe","chapter":"Lista 3: Prawdopodobieństwo warunkowe","heading":"Lista 3: Prawdopodobieństwo warunkowe","text":"Zadania na ćwiczenia: 2025-03-10Lista zadań w formacie pdf","code":""},{"path":"lista-3-prawdopodobieństwo-warunkowe.html","id":"zadania-do-samodzielnego-rozwiązania-2","chapter":"Lista 3: Prawdopodobieństwo warunkowe","heading":"Zadania do samodzielnego rozwiązania","text":"W urnie znajduje się \\(20\\) kul białych \\(5\\) czarnych.\nLosujemy po jednej kuli aż momentu, gdy wylosujemy czarną kulę.\nJakie jest prawdopodobieństwo, że wykonamy \\(k\\) losowań,\njeżeli\nlosujemy bez zwracania\nlosujemy ze zwracaniem?\n\nOdpowiedź\n\n\n\nbez zwracania:\n\\[\n\\prod_{j=0}^{k-2} \\frac{20-j}{25-j} \\cdot \\frac{5}{25-k+1}\n\\]\nze zwracaniem: \\((4/5)^{k-1}/5\\)\n\n\nlosujemy bez zwracanialosujemy ze zwracaniem?\n\nOdpowiedź\n\n\n\nbez zwracania:\n\\[\n\\prod_{j=0}^{k-2} \\frac{20-j}{25-j} \\cdot \\frac{5}{25-k+1}\n\\]\nze zwracaniem: \\((4/5)^{k-1}/5\\)\n\n\nbez zwracania:\n\\[\n\\prod_{j=0}^{k-2} \\frac{20-j}{25-j} \\cdot \\frac{5}{25-k+1}\n\\]\nze zwracaniem: \\((4/5)^{k-1}/5\\)\nDwoje graczy, Maciek Dawid, dostało po \\(13\\) kart z \\(52\\).\nMaciek zobaczył przypadkowo u Dawida\nasa pik,\njakiegoś asa czarnego koloru,\njakiegoś asa.\nObliczyć prawdopodobieństwo, że Maciek nie ma asa.\n\nOdpowiedź\n\n\n\n,b,c \\({48 \\choose 13}/{51 \\choose 13}\\)\n\n\nasa pik,jakiegoś asa czarnego koloru,jakiegoś asa.\nObliczyć prawdopodobieństwo, że Maciek nie ma asa.\n\nOdpowiedź\n\n\n\n,b,c \\({48 \\choose 13}/{51 \\choose 13}\\)\n\n\n,b,c \\({48 \\choose 13}/{51 \\choose 13}\\)\nTrzech strzelców oddało niezależnie po jednym strzale tego samego celu.\nPrawdopodobieństwa trafień wynoszą odpowiednio \\(p_1\\), \\(p_2\\), \\(p_3\\).\nWyznacz prawdopodobieństwo, że pierszy strzelec trafił, jeżeli cel został trafiony\ndokładnie jednym pociskiem\ndokładnie dwoma pociskami;\ntrzema pociskami.\n\nOdpowiedź\n\n\n\n\\[\\begin{align*}\n& \\frac{p_1(1-p_2)(1-p_3)}{ p_1(1-p_2)(1-p_3) + p_2(1-p_1)(1-p_3)+p_3(1-p_1)(1-p_2)}\\\\\nb & \\frac{p_1(1-p_2)p_3+(1-p_1)p_2p_3}{p_1p_2(1-p_3)+p_1(1-p_2)p_3 +(1-p_1)p_2p_3} \\\\ c& 1\n\\end{align*}\\]\n\n\ndokładnie jednym pociskiemdokładnie dwoma pociskami;trzema pociskami.\n\nOdpowiedź\n\n\n\n\\[\\begin{align*}\n& \\frac{p_1(1-p_2)(1-p_3)}{ p_1(1-p_2)(1-p_3) + p_2(1-p_1)(1-p_3)+p_3(1-p_1)(1-p_2)}\\\\\nb & \\frac{p_1(1-p_2)p_3+(1-p_1)p_2p_3}{p_1p_2(1-p_3)+p_1(1-p_2)p_3 +(1-p_1)p_2p_3} \\\\ c& 1\n\\end{align*}\\]\n\n\n\\[\\begin{align*}\n& \\frac{p_1(1-p_2)(1-p_3)}{ p_1(1-p_2)(1-p_3) + p_2(1-p_1)(1-p_3)+p_3(1-p_1)(1-p_2)}\\\\\nb & \\frac{p_1(1-p_2)p_3+(1-p_1)p_2p_3}{p_1p_2(1-p_3)+p_1(1-p_2)p_3 +(1-p_1)p_2p_3} \\\\ c& 1\n\\end{align*}\\]\nPodaj przykłady zdarzeń takich, że\n\\(\\mathbb{P}[|B] < \\mathbb{P}[]\\),\n\\(\\mathbb{P}[|B] = \\mathbb{P}[]\\)\n\\(\\mathbb{P}[|B] > \\mathbb{P}[]\\).\n\nOdpowiedź\n\n\n\nRozważmy\n\\(\\mathbb{P}[\\cap B]=\\alpha\\), \\(\\mathbb{P}[B \\cap ^c]=\\beta\\),\n\\(\\mathbb{P}[\\setminus B]=\\gamma\\).\nSzukamy takiego doboru liczb \\(\\alpha, \\beta \\gamma\\)\ntakich, że \\(\\alpha+\\beta\\), \\(\\alpha+\\gamma \\leq 1\\) oraz\n\\(\\alpha /(\\alpha+\\beta)\\) było mniejsz/większe/równe \\(\\alpha+\\gamma\\).\n\n\n\\(\\mathbb{P}[|B] < \\mathbb{P}[]\\),\\(\\mathbb{P}[|B] = \\mathbb{P}[]\\)\\(\\mathbb{P}[|B] > \\mathbb{P}[]\\).\n\nOdpowiedź\n\n\n\nRozważmy\n\\(\\mathbb{P}[\\cap B]=\\alpha\\), \\(\\mathbb{P}[B \\cap ^c]=\\beta\\),\n\\(\\mathbb{P}[\\setminus B]=\\gamma\\).\nSzukamy takiego doboru liczb \\(\\alpha, \\beta \\gamma\\)\ntakich, że \\(\\alpha+\\beta\\), \\(\\alpha+\\gamma \\leq 1\\) oraz\n\\(\\alpha /(\\alpha+\\beta)\\) było mniejsz/większe/równe \\(\\alpha+\\gamma\\).\n\n\nRozważmy\n\\(\\mathbb{P}[\\cap B]=\\alpha\\), \\(\\mathbb{P}[B \\cap ^c]=\\beta\\),\n\\(\\mathbb{P}[\\setminus B]=\\gamma\\).\nSzukamy takiego doboru liczb \\(\\alpha, \\beta \\gamma\\)\ntakich, że \\(\\alpha+\\beta\\), \\(\\alpha+\\gamma \\leq 1\\) oraz\n\\(\\alpha /(\\alpha+\\beta)\\) było mniejsz/większe/równe \\(\\alpha+\\gamma\\).\n","code":""},{"path":"lista-3-prawdopodobieństwo-warunkowe.html","id":"zadania-na-ćwiczenia-2","chapter":"Lista 3: Prawdopodobieństwo warunkowe","heading":"Zadania na ćwiczenia","text":"W populacji jest \\(15\\%\\) dyslektyków.\nJeżeli w teście diagnostycznym uczeń popełni \\(6\\) lub więcej błędów, zostaje uznany za dyslektyka.\nKażdy dyslektyk na pewno popełni co najmniej \\(6\\) błędów.\nRównież nie-dyslektyk może popełnić co najmniej \\(6\\) błędów dzieje się z prawdopodobieństwem \\(0,1\\).\nJasiu popełnił \\(6\\) błędów.\nOblicz prawdopodobieństwo, że jest dyslektykiem.\nJakie jest prawdopodobieństwo, że w kolejnym teście też popełni co najmniej \\(6\\) błędów?W populacji jest \\(15\\%\\) dyslektyków.\nJeżeli w teście diagnostycznym uczeń popełni \\(6\\) lub więcej błędów, zostaje uznany za dyslektyka.\nKażdy dyslektyk na pewno popełni co najmniej \\(6\\) błędów.\nRównież nie-dyslektyk może popełnić co najmniej \\(6\\) błędów dzieje się z prawdopodobieństwem \\(0,1\\).\nJasiu popełnił \\(6\\) błędów.\nOblicz prawdopodobieństwo, że jest dyslektykiem.\nJakie jest prawdopodobieństwo, że w kolejnym teście też popełni co najmniej \\(6\\) błędów?Dawid Maciek grają w pokera. Maciek ma silną rękę zaczął od \\(5\\) dolarów.\nPrawdopodobieństwo, że Dawid ma silniejsze karty wynosi \\(0,1\\).\nGdyby Dawid miał mocniejsze/ słabsze karty podbiłby stawkę z prawdopodobieństwem \\(0,9 / 0,1\\).\nDawid podbił stawkę. Jakie jest prawdopodobieństwo, że ma lepsze karty?Dawid Maciek grają w pokera. Maciek ma silną rękę zaczął od \\(5\\) dolarów.\nPrawdopodobieństwo, że Dawid ma silniejsze karty wynosi \\(0,1\\).\nGdyby Dawid miał mocniejsze/ słabsze karty podbiłby stawkę z prawdopodobieństwem \\(0,9 / 0,1\\).\nDawid podbił stawkę. Jakie jest prawdopodobieństwo, że ma lepsze karty?W pewnej fabryce telewizorów każdy z aparatów może być wadliwy z prawdopodobieństwem \\(p\\).\nW fabryce są trzy stanowiska kontroli wyprodukowany telewizor trafia na każde ze stanowisk\nz jednakowym prawdopodobieństwem. \\(\\)-te stanowisko wykrywa wadliwy telewizor z\nprawdopodobieństwem \\(p_i\\) (\\(= 1, 2, 3\\)).\nTelewizory nie odrzucone w fabryce trafiają hurtowni tam poddawane są dodatkowej kontroli,\nktóra wykrywa wadliwy telewizor z prawdopodobieństwem \\(p_0\\).\nObliczyć prawdopodobieństwo tego, że dany nowowyprodukowany telewizor znajdzie się w sprzedaży (tzn. przejdzie przez obie kontrole).\nPrzypuśćmy, że telewizor jest już w sklepie. Jakie jest prawdopodobieństwo, że jest wadliwy?\nW pewnej fabryce telewizorów każdy z aparatów może być wadliwy z prawdopodobieństwem \\(p\\).\nW fabryce są trzy stanowiska kontroli wyprodukowany telewizor trafia na każde ze stanowisk\nz jednakowym prawdopodobieństwem. \\(\\)-te stanowisko wykrywa wadliwy telewizor z\nprawdopodobieństwem \\(p_i\\) (\\(= 1, 2, 3\\)).\nTelewizory nie odrzucone w fabryce trafiają hurtowni tam poddawane są dodatkowej kontroli,\nktóra wykrywa wadliwy telewizor z prawdopodobieństwem \\(p_0\\).Obliczyć prawdopodobieństwo tego, że dany nowowyprodukowany telewizor znajdzie się w sprzedaży (tzn. przejdzie przez obie kontrole).Przypuśćmy, że telewizor jest już w sklepie. Jakie jest prawdopodobieństwo, że jest wadliwy?Mamy dwie urny \\(50\\) kul. Połowa z kul jest biała, połowa czarna.\nJak rozłożyć kule urn, aby zmaksymalizować prawdopodobieństwo zdarzenia,\nże losowo wybrana kula z losowej urny jest biała (najpierw losujemy urnę,\npotem z wybranej urny losujemy kulę)?Mamy dwie urny \\(50\\) kul. Połowa z kul jest biała, połowa czarna.\nJak rozłożyć kule urn, aby zmaksymalizować prawdopodobieństwo zdarzenia,\nże losowo wybrana kula z losowej urny jest biała (najpierw losujemy urnę,\npotem z wybranej urny losujemy kulę)?Rzucamy trzema sześciennymi kostkami gry.\nNastępnie rzucamy ponownie tymi kostkami, na których nie wypadły “jedynki”.\nObliczyć prawdopodobieństwo, że na wszystkich trzech kostkach będą “jedynki”.Rzucamy trzema sześciennymi kostkami gry.\nNastępnie rzucamy ponownie tymi kostkami, na których nie wypadły “jedynki”.\nObliczyć prawdopodobieństwo, że na wszystkich trzech kostkach będą “jedynki”.Przypuśćmy, że \\(1/20\\) wszystkich kości gry jest sfałszowana zawsze wypada na nich szóstka. Wybieramy losowo trzy kostki rzucamy nimi. Oblicz\nprawdopodobieństwo wyrzucenia w sumie 18 oczek;\nprawdopodobieństwo, że co najmniej jedna kostka była sfałszowana, jeżeli wyrzuciliśmy 18 oczek;\nPrzypuśćmy, że \\(1/20\\) wszystkich kości gry jest sfałszowana zawsze wypada na nich szóstka. Wybieramy losowo trzy kostki rzucamy nimi. Obliczprawdopodobieństwo wyrzucenia w sumie 18 oczek;prawdopodobieństwo, że co najmniej jedna kostka była sfałszowana, jeżeli wyrzuciliśmy 18 oczek;Kierowcy dzielą się na ostrożnych (jest ich $ 95 % $ taki kierowca powoduje w ciągu roku wypadek z prawdopodobieństwem $ 0.01 $) piratów (jest ich $ 5 % $ taki kierowca powoduje w ciągu roku wypadek z prawdopodobieństwem $ 0.5 $). Wybrany losowo kierowca nie spowodował wypadku w pierwszym drugim roku. Obliczyć prawdopodobieństwo warunkowe, że spowoduje wypadek w trzecim roku.\\Kierowcy dzielą się na ostrożnych (jest ich $ 95 % $ taki kierowca powoduje w ciągu roku wypadek z prawdopodobieństwem $ 0.01 $) piratów (jest ich $ 5 % $ taki kierowca powoduje w ciągu roku wypadek z prawdopodobieństwem $ 0.5 $). Wybrany losowo kierowca nie spowodował wypadku w pierwszym drugim roku. Obliczyć prawdopodobieństwo warunkowe, że spowoduje wypadek w trzecim roku.\\W zabawę w ‘głuchy telefon’ gra \\(n\\) osób: \\(L_1, \\ldots, L_n\\). Pierwsza osoba \\(L_1\\) otrzymuje\ninformację w postaci ‘tak’ lub ‘nie’ przekazuje ją \\(L_2\\). Osoba \\(L_2\\) przekazują ją dalej, z prawdopodobieństwem \\(p\\)\ntaką samą, z prawdopodobieństwem \\(1-p\\) przeciwną, itd. Każdy uczestnik przekazuje kolejnemu informację,\nktórą uzyskał w prawdopodobieństwem \\(p\\) przeciwną z prawdopodobieństwem \\(1-p\\).\nOblicz prawdopodobieństwo \\(q_n\\), że osoba \\(L_n\\) otrzyma prawidłową informację.\nOblicz \\(\\lim_n q_n\\).W zabawę w ‘głuchy telefon’ gra \\(n\\) osób: \\(L_1, \\ldots, L_n\\). Pierwsza osoba \\(L_1\\) otrzymuje\ninformację w postaci ‘tak’ lub ‘nie’ przekazuje ją \\(L_2\\). Osoba \\(L_2\\) przekazują ją dalej, z prawdopodobieństwem \\(p\\)\ntaką samą, z prawdopodobieństwem \\(1-p\\) przeciwną, itd. Każdy uczestnik przekazuje kolejnemu informację,\nktórą uzyskał w prawdopodobieństwem \\(p\\) przeciwną z prawdopodobieństwem \\(1-p\\).\nOblicz prawdopodobieństwo \\(q_n\\), że osoba \\(L_n\\) otrzyma prawidłową informację.\nOblicz \\(\\lim_n q_n\\).","code":""},{"path":"lista-3-prawdopodobieństwo-warunkowe.html","id":"zadania-dodatkowe-2","chapter":"Lista 3: Prawdopodobieństwo warunkowe","heading":"Zadania dodatkowe","text":"Na rodzinie wszystkich podzbiorów \\(\\mathbb{N}\\) określamy miarę probabilistyczną \\(\\mathbb{P}_n\\)\nwzorem\n\\[\n\\mathbb{P}_n() = \\frac{|\\{m:\\; 1\\le m\\le n, m\\\\}|}{n}.\n\\] Mówimy, że zbiór \\(\\) ma gęstość\n\\[\nD() = \\lim_n \\mathbb{P}_n()\n\\]\njeżeli istnieje powyższa granica. Niech \\(\\mathcal D\\) oznacza rodzinę zbiorów posiadających gęstość.\nPokaż, że \\(D\\) jest skończenie addytywna na \\(\\mathcal D\\), ale nie jest przeliczalnie addytywna.\nCzy \\(\\mathcal D\\) jest \\(\\sigma\\)-ciałem?\nWykaż, że jeżeli \\(x\\[0,1]\\), istnieje zbiór \\(\\) taki, że \\(D() = x\\).\nPokaż, że \\(D\\) jest skończenie addytywna na \\(\\mathcal D\\), ale nie jest przeliczalnie addytywna.Czy \\(\\mathcal D\\) jest \\(\\sigma\\)-ciałem?Wykaż, że jeżeli \\(x\\[0,1]\\), istnieje zbiór \\(\\) taki, że \\(D() = x\\).Niech \\(\\Omega\\) będzie przestrzenią przeliczalnych ciągów 0-1, tj. \\(\\Omega = \\{0,1\\}^\\mathbb{N}\\).\nDla \\(\\omega\\\\Omega\\) oznaczmy przez \\(\\omega_n\\) wartość \\(n\\)-tej składowej.\nDla ustalonego ciągu \\(u=(u_1,\\ldots,u_n)\\\\{0,1\\}^n\\) niech\n\\[C_u =\\{\\omega:\\; \\omega_i = u_i; =1,\\ldots,n\\}. \\]\nZbiór \\(C_u\\) nazywamy cylindrem rzędu \\(n\\). Każdemu takiemu zbiorowi przypisujemy\nmiarę probabilistyczną~\\(\\mathbb{P}\\) równą \\(2^{-n}\\).\nOznaczmy przez \\(\\mathcal F_0\\) ciało składające się ze zbioru pustego oraz skończonych\nsum rozłącznych cylindrów. W naturalny sposób definiujemy \\(\\mathbb{P}\\) na \\(\\mathcal F_0\\).\nPokaż, że miara \\(\\mathbb P\\) jest przeliczalnie addytywna na \\(\\mathcal F_0\\).\nUtożsamiając \\(\\Omega\\) z przedziałem (0,1] porównaj miarę \\(\\mathbb P\\) z miarą Lebesgue’.\nPokaż, że miara \\(\\mathbb P\\) jest przeliczalnie addytywna na \\(\\mathcal F_0\\).Utożsamiając \\(\\Omega\\) z przedziałem (0,1] porównaj miarę \\(\\mathbb P\\) z miarą Lebesgue’.","code":""},{"path":"lista-4-niezależność-i-lemat-borela-cantellego.html","id":"lista-4-niezależność-i-lemat-borela-cantellego","chapter":"Lista 4: Niezależność i lemat Borela-Cantellego","heading":"Lista 4: Niezależność i lemat Borela-Cantellego","text":"Zadania na ćwiczenia: 2025-03-17Lista zadań w formacie PDF","code":""},{"path":"lista-4-niezależność-i-lemat-borela-cantellego.html","id":"zadania-do-samodzielnego-rozwiązania-3","chapter":"Lista 4: Niezależność i lemat Borela-Cantellego","heading":"Zadania do samodzielnego rozwiązania","text":"Maciek rzuca dwiema kośćmi. Pierwsza kość jest dobrze wyważona.\nNa drugiej szóstka wypada dwa razy częściej niż pozostałe liczby. Jakie jest prawdopodobieństwo,\nże suma oczek będzie większa niż 10?\n\nOdpowiedź\n\n\n\n\\(5/42\\)\n\n\n\\(5/42\\)\nLosujemy jednostajnie punkt \\((x,y)\\) z kwadratu \\([0,1]^2\\). Niech\n\\(= \\{ |x -y|\\leq 1/3\\}\\), \\(B = \\{x\\leq 1/2\\}\\) oraz \\(C = \\{ y\\geq 1/2\\}\\).\nCzy zdarzenia \\(\\), \\(B\\) \\(C\\) są niezależne? Czy są niezależne parami?\n\nOdpowiedź\n\n\n\nZdarzenia nie są niezależne. Są niezależne parami.\n\n\nZdarzenia nie są niezależne. Są niezależne parami.\nNiech \\(\\Omega = [-2,2]\\) oraz \\(A_n = [(-1)^n-1/n, (-1)^n+1/n]\\).\nZnajdź \\(\\limsup_n A_n\\)\nOdpowiedź\n\n\n\n\\(\\{-1,1\\}\\).\n\n\n\\(\\{-1,1\\}\\).\nZdarzenia \\(A_1, A_2, \\ldots , A_{2025}\\) są niezależne mają jednakowe prawdopodobieństwo \\(p\\).\nJaka jest szansa, że zajdzie dokładnie jedno?\n\nOdpowiedź\n\n\n\n\\(2025p(1-p)^{2024}\\)\n\n\n\\(2025p(1-p)^{2024}\\)\nZdarzenia \\(A_1, \\dots, A_n, \\dots\\) są niezależne mają równe prawdopodobieństwa.\nJaka jest szansa, że zajdzie skończenie wiele zdarzeń \\(A_n\\)?\n\nOdpowiedź\n\n\n\nNiech \\(\\mathbb{P}[A_k]=p\\). Jeżeli \\(p>0\\), szukana szansa wynosi zero.\n\n\nNiech \\(\\mathbb{P}[A_k]=p\\). Jeżeli \\(p>0\\), szukana szansa wynosi zero.\nRzucono \\(10\\) razy kostką. Jakie jest prawdopodobieństwo otrzymania\n\\(6\\) oczek co najmniej raz?\n\\(5\\) oczek dokładnie \\(3\\) razy?\n\\(6\\) oczek co najmniej raz?\\(5\\) oczek dokładnie \\(3\\) razy?\n0.8385, b. 0.155","code":""},{"path":"lista-4-niezależność-i-lemat-borela-cantellego.html","id":"zadania-na-ćwiczenia-3","chapter":"Lista 4: Niezależność i lemat Borela-Cantellego","heading":"Zadania na ćwiczenia","text":"Wykonujemy dwa eksperymenty: rzucamy kością, na której liczby parzyste wypadają dwa\nrazy częściej niż liczby nieparzyste (wszystkie parzyste są tak samo prawdopodobne wszystkie nieparzyste są tak samo prawdopodobne) losujemy liczbę z przedziału \\([0,1]\\).\nSkonstruuj przestrzeń probabilistyczną opisująca wykonanie tych eksperymentów\nniezależnie. Jakie jest prawdopodobieństwo, że suma otrzymanych liczb jest\nmniejsza niż \\(5/2\\)?Wykonujemy dwa eksperymenty: rzucamy kością, na której liczby parzyste wypadają dwa\nrazy częściej niż liczby nieparzyste (wszystkie parzyste są tak samo prawdopodobne wszystkie nieparzyste są tak samo prawdopodobne) losujemy liczbę z przedziału \\([0,1]\\).\nSkonstruuj przestrzeń probabilistyczną opisująca wykonanie tych eksperymentów\nniezależnie. Jakie jest prawdopodobieństwo, że suma otrzymanych liczb jest\nmniejsza niż \\(5/2\\)?Pokaż, że zdarzenia \\(A_1\\), …, \\(A_n\\) są niezależne wtedy tylko wtedy, gdy \\(\\sigma\\)-ciała\n\\(\\mathcal{F}_j=\\sigma(A_j) = \\{\\emptyset, \\Omega, A_j, A_j^c\\}\\) dla \\(j \\[n]\\) są niezależne.Pokaż, że zdarzenia \\(A_1\\), …, \\(A_n\\) są niezależne wtedy tylko wtedy, gdy \\(\\sigma\\)-ciała\n\\(\\mathcal{F}_j=\\sigma(A_j) = \\{\\emptyset, \\Omega, A_j, A_j^c\\}\\) dla \\(j \\[n]\\) są niezależne.Niech \\(\\Omega=[0,1]\\), \\(\\mathcal{F}=\\mathcal{B}(\\mathbb{R})\\) \\(\\mathbb{P}=\\lambda_1\\)\njednowymiarowa miara Lebesgue’. Rozważmy zdarzenia\n\\[\\begin{align*}\nA_1& =[0,1/2),\\\\\n     A_2 &= [0,1/4)\\cup [1/2,3/4), \\\\\nA_3 &= [0,1/8)\\cup[2/8, 3/8)\\cup[4/8,5/8)\\cup[6/8,7/8),\n     ....\n\\end{align*}\\]\nPokaż, że zdarzenia \\(\\{A_n\\}_{n \\\\mathbb{N}}\\) są niezależne.Niech \\(\\Omega=[0,1]\\), \\(\\mathcal{F}=\\mathcal{B}(\\mathbb{R})\\) \\(\\mathbb{P}=\\lambda_1\\)\njednowymiarowa miara Lebesgue’. Rozważmy zdarzenia\n\\[\\begin{align*}\nA_1& =[0,1/2),\\\\\n     A_2 &= [0,1/4)\\cup [1/2,3/4), \\\\\nA_3 &= [0,1/8)\\cup[2/8, 3/8)\\cup[4/8,5/8)\\cup[6/8,7/8),\n     ....\n\\end{align*}\\]\nPokaż, że zdarzenia \\(\\{A_n\\}_{n \\\\mathbb{N}}\\) są niezależne.Student musi poprawić oceny niedostateczne z dwóch przedmiotów. Szansa poprawienia oceny z\npierwszego przedmiotu w jednej próbie wynosi \\(p\\), z drugiego — \\(q\\).\nŻeby móc poprawić drugą ocenę, trzeba najpierw poprawić pierwszą. Poszczególne próby\npoprawiania są niezależne. Wiadomo, że po piętnastu próbach poprawiania oceny student\njeszcze nie poprawił oceny z drugiego przedmiotu.\nJaka jest szansa – pod tym warunkiem – że nie poprawił jeszcze oceny z pierwszego przedmiotu?Student musi poprawić oceny niedostateczne z dwóch przedmiotów. Szansa poprawienia oceny z\npierwszego przedmiotu w jednej próbie wynosi \\(p\\), z drugiego — \\(q\\).\nŻeby móc poprawić drugą ocenę, trzeba najpierw poprawić pierwszą. Poszczególne próby\npoprawiania są niezależne. Wiadomo, że po piętnastu próbach poprawiania oceny student\njeszcze nie poprawił oceny z drugiego przedmiotu.\nJaka jest szansa – pod tym warunkiem – że nie poprawił jeszcze oceny z pierwszego przedmiotu?Rzucamy \\(2n\\) razy symetryczną monetą. Niech \\(O_{2n}\\) (odpowiednio \\(R_{2n}\\))\noznacza liczbę orłów (odpowiednio reszek). Dla ustalonego \\(k\\) obliczyć\\[\n\\lim_{n \\\\infty} \\mathbb{P}(|O_{2n} - R_{2n}| \\leq 2k).\n\\]Rzucamy \\(2n\\) razy symetryczną monetą. Niech \\(O_{2n}\\) (odpowiednio \\(R_{2n}\\))\noznacza liczbę orłów (odpowiednio reszek). Dla ustalonego \\(k\\) obliczyć\\[\n\\lim_{n \\\\infty} \\mathbb{P}(|O_{2n} - R_{2n}| \\leq 2k).\n\\]Niech \\(A_n\\) będą zdarzeniami niezależnymi, przy czym \\(\\mathbb{P}(A_n) = p_n \\(0,1)\\).\nWykaż, że zachodzi co najmniej jedno ze zdarzeń \\(A_n\\) wtedy tylko wtedy,\ngdy z prawdopodobieństwem 1 zachodzi nieskończenie wiele zdarzeń \\(A_n\\).Niech \\(A_n\\) będą zdarzeniami niezależnymi, przy czym \\(\\mathbb{P}(A_n) = p_n \\(0,1)\\).\nWykaż, że zachodzi co najmniej jedno ze zdarzeń \\(A_n\\) wtedy tylko wtedy,\ngdy z prawdopodobieństwem 1 zachodzi nieskończenie wiele zdarzeń \\(A_n\\).Rzucamy nieskończenie wiele razy monetą, w której orzeł wypada z prawdopodobieństwem \\(p \\geq 1/2\\).\nNiech \\(A_n\\) oznacza zdarzenie, że pomiędzy rzutem \\(2^n\\) \\(2^{n+1}\\) otrzymano ciąg \\(n\\)\nkolejnych orłów. Pokaż, że zdarzenia \\(A_n\\) z prawdopodobieństwem \\(1\\) zachodzą\nnieskończenie wiele razy.Rzucamy nieskończenie wiele razy monetą, w której orzeł wypada z prawdopodobieństwem \\(p \\geq 1/2\\).\nNiech \\(A_n\\) oznacza zdarzenie, że pomiędzy rzutem \\(2^n\\) \\(2^{n+1}\\) otrzymano ciąg \\(n\\)\nkolejnych orłów. Pokaż, że zdarzenia \\(A_n\\) z prawdopodobieństwem \\(1\\) zachodzą\nnieskończenie wiele razy.Niech \\(\\{A_n\\}_{n \\\\mathbb{N}}\\) będzie ciągiem zdarzeń.\nPokaż, że jeśli \\(\\mathbb{P}(A_n) \\0\\) oraz\n\\[\n\\sum_{n=1}^{\\infty} P(A_n^c \\cap A_{n+1}) < \\infty,\n\\]\n\n\\[\n\\mathbb{P}(\\limsup_n A_n) = 0.\n\\]\nZnajdź przykład ciągu zdarzeń \\(A_n\\), którego można zastosować wynik z punktu ,\nale nie można zastosować lematu Borela-Cantellego.\nNiech \\(\\{A_n\\}_{n \\\\mathbb{N}}\\) będzie ciągiem zdarzeń.Pokaż, że jeśli \\(\\mathbb{P}(A_n) \\0\\) oraz\n\\[\n\\sum_{n=1}^{\\infty} P(A_n^c \\cap A_{n+1}) < \\infty,\n\\]\n\n\\[\n\\mathbb{P}(\\limsup_n A_n) = 0.\n\\]Znajdź przykład ciągu zdarzeń \\(A_n\\), którego można zastosować wynik z punktu ,\nale nie można zastosować lematu Borela-Cantellego.","code":""},{"path":"lista-4-niezależność-i-lemat-borela-cantellego.html","id":"zadania-dodatkowe-3","chapter":"Lista 4: Niezależność i lemat Borela-Cantellego","heading":"Zadania dodatkowe","text":"Rzucamy nieskończenie wiele razy symetryczną monetą.\nNiech \\(A_n\\) oznacza zdarzenie, że w pierwszych \\(n\\) rzutach było tyle samo orłów reszek.\nWykazać, że z prawdopodobieństwem 1 zachodzi nieskończenie wiele zdarzeń \\(A_n\\).","code":""},{"path":"lista-5-zmienne-losowe.html","id":"lista-5-zmienne-losowe","chapter":"Lista 5: Zmienne losowe","heading":"Lista 5: Zmienne losowe","text":"Zadania na ćwiczenia: 2025-03-24Lista zadań w formacie PDF","code":""},{"path":"lista-5-zmienne-losowe.html","id":"zadania-do-samodzielnego-rozwiązania-4","chapter":"Lista 5: Zmienne losowe","heading":"Zadania do samodzielnego rozwiązania","text":"Losujemy bez zwracania dwie liczby ze zbioru {1,2,3,4,5}.\nNiech \\(X\\) będzie sumą wylosowanych liczb - wyznacz jego rozkład oraz dystrybuantę.\n\nOdpowiedź\n\n\n\n\\[\\begin{align*}\n\\mu_X =&  0.1\\delta_3+0.1\\delta_4+0.2\\delta_5+0.2\\delta_6\\\\ &+0.2\\delta_7+0.1\\delta_8+0.1\\delta_9\\\\\nF_X(t) =&  0.1\\mathbf{1}_{[3,4)}(t) +0.2\\mathbf{1}_{[4,5)}(t)+0.4\\mathbf{1}_{[5,6)}(t) +\n    0.6\\mathbf{1}_{[6,7)}(t)\\\\ &+0.8\\mathbf{1}_{[7,8)}(t) +0.9 \\mathbf{1}_{[8,9)}(t) + \\mathbf{1}_{[9, +\\infty)}(t)\n\\end{align*}\\]\n\n\n\\[\\begin{align*}\n\\mu_X =&  0.1\\delta_3+0.1\\delta_4+0.2\\delta_5+0.2\\delta_6\\\\ &+0.2\\delta_7+0.1\\delta_8+0.1\\delta_9\\\\\nF_X(t) =&  0.1\\mathbf{1}_{[3,4)}(t) +0.2\\mathbf{1}_{[4,5)}(t)+0.4\\mathbf{1}_{[5,6)}(t) +\n    0.6\\mathbf{1}_{[6,7)}(t)\\\\ &+0.8\\mathbf{1}_{[7,8)}(t) +0.9 \\mathbf{1}_{[8,9)}(t) + \\mathbf{1}_{[9, +\\infty)}(t)\n\\end{align*}\\]\nLosujemy jednostajnie punkt z okręgu o promieniu jeden. Niech \\(X\\) będzie odległością\nwylosowanego punktu od środka okręgu. Uzasadnij, że \\(X\\) jest zmienną losową. Znajdź jej dystrybuantę.\n\nOdpowiedź\n\n\n\nPrzyjmujemy \\(\\Omega = \\{ (x,y) \\: : \\: x^2+y^2 <1\\}\\), \\(\\mathcal{F} = \\mathcal{B}(\\Omega)\\),\n\\(\\mathbb{P} = \\pi^{-1}\\lambda_1\\).\n\\[\\begin{align*}\n   X^{-1}[(-\\infty,t]] &= \\Omega\\cap \\{(x,y) \\: : \\: x^2+y^2<t^2\\}\\\\mathcal{F}\\\\\n   F_X(t) &= \\mathbf{1}_{[0,1]}(t)t^2 + \\mathbf{1}_{[ 1,\\infty)}(t).\n\\end{align*}\\]\n\n\nPrzyjmujemy \\(\\Omega = \\{ (x,y) \\: : \\: x^2+y^2 <1\\}\\), \\(\\mathcal{F} = \\mathcal{B}(\\Omega)\\),\n\\(\\mathbb{P} = \\pi^{-1}\\lambda_1\\).\n\\[\\begin{align*}\n   X^{-1}[(-\\infty,t]] &= \\Omega\\cap \\{(x,y) \\: : \\: x^2+y^2<t^2\\}\\\\mathcal{F}\\\\\n   F_X(t) &= \\mathbf{1}_{[0,1]}(t)t^2 + \\mathbf{1}_{[ 1,\\infty)}(t).\n\\end{align*}\\]\nNiech \\(F_1, F_2\\) będą dystrybuantami (spełniają założenia Twierdzenia 6.3).\nSprawdzić, czy następujące funkcje są jednowymiarowymi dystrybuantami (spełniają założenia Twierdzenia 6.3):\n\\(F_1 F_2\\);\n\\(F_1 + F_2\\);\n\\(F_1 - F_2\\);\n\\(F_1 / F_2\\);\n\\(\\max(F_1, F_2)\\);\n\\(\\min(F_1, F_2)\\).\nOdpowiedź\n\n\n\n,e,f.\n\n\n\\(F_1 F_2\\);\\(F_1 + F_2\\);\\(F_1 - F_2\\);\\(F_1 / F_2\\);\\(\\max(F_1, F_2)\\);\\(\\min(F_1, F_2)\\).\nOdpowiedź\n\n\n\n,e,f.\n\n\n,e,f.\nPodaj przykład przestrzeni probabilistycznej \\((\\Omega, {\\mathcal F}, {\\mathbb P})\\) funkcji\n\\(X: \\Omega\\ \\mathbb{R}\\), która nie jest zmienną losową.\n\nOdpowiedź\n\n\n\nNiech \\(\\Omega=\\{1,2\\}\\), \\(\\mathcal{F} = \\{\\emptyset, \\Omega\\}\\), \\(X(\\omega)=\\omega\\).\n\n\nNiech \\(\\Omega=\\{1,2\\}\\), \\(\\mathcal{F} = \\{\\emptyset, \\Omega\\}\\), \\(X(\\omega)=\\omega\\).\nDystrybuanta zmiennej losowej \\(X\\) dana jest wzorem\n\\[\\begin{equation*}\nF_X(x) \\; = \\; (0.1 + x)\\mathbf{1}_{[0,0.5)}(x) + (0.4 + x)\\mathbf{1}_{[0.5,0.55)}(x)\n+ \\mathbf{1}_{[0.55,\\infty)}(x) \\; .\n\\end{equation*}\\]\nNiech \\(\\mu_X\\) będzie rozkładem \\(X\\). Wyznacz\n\\(\\mu_X(\\{ 1/2 \\})\\),\n\\(\\mu_X([0,1/2])\\),\n\\(\\mu_X((0,0.55))\\).\n\nOdpowiedź\n\n\n\n0.3, b 0.9, c. 0.85\n\n\\(\\mu_X(\\{ 1/2 \\})\\),\\(\\mu_X([0,1/2])\\),\\(\\mu_X((0,0.55))\\).\n0.3, b 0.9, c. 0.85\nZmienna losowa \\(X\\) ma rozkład zadany przez\n\\[\\begin{equation*}\n\\mu_X() = \\int_A c (1-x)^2 \\mathbf{1}_{[0,1]}(x) \\mathrm{d}x.\n\\end{equation*}\\]\nZnajdź wartość parametru \\(c\\).\n\nOdpowiedź\n\n\n\n\\(3\\).\n\n\n\\(3\\).\nLosujemy jednostajnie punkt z przedziału \\((0,1)\\). Niech \\(U\\) będzie wylosowanym punktem. Wyznacz dystrybuantę\nzmiennej losowej \\(X = -\\log(U)\\).\n\nOdpowiedź\n\n\n\n\\(F_X(t)=1-e^{-t}\\).\n\n\n\\(F_X(t)=1-e^{-t}\\).\n","code":""},{"path":"lista-5-zmienne-losowe.html","id":"zadania-na-ćwiczenia-4","chapter":"Lista 5: Zmienne losowe","heading":"Zadania na ćwiczenia","text":"Dana jest przestrzeń probabilistyczna \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) oraz funkcja\n\\(X: \\Omega \\\\mathbb{R}\\). Uzasadnij, że jeżeli \\(X^{-1}(,b)\\\\mathcal{F}\\) dla dowolnych\n\\(,b\\\\mathbb{R}\\), \\(X\\) jest zmienną losową.Dana jest przestrzeń probabilistyczna \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) oraz funkcja\n\\(X: \\Omega \\\\mathbb{R}\\). Uzasadnij, że jeżeli \\(X^{-1}(,b)\\\\mathcal{F}\\) dla dowolnych\n\\(,b\\\\mathbb{R}\\), \\(X\\) jest zmienną losową.Dystrybuanta zmiennej losowej \\(X\\) dana jest wzorem:\n\\[\nF_X(t) = \\left\\{\n\\begin{array}{cc}\n   0 &  \\mbox{ dla } t<0\\\\\n   t^2 \\ \\  &  \\mbox{ dla } 0\\leq t < 1/2\\\\\n   1/4 & \\mbox{ dla } 1/2 \\leq t < 4 \\\\\n   1 & \\mbox{ dla }t\\geq 4.\n\\end{array}\n\\right.\n\\]\nOblicz \\(\\mathbb{P}[X=5]\\), \\(\\mathbb{P}[X=4]\\), \\(\\mathbb{P}[1/3 < X \\leq 5]\\), \\(\\mathbb{P}[0<X<1]\\).Dystrybuanta zmiennej losowej \\(X\\) dana jest wzorem:\n\\[\nF_X(t) = \\left\\{\n\\begin{array}{cc}\n   0 &  \\mbox{ dla } t<0\\\\\n   t^2 \\ \\  &  \\mbox{ dla } 0\\leq t < 1/2\\\\\n   1/4 & \\mbox{ dla } 1/2 \\leq t < 4 \\\\\n   1 & \\mbox{ dla }t\\geq 4.\n\\end{array}\n\\right.\n\\]\nOblicz \\(\\mathbb{P}[X=5]\\), \\(\\mathbb{P}[X=4]\\), \\(\\mathbb{P}[1/3 < X \\leq 5]\\), \\(\\mathbb{P}[0<X<1]\\).Niech \\(X_1,\\ldots, X_n\\) będą zmiennymi losowymi określonymi na przestrzeni probabilistycznej\n\\((\\Omega,\\mathcal{F},\\mathbb{P})\\) niech \\(B_1,B_2,\\ldots, B_n \\\\mathcal{F}\\) będzie rozbiciem \\(\\Omega\\)\n(tzn. zbiory te są rozłączne ich sumą jest \\(\\Omega\\)).\nNiech \\(Z(\\omega) = X_i(\\omega)\\) dla \\(\\omega\\B_i\\). Uzasadnij, że \\(Z\\) jest zmienną losową.Niech \\(X_1,\\ldots, X_n\\) będą zmiennymi losowymi określonymi na przestrzeni probabilistycznej\n\\((\\Omega,\\mathcal{F},\\mathbb{P})\\) niech \\(B_1,B_2,\\ldots, B_n \\\\mathcal{F}\\) będzie rozbiciem \\(\\Omega\\)\n(tzn. zbiory te są rozłączne ich sumą jest \\(\\Omega\\)).\nNiech \\(Z(\\omega) = X_i(\\omega)\\) dla \\(\\omega\\B_i\\). Uzasadnij, że \\(Z\\) jest zmienną losową.Punkt \\(x\\) nazywamy atomem rozkładu \\(\\mu\\) na \\(\\mathbb{R}\\), gdy \\(\\mu(\\{x\\}) > 0\\).\nPokaż, że rozkład prawdopodobieństwa \\(\\mu\\) może mieć co najwyżej przeliczalną liczbę atomów.\nCzy zbiór atomów może mieć punkt skupienia?\nCzy zbiór atomów może być gęsty w \\(\\mathbb{R}\\)?\nPunkt \\(x\\) nazywamy atomem rozkładu \\(\\mu\\) na \\(\\mathbb{R}\\), gdy \\(\\mu(\\{x\\}) > 0\\).Pokaż, że rozkład prawdopodobieństwa \\(\\mu\\) może mieć co najwyżej przeliczalną liczbę atomów.Czy zbiór atomów może mieć punkt skupienia?Czy zbiór atomów może być gęsty w \\(\\mathbb{R}\\)?Dane są dwie miary probabilistyczne \\(\\mu\\) \\(\\nu\\) na \\((\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))\\) takie, że dla dowolnej liczby \\(t>0\\) mamy \\(\\nu([-t,t]) = \\mu([-t,t])\\). Uzasadnić, że \\(\\mu() = \\nu()\\) dla dowolnego symetrycznego zbioru \\(\\\\mathcal{B}(\\mathbb{R})\\) (zbiór \\(\\) nazywamy symetrycznym, jeżeli \\(= -\\)).Dane są dwie miary probabilistyczne \\(\\mu\\) \\(\\nu\\) na \\((\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))\\) takie, że dla dowolnej liczby \\(t>0\\) mamy \\(\\nu([-t,t]) = \\mu([-t,t])\\). Uzasadnić, że \\(\\mu() = \\nu()\\) dla dowolnego symetrycznego zbioru \\(\\\\mathcal{B}(\\mathbb{R})\\) (zbiór \\(\\) nazywamy symetrycznym, jeżeli \\(= -\\)).Zmienna losowa \\(X\\) ma rozkład zadany przez\n\\[\n\\mu_X() = \\int_A \\frac{2}{\\pi}\\frac{1}{1+x^2} \\mathbf{1}_{[0, +\\infty)}(x) \\mathrm{d}x.\n\\]\nUdowodnij, że\n\\[\\begin{equation*}\nY(\\omega) = \\left\\{ \\begin{array}{cc} 1/X(\\omega), & X(\\omega)>0 \\\\  17, & X(\\omega)\\leq 0\\end{array} \\right.   \n\\end{equation*}\\]\nma ten sam rozkład, co \\(X\\).Zmienna losowa \\(X\\) ma rozkład zadany przez\n\\[\n\\mu_X() = \\int_A \\frac{2}{\\pi}\\frac{1}{1+x^2} \\mathbf{1}_{[0, +\\infty)}(x) \\mathrm{d}x.\n\\]\nUdowodnij, że\n\\[\\begin{equation*}\nY(\\omega) = \\left\\{ \\begin{array}{cc} 1/X(\\omega), & X(\\omega)>0 \\\\  17, & X(\\omega)\\leq 0\\end{array} \\right.   \n\\end{equation*}\\]\nma ten sam rozkład, co \\(X\\).(rozkład geometryczny) Wykonujemy doświadczenia Bernoulliego (z prawdopodobieństwem pojedynczego sukcesu \\(p\\))\naż chwili otrzymania pierwszego sukcesu. Niech \\(X\\) oznacza liczbę wykonanych doświadczeń,\n\\(Y\\) – czas oczekiwania na pierwszy sukces, czyli liczbę porażek przed pierwszym sukcesem. Wyznaczyć rozkłady zmiennych\nlosowych \\(X\\) \\(Y\\), tj. wyznaczyć funkcje \\(\\mathbb{P}[X=k]\\) oraz \\(\\mathbb{P}[Y=k]\\).(rozkład geometryczny) Wykonujemy doświadczenia Bernoulliego (z prawdopodobieństwem pojedynczego sukcesu \\(p\\))\naż chwili otrzymania pierwszego sukcesu. Niech \\(X\\) oznacza liczbę wykonanych doświadczeń,\n\\(Y\\) – czas oczekiwania na pierwszy sukces, czyli liczbę porażek przed pierwszym sukcesem. Wyznaczyć rozkłady zmiennych\nlosowych \\(X\\) \\(Y\\), tj. wyznaczyć funkcje \\(\\mathbb{P}[X=k]\\) oraz \\(\\mathbb{P}[Y=k]\\).(rozkład wykładniczy) Przypuśćmy, że doświadczenie opisane w poprzednim zadaniu\nwykonuje się \\(n\\) razy na sekundę,\nzaś prawdopodobieństwo sukcesu wynosi \\(\\lambda/n\\), \\(\\lambda > 0\\),\nczas oczekiwania na pierwszy sukces, \\(Y_n\\), mierzy się w sekundach.\nWyznaczyć dystrybuantę zmiennej losowej \\(Y_n\\) zbadać jej zachowanie gdy \\(n \\\\infty\\).(rozkład wykładniczy) Przypuśćmy, że doświadczenie opisane w poprzednim zadaniu\nwykonuje się \\(n\\) razy na sekundę,\nzaś prawdopodobieństwo sukcesu wynosi \\(\\lambda/n\\), \\(\\lambda > 0\\),\nczas oczekiwania na pierwszy sukces, \\(Y_n\\), mierzy się w sekundach.\nWyznaczyć dystrybuantę zmiennej losowej \\(Y_n\\) zbadać jej zachowanie gdy \\(n \\\\infty\\).","code":""},{"path":"lista-5-zmienne-losowe.html","id":"zadania-dodatkowe-4","chapter":"Lista 5: Zmienne losowe","heading":"Zadania dodatkowe","text":"Mówimy, że zmienna losowa \\(X\\) jest niezdegenerowana, gdy \\(\\mathbb{P}[X = ] < 1\\) dla każdego \\(\\\\mathbb{R}\\).\nWyznacz wszystkie liczby rzeczywiste \\(b, c\\) dla których istnieje niezdegenerowana zmienna losowa \\(X\\)\ntaka, że \\(X\\) ma taki sam rozkład jak \\(bX+c\\). Scharakteryzuj rozkład \\(X\\) w terminach \\(b\\) \\(c\\).","code":""},{"path":"lista-6-wartość-oczekiwana.html","id":"lista-6-wartość-oczekiwana","chapter":"Lista 6: Wartość oczekiwana","heading":"Lista 6: Wartość oczekiwana","text":"Zadania na ćwiczenia: 2025-03-31Lista zadań w formacie PDF","code":""},{"path":"lista-6-wartość-oczekiwana.html","id":"zadania-do-samodzielnego-rozwiązania-5","chapter":"Lista 6: Wartość oczekiwana","heading":"Zadania do samodzielnego rozwiązania","text":"W urnie jest \\(b \\geq 1\\) kul białych \\(c \\geq 1\\) czarnych.\nObliczyć \\(\\mathbb{E}[X]\\), jeśli \\(X\\) jest liczbą wylosowanych kul białych podczas\nlosowania bez zwracania \\(n\\) kul (\\(n \\leq b + c\\));\n\\(bn/(b+c)\\)\nZmienna losowa ma rozkład o gęstości \\(g(x) = 5x^4 \\mathbb{}_{[0,1]}(x)\\).\nObliczyć\n\\(\\mathbb{E}[X]\\)\n\\(\\mathbb{E}[1/(1+X^5)]\\).\nZmienna losowa ma rozkład o gęstości \\(g(x) = 5x^4 \\mathbb{}_{[0,1]}(x)\\).\nObliczyć\\(\\mathbb{E}[X]\\)\\(\\mathbb{E}[1/(1+X^5)]\\).\n5/6 b. \\(\\log(2)\\)Oblicz \\(\\mathbb{E}[X]\\) jeżeli \\(X\\) jest zmienną o rozkładzie:\nPoiss(\\(\\lambda\\)),\nExp(\\(\\lambda\\)),\nGeom(\\(p\\)).\n\\(\\mathcal{N}(\\mu, \\sigma^2)\\).\nOblicz \\(\\mathbb{E}[X]\\) jeżeli \\(X\\) jest zmienną o rozkładzie:Poiss(\\(\\lambda\\)),Exp(\\(\\lambda\\)),Geom(\\(p\\)).\\(\\mathcal{N}(\\mu, \\sigma^2)\\).\n\\(\\lambda\\) b. \\(1/\\lambda\\), c. \\(1/p\\), d. \\(\\mu\\)Zmienna losowa \\(X\\) ma rozkład jednostajny \\(U[0,1]\\).\nObliczyć \\(\\mathbb{E}[Y]\\) jeżeli\n\\(Y = e^X\\),\n\\(Y = \\cos^2(\\pi X)\\).\nZmienna losowa \\(X\\) ma rozkład jednostajny \\(U[0,1]\\).\nObliczyć \\(\\mathbb{E}[Y]\\) jeżeli\\(Y = e^X\\),\\(Y = \\cos^2(\\pi X)\\).\n\\(e-1\\) b. \\(1/2\\)Niech \\(F\\) będzie dystrybuantą zmiennej losowej \\(X\\), \\(f\\) jej gęstością.\nWyznaczyć dystrybuanty gęstości zmiennych losowych:\n\\(X + b\\) dla \\(> 0\\);\n\\(|X|\\);\n\\(X^2\\);\n\\(e^X\\);\nNiech \\(F\\) będzie dystrybuantą zmiennej losowej \\(X\\), \\(f\\) jej gęstością.\nWyznaczyć dystrybuanty gęstości zmiennych losowych:\\(X + b\\) dla \\(> 0\\);\\(|X|\\);\\(X^2\\);\\(e^X\\);\n\\[\\begin{align*}\n    . & F((t-b)/) & f((x-b)/)/\\\\\n    b. & F(t)-0F(-t) & f(x)-f(-x) \\\\\n    c. & F(\\sqrt{t}) - F(-\\sqrt{t}) & (f(\\sqrt{x}) +f(\\sqrt{x}))/(2\\sqrt{x}) \\\\\n    d. & F(\\log(t)) & f(\\log(x))/x 1_{y>0}\n\\end{align*}\\]\n","code":""},{"path":"lista-6-wartość-oczekiwana.html","id":"zadania-na-ćwiczenia-5","chapter":"Lista 6: Wartość oczekiwana","heading":"Zadania na ćwiczenia","text":"Monika wybrała się kasyna w Las Vegas mając przy sobie 255$.\nJako cel postawiła sobie wygranie \\(1\\) dolara w ruletkę wyjście z kasyna z kwotą 256$.\nPodczas tej wizyty obstawiała kolory. Wszystkie pola poza 0 00 są czerwone lub czarne\n(po 18 pól). Poprawne wskazanie koloru (z prawdopodobieństwem 18/38) podwaja zaryzykowaną kwotę.\nMonika zastosowała następującą strategię: postanowiła, że będzie grać kolejno\no 1$, 2$, 4$, 8$, 16$, 32$, 64$, 128$. Jeżeli w jednej z gier wygra,\nzabiera nagrodę opuszcza kasyno z 256 dolarami.\nObliczyć prawdopodobieństwo, że jej się powiodło. Obliczyć wartość oczekiwaną wygranej.Monika wybrała się kasyna w Las Vegas mając przy sobie 255$.\nJako cel postawiła sobie wygranie \\(1\\) dolara w ruletkę wyjście z kasyna z kwotą 256$.\nPodczas tej wizyty obstawiała kolory. Wszystkie pola poza 0 00 są czerwone lub czarne\n(po 18 pól). Poprawne wskazanie koloru (z prawdopodobieństwem 18/38) podwaja zaryzykowaną kwotę.\nMonika zastosowała następującą strategię: postanowiła, że będzie grać kolejno\no 1$, 2$, 4$, 8$, 16$, 32$, 64$, 128$. Jeżeli w jednej z gier wygra,\nzabiera nagrodę opuszcza kasyno z 256 dolarami.\nObliczyć prawdopodobieństwo, że jej się powiodło. Obliczyć wartość oczekiwaną wygranej.Losujemy cięciwę w okręgu o promieniu jeden poprzez wylosowanie jej środka.\nNiech \\(X_2\\) będzie długością wylosowanej cięciwy. Wyznacz rozkład \\(X_2\\).\nCzy rozkład ten jest absolutnie ciągły? Wyznacz gęstość. Znajdź \\(\\mathbb{E}[X_2]\\).Losujemy cięciwę w okręgu o promieniu jeden poprzez wylosowanie jej środka.\nNiech \\(X_2\\) będzie długością wylosowanej cięciwy. Wyznacz rozkład \\(X_2\\).\nCzy rozkład ten jest absolutnie ciągły? Wyznacz gęstość. Znajdź \\(\\mathbb{E}[X_2]\\).Każdy bok każda przekątna \\(2n\\)-kąta foremnego malujemy losowo na jeden z trzech kolorów.\nWybór każdego koloru jest jednakowo prawdopodobny, kolorowania różnych odcinków są niezależne.\nNiech \\(X\\) oznacza liczbę jednobarwnych trójkątów prostokątnych o wierzchołkach będących\nwierzchołkami \\(2n\\)-kąta. Obliczyć \\(\\mathbb{E}[X]\\).Każdy bok każda przekątna \\(2n\\)-kąta foremnego malujemy losowo na jeden z trzech kolorów.\nWybór każdego koloru jest jednakowo prawdopodobny, kolorowania różnych odcinków są niezależne.\nNiech \\(X\\) oznacza liczbę jednobarwnych trójkątów prostokątnych o wierzchołkach będących\nwierzchołkami \\(2n\\)-kąta. Obliczyć \\(\\mathbb{E}[X]\\).W urnie znajduje się 50 białych kul. Losujemy ze zwracaniem po jednej kuli,\nprzy czym wyciągniętą kulę malujemy na czerwono, jeśli jest biała.\nNiech \\(X\\) będzie liczbą czerwonych kul w urnie po 20 losowaniach.\nObliczyć \\(\\mathbb{E}[X]\\).W urnie znajduje się 50 białych kul. Losujemy ze zwracaniem po jednej kuli,\nprzy czym wyciągniętą kulę malujemy na czerwono, jeśli jest biała.\nNiech \\(X\\) będzie liczbą czerwonych kul w urnie po 20 losowaniach.\nObliczyć \\(\\mathbb{E}[X]\\).Na płaszczyźnie zaznaczono \\(n\\) punktów w taki sposób, że żadne trzy nie są współliniowe.\nKażda para punktów została połączona odcinkiem z prawdopodobieństwem \\(p\\).\nNiech \\(X\\) oznacza liczbę powstałych trójkątów.\nOblicz \\(\\mathbb{E}X\\).Na płaszczyźnie zaznaczono \\(n\\) punktów w taki sposób, że żadne trzy nie są współliniowe.\nKażda para punktów została połączona odcinkiem z prawdopodobieństwem \\(p\\).\nNiech \\(X\\) oznacza liczbę powstałych trójkątów.\nOblicz \\(\\mathbb{E}X\\).Pokaż, że jeżeli zmienna losowa \\(X\\) ma rozkład dyskretny skoncentrowany na liczbach całkowitych nieujemnych, \n\\[\n     \\mathbb{E}[X] = \\sum_{k=1}^{\\infty} \\mathbb{P}(X \\geq k).\n\\]Pokaż, że jeżeli zmienna losowa \\(X\\) ma rozkład dyskretny skoncentrowany na liczbach całkowitych nieujemnych, \n\\[\n     \\mathbb{E}[X] = \\sum_{k=1}^{\\infty} \\mathbb{P}(X \\geq k).\n\\]Niech \\(X\\) będzie zmienną losową o dystrybuancie \\(F\\). Wykaż, że\nJeżeli \\(X \\geq 0\\), \\(\\mathbb{E}[X]  =  \\int_0^\\infty \\mathbb{P}(X>t)\\: d t\n=  \\int_0^\\infty \\mathbb{P}(X \\geq t)\\:d t\\) przy czym istnienie\njednej strony implikuje istnienie drugiej ich równość.\nJeżeli \\(X\\) jest dowolną zmienną losową o skończonej wartości oczekiwanej,\n\\(\\mathbb{E}[X] =\n\\int_0^\\infty \\left( 1 - F(t) \\right)\\: d t - \\int_{-\\infty}^0 F(t)\\: d t\\).\nJeżeli \\(X \\geq 0\\), \\(\\varphi\\) jest rosnąca różniczkowalna,\n\\(\\varphi(0) = 0\\), \n\\(\\mathbb{E}[\\varphi(X)]  =  \\int_0^\\infty \\varphi '(t) \\mathbb{P}(X>t)\\: d t\\).\nW szczególności, jeżeli \\(r > 0\\), \\(\\mathbb{E}[X^r] =\n   \\int_0^\\infty r t^{r-1} \\mathbb{P}(X>t)\\: d t\\).\nNiech \\(X\\) będzie zmienną losową o dystrybuancie \\(F\\). Wykaż, żeJeżeli \\(X \\geq 0\\), \\(\\mathbb{E}[X]  =  \\int_0^\\infty \\mathbb{P}(X>t)\\: d t\n=  \\int_0^\\infty \\mathbb{P}(X \\geq t)\\:d t\\) przy czym istnienie\njednej strony implikuje istnienie drugiej ich równość.Jeżeli \\(X \\geq 0\\), \\(\\mathbb{E}[X]  =  \\int_0^\\infty \\mathbb{P}(X>t)\\: d t\n=  \\int_0^\\infty \\mathbb{P}(X \\geq t)\\:d t\\) przy czym istnienie\njednej strony implikuje istnienie drugiej ich równość.Jeżeli \\(X\\) jest dowolną zmienną losową o skończonej wartości oczekiwanej,\n\\(\\mathbb{E}[X] =\n\\int_0^\\infty \\left( 1 - F(t) \\right)\\: d t - \\int_{-\\infty}^0 F(t)\\: d t\\).Jeżeli \\(X\\) jest dowolną zmienną losową o skończonej wartości oczekiwanej,\n\\(\\mathbb{E}[X] =\n\\int_0^\\infty \\left( 1 - F(t) \\right)\\: d t - \\int_{-\\infty}^0 F(t)\\: d t\\).Jeżeli \\(X \\geq 0\\), \\(\\varphi\\) jest rosnąca różniczkowalna,\n\\(\\varphi(0) = 0\\), \n\\(\\mathbb{E}[\\varphi(X)]  =  \\int_0^\\infty \\varphi '(t) \\mathbb{P}(X>t)\\: d t\\).\nW szczególności, jeżeli \\(r > 0\\), \\(\\mathbb{E}[X^r] =\n   \\int_0^\\infty r t^{r-1} \\mathbb{P}(X>t)\\: d t\\).Jeżeli \\(X \\geq 0\\), \\(\\varphi\\) jest rosnąca różniczkowalna,\n\\(\\varphi(0) = 0\\), \n\\(\\mathbb{E}[\\varphi(X)]  =  \\int_0^\\infty \\varphi '(t) \\mathbb{P}(X>t)\\: d t\\).\nW szczególności, jeżeli \\(r > 0\\), \\(\\mathbb{E}[X^r] =\n   \\int_0^\\infty r t^{r-1} \\mathbb{P}(X>t)\\: d t\\).Udowodnić, że jeżeli \\(X \\geq 0\\), \\(\\sum_{n=1}^\\infty \\mathbb{P}(X \\geq n) \\; \\leq \\; \\\n\\mathbb{E}[X] \\; \\leq 1 + \\sum_{n=1}^\\infty \\mathbb{P}(X \\geq n)\\).Udowodnić, że jeżeli \\(X \\geq 0\\), \\(\\sum_{n=1}^\\infty \\mathbb{P}(X \\geq n) \\; \\leq \\; \\\n\\mathbb{E}[X] \\; \\leq 1 + \\sum_{n=1}^\\infty \\mathbb{P}(X \\geq n)\\).","code":""},{"path":"lista-7-powtórka-przed-kolokwium.html","id":"lista-7-powtórka-przed-kolokwium","chapter":"Lista 7: Powtórka przed kolokwium","heading":"Lista 7: Powtórka przed kolokwium","text":"Lista zadań w formacie PDF","code":""},{"path":"lista-7-powtórka-przed-kolokwium.html","id":"zadania-do-samodzielnego-rozwiązania-6","chapter":"Lista 7: Powtórka przed kolokwium","heading":"Zadania do samodzielnego rozwiązania","text":"Wybieramy losowo liczbę naturalną z przedziału \\([1, 1000]\\).\nObliczyć prawdopodobieństwo, że wybrana liczba jest podzielna przez co najmniej\njedną z liczb: \\(4\\), \\(6\\), \\(9\\).Wybieramy losowo liczbę naturalną z przedziału \\([1, 1000]\\).\nObliczyć prawdopodobieństwo, że wybrana liczba jest podzielna przez co najmniej\njedną z liczb: \\(4\\), \\(6\\), \\(9\\).Co jest bardziej prawdopodobne: otrzymanie co najmniej jednej jedynki przy\nrzucie \\(4\\) kostek, czy co najmniej raz dwóch jedynek na obu kostkach przy \\(24\\)\nrzutach obu kostek?Co jest bardziej prawdopodobne: otrzymanie co najmniej jednej jedynki przy\nrzucie \\(4\\) kostek, czy co najmniej raz dwóch jedynek na obu kostkach przy \\(24\\)\nrzutach obu kostek?Znaleźć prawdopodobieństwo, że przy wielokrotnym rzucaniu parą kostek sześciennych suma\noczek \\(8\\) wypadnie przed sumą oczek \\(7\\).Znaleźć prawdopodobieństwo, że przy wielokrotnym rzucaniu parą kostek sześciennych suma\noczek \\(8\\) wypadnie przed sumą oczek \\(7\\).Każda z \\(n\\) pałek została złamana na dwie części -\ndługą krótką. \\(2n\\) części połączono w \\(n\\) par, z których\nutworzono nowe pałki. Znaleźć prawdopodobieństwo, że\nczęści zostaną połączone w takich samych kombinacjach jak przed złamaniem;\nwszystkie długie części zostaną połączone z krótkimi.\nKażda z \\(n\\) pałek została złamana na dwie części -\ndługą krótką. \\(2n\\) części połączono w \\(n\\) par, z których\nutworzono nowe pałki. Znaleźć prawdopodobieństwo, żeczęści zostaną połączone w takich samych kombinacjach jak przed złamaniem;wszystkie długie części zostaną połączone z krótkimi.Niech \\(n \\\\mathbb{N}\\). Losujemy jednostajnie podzbiór\n\\([n]=\\{1,2, \\ldots , n\\}\\). Skonstruuj odpowiednią przestrzeń probabilistyczną.\nDla \\(k \\[n]\\) niech \\(A_k\\) będzie zdarzeniem, że wylosowany zbiór zawiera \\(k\\).\nPokaż, że zdarzenia \\(\\{A_k\\}_{k \\[n]}\\) są niezależne.Niech \\(n \\\\mathbb{N}\\). Losujemy jednostajnie podzbiór\n\\([n]=\\{1,2, \\ldots , n\\}\\). Skonstruuj odpowiednią przestrzeń probabilistyczną.\nDla \\(k \\[n]\\) niech \\(A_k\\) będzie zdarzeniem, że wylosowany zbiór zawiera \\(k\\).\nPokaż, że zdarzenia \\(\\{A_k\\}_{k \\[n]}\\) są niezależne.Niech \\(n \\\\mathbb{N}\\). Losujemy jednostajnie podzbiór \\(\\{1,2, \\ldots , n\\}\\).\nNiech \\(X\\) będzie liczebnością\nwylosowanego zbioru. Znajdź rozkład \\(X\\).Niech \\(n \\\\mathbb{N}\\). Losujemy jednostajnie podzbiór \\(\\{1,2, \\ldots , n\\}\\).\nNiech \\(X\\) będzie liczebnością\nwylosowanego zbioru. Znajdź rozkład \\(X\\).Zdarzenia \\(\\) \\(B\\) są niezależne oraz \\(\\cup B = \\Omega\\).\nWykazać, że \\(\\mathbb{P}() = 1\\) lub \\(\\mathbb{P}(B) = 1\\).Zdarzenia \\(\\) \\(B\\) są niezależne oraz \\(\\cup B = \\Omega\\).\nWykazać, że \\(\\mathbb{P}() = 1\\) lub \\(\\mathbb{P}(B) = 1\\).Niech \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) będzie przestrzenią probabilistyczną.\nDla zdarzeń \\(\\{A_k\\}_{k \\[n]}\\) \\(\\{\\epsilon_k\\}_{k\\[n]} \\\\{0,1\\}^n\\)\nniech\n\\[\\begin{equation*}\nA_k^{\\epsilon_k} = \\left\\{ \\begin{array}{cc} A_k, & \\epsilon_k=1 \\\\ A_k^c, & \\epsilon_k=0 \\end{array} \\right..\n\\end{equation*}\\]\nPokaż, że \\(\\{A_k\\}_{k \\[n]}\\) są niezależne wtedy tylko wtedy, gdy dla każdego\n\\(\\{\\epsilon_k\\}_{k \\[n]} \\\\{0,1\\}^n\\),\\[\\begin{equation*}\n\\mathbb{P}\\left[A_1^{\\epsilon_1}\\cap A_2^{\\epsilon_2}\\cap \\ldots \\cap A_{n}^{\\epsilon_n}\\right] =\n     \\mathbb{P}\\left[A_1^{\\epsilon_1}\\right]\n\\mathbb{P}\\left[A_2^{\\epsilon_2}\\right] \\cdots\n\\mathbb{P}\\left[A_{n}^{\\epsilon_n}\\right].\n\\end{equation*}\\]Niech \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) będzie przestrzenią probabilistyczną.\nDla zdarzeń \\(\\{A_k\\}_{k \\[n]}\\) \\(\\{\\epsilon_k\\}_{k\\[n]} \\\\{0,1\\}^n\\)\nniech\n\\[\\begin{equation*}\nA_k^{\\epsilon_k} = \\left\\{ \\begin{array}{cc} A_k, & \\epsilon_k=1 \\\\ A_k^c, & \\epsilon_k=0 \\end{array} \\right..\n\\end{equation*}\\]\nPokaż, że \\(\\{A_k\\}_{k \\[n]}\\) są niezależne wtedy tylko wtedy, gdy dla każdego\n\\(\\{\\epsilon_k\\}_{k \\[n]} \\\\{0,1\\}^n\\),\\[\\begin{equation*}\n\\mathbb{P}\\left[A_1^{\\epsilon_1}\\cap A_2^{\\epsilon_2}\\cap \\ldots \\cap A_{n}^{\\epsilon_n}\\right] =\n     \\mathbb{P}\\left[A_1^{\\epsilon_1}\\right]\n\\mathbb{P}\\left[A_2^{\\epsilon_2}\\right] \\cdots\n\\mathbb{P}\\left[A_{n}^{\\epsilon_n}\\right].\n\\end{equation*}\\]Niech \\(\\Omega\\) będzie zbiorem wszystkich grafów prostych na wierzchołkach\n\\(\\{1, 2, \\ldots , n\\}\\).\nRozważmy \\(\\mathcal{F} = 2^\\Omega\\) oraz \\(\\mathbb{P}[] = ||/|\\Omega|\\).\nDla każdej pary \\(<j\\) niech \\(A_{,j}\\) będzie zdarzeniem, że wylosowany graf\nzawiera krawędź \\(\\{,j\\}\\). Pokaż, że \\((A_{,j})_{<j}\\) są niezależne.Niech \\(\\Omega\\) będzie zbiorem wszystkich grafów prostych na wierzchołkach\n\\(\\{1, 2, \\ldots , n\\}\\).\nRozważmy \\(\\mathcal{F} = 2^\\Omega\\) oraz \\(\\mathbb{P}[] = ||/|\\Omega|\\).\nDla każdej pary \\(<j\\) niech \\(A_{,j}\\) będzie zdarzeniem, że wylosowany graf\nzawiera krawędź \\(\\{,j\\}\\). Pokaż, że \\((A_{,j})_{<j}\\) są niezależne.Rozważmy przestrzeń probabilistyczną z poprzedniego zadania. Niech \\(X\\) będzie liczbą trójkątów\nw wylosowanym grafie, tj. liczbą trójek \\(<j<k\\) takich, że wylosowany graf zawiera krawędzie\n\\(\\{,j\\}\\), \\(\\{j,k\\}\\) oraz \\(\\{k,\\}\\). Znajdź \\(\\mathbb{E}[X]\\).Rozważmy przestrzeń probabilistyczną z poprzedniego zadania. Niech \\(X\\) będzie liczbą trójkątów\nw wylosowanym grafie, tj. liczbą trójek \\(<j<k\\) takich, że wylosowany graf zawiera krawędzie\n\\(\\{,j\\}\\), \\(\\{j,k\\}\\) oraz \\(\\{k,\\}\\). Znajdź \\(\\mathbb{E}[X]\\).Niech\n\\[\\begin{equation*}\n\\Omega = \\{ \\omega=(\\omega_j)_{j \\[n]} \\\\mathbb{Z}^n \\: : \\: |\\omega_1| = |\\omega_{j}-\\omega_{j-1}|=1\\}\n\\end{equation*}\\]\nNiech \\(\\mathcal{F}=2^\\Omega\\) \\(\\mathbb{P}[] = ||/|\\Omega|\\).\nNiech \\(A_1 = \\{\\omega_1=1\\}\\).\nDla \\(k \\[n]\\), \\(k \\geq 2\\) połóżmy \\(A_k = \\{\\omega_k-\\omega_{k-1}=1\\}\\).\nPokaż, że \\(\\{A_k\\}_{k \\[n]}\\) są niezależne.Niech\n\\[\\begin{equation*}\n\\Omega = \\{ \\omega=(\\omega_j)_{j \\[n]} \\\\mathbb{Z}^n \\: : \\: |\\omega_1| = |\\omega_{j}-\\omega_{j-1}|=1\\}\n\\end{equation*}\\]\nNiech \\(\\mathcal{F}=2^\\Omega\\) \\(\\mathbb{P}[] = ||/|\\Omega|\\).\nNiech \\(A_1 = \\{\\omega_1=1\\}\\).\nDla \\(k \\[n]\\), \\(k \\geq 2\\) połóżmy \\(A_k = \\{\\omega_k-\\omega_{k-1}=1\\}\\).\nPokaż, że \\(\\{A_k\\}_{k \\[n]}\\) są niezależne.Rozważmy przestrzeń probabilistyczną z poprzedniego zadania. Rozważmy \\(X(\\omega) =\\omega_n\\). Znajdź rozkład \\(X\\).Rozważmy przestrzeń probabilistyczną z poprzedniego zadania. Rozważmy \\(X(\\omega) =\\omega_n\\). Znajdź rozkład \\(X\\).Na odcinku \\([0, 1]\\) umieszczono losowo punkty \\(A_1\\) , \\(A_2\\) \\(A_3\\) .\nObliczyć prawdopodobieństwo, że \\(A_1 \\leq A_2 \\leq A_3\\) .Na odcinku \\([0, 1]\\) umieszczono losowo punkty \\(A_1\\) , \\(A_2\\) \\(A_3\\) .\nObliczyć prawdopodobieństwo, że \\(A_1 \\leq A_2 \\leq A_3\\) .Niech \\(Q\\) będzie wielomianem rzeczywistym stopnia \\(2n\\) o losowych współczynnikach.\nKażdy współczynnik jest losowany niezależnie może wynieść \\(1\\) z prawdopodobienstawem \\(p\\)\noraz \\(-1\\) z prawdopodobieństwem \\(1-p\\). Znaleźć:\n\\(\\mathbb{P}[Q(2) > 0];\\)\n\\(\\mathbb{P}[Q(1) > 0]\\);\n\\(\\mathbb{P}[Q(2) > 0|Q(1) > 0]\\).\nNiech \\(Q\\) będzie wielomianem rzeczywistym stopnia \\(2n\\) o losowych współczynnikach.\nKażdy współczynnik jest losowany niezależnie może wynieść \\(1\\) z prawdopodobienstawem \\(p\\)\noraz \\(-1\\) z prawdopodobieństwem \\(1-p\\). Znaleźć:\\(\\mathbb{P}[Q(2) > 0];\\)\\(\\mathbb{P}[Q(1) > 0]\\);\\(\\mathbb{P}[Q(2) > 0|Q(1) > 0]\\).Przypomnijmy, że\n\\[\\begin{equation*}\n\\frac{\\pi^2}{6} = \\sum_{n=1}^\\infty \\frac{1}{n^2}.\n\\end{equation*}\\]\nRozważmy \\(\\Omega = \\mathbb{N}\\), \\(\\mathcal{F}=2^\\Omega\\) oraz \\(\\mathbb{P}[\\{n\\}]=6n^{-2}\\pi^{-2}\\).\nNiech \\(\\mathcal{P}\\) oznacza zbiór liczb pierwszych.\nPokaż, że \\(\\{p\\mathbb{N}\\}_{p \\\\mathcal{P}}\\) są niezależne. Wywnioskuj, że\n\\[\\begin{equation*}\n\\prod_{p \\\\mathcal{P}}\\left(1 -\\frac{1}{p^2} \\right) = \\frac{6}{\\pi^2}.\n\\end{equation*}\\]Przypomnijmy, że\n\\[\\begin{equation*}\n\\frac{\\pi^2}{6} = \\sum_{n=1}^\\infty \\frac{1}{n^2}.\n\\end{equation*}\\]\nRozważmy \\(\\Omega = \\mathbb{N}\\), \\(\\mathcal{F}=2^\\Omega\\) oraz \\(\\mathbb{P}[\\{n\\}]=6n^{-2}\\pi^{-2}\\).\nNiech \\(\\mathcal{P}\\) oznacza zbiór liczb pierwszych.\nPokaż, że \\(\\{p\\mathbb{N}\\}_{p \\\\mathcal{P}}\\) są niezależne. Wywnioskuj, że\n\\[\\begin{equation*}\n\\prod_{p \\\\mathcal{P}}\\left(1 -\\frac{1}{p^2} \\right) = \\frac{6}{\\pi^2}.\n\\end{equation*}\\]Rodzina \\(\\mathcal{R}_1\\) składa się z jednego zdarzenia \\(= \\{1,2\\}\\) zaś rodzina\n\\(\\mathcal{R}_2\\) z dwóch zdarzeń \\(B = \\{1,3\\}\\) \\(C = \\{2,3\\}\\).\nWiemy, że \\(\\Omega = \\{1,2,3,4\\}\\) że wszystkie zdarzenia elementarne są jednakowo prawdopodobne.\nCzy \\(\\sigma\\)-algebry \\(\\sigma(\\mathcal{R}_1)\\) \\(\\sigma(\\mathcal{R}_2)\\) są niezależne?Rodzina \\(\\mathcal{R}_1\\) składa się z jednego zdarzenia \\(= \\{1,2\\}\\) zaś rodzina\n\\(\\mathcal{R}_2\\) z dwóch zdarzeń \\(B = \\{1,3\\}\\) \\(C = \\{2,3\\}\\).\nWiemy, że \\(\\Omega = \\{1,2,3,4\\}\\) że wszystkie zdarzenia elementarne są jednakowo prawdopodobne.\nCzy \\(\\sigma\\)-algebry \\(\\sigma(\\mathcal{R}_1)\\) \\(\\sigma(\\mathcal{R}_2)\\) są niezależne?Czy funkcja\\[\nG(x) =\n\\begin{cases}\n0, & \\text{dla } x \\leq 0, \\\\\n1, & \\text{dla } x \\geq 1, \\\\\nx^2, & \\text{dla } 0 \\leq x < \\frac{1}{2}, \\\\\nx, & \\text{dla } \\frac{1}{2} \\leq x \\leq 1\n\\end{cases}\n\\]\njest dystrybuantą? Naszkicuj wykres funkcji \\(G\\) wyznacz jej uogólnioną funkcję odwrotną.\nZnajdź przestrzeń probabilistyczną \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) oraz zmienną losową \\(Y\\),\ndla której funkcja \\(G\\) jest dystrybuantą. Znajdź \\(\\mathbb{E}[e^{Y}]\\).Czy funkcja\\[\nG(x) =\n\\begin{cases}\n0, & \\text{dla } x \\leq 0, \\\\\n1, & \\text{dla } x \\geq 1, \\\\\nx^2, & \\text{dla } 0 \\leq x < \\frac{1}{2}, \\\\\nx, & \\text{dla } \\frac{1}{2} \\leq x \\leq 1\n\\end{cases}\n\\]\njest dystrybuantą? Naszkicuj wykres funkcji \\(G\\) wyznacz jej uogólnioną funkcję odwrotną.\nZnajdź przestrzeń probabilistyczną \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) oraz zmienną losową \\(Y\\),\ndla której funkcja \\(G\\) jest dystrybuantą. Znajdź \\(\\mathbb{E}[e^{Y}]\\).Niech \\(X\\) będzie zmienną losową o gęstości \\(\\alpha x^{\\alpha-1}\\mathbf{1}_{[0,1]}(x)\\). Niech\n\\(\\varphi(s) = \\max\\{ \\frac 14, s-s^2\\}\\). Oblicz \\(\\mathbb{E} \\varphi(X)\\).Niech \\(X\\) będzie zmienną losową o gęstości \\(\\alpha x^{\\alpha-1}\\mathbf{1}_{[0,1]}(x)\\). Niech\n\\(\\varphi(s) = \\max\\{ \\frac 14, s-s^2\\}\\). Oblicz \\(\\mathbb{E} \\varphi(X)\\).Niech \\(F\\) będzie dystrybuantą na \\(\\mathbb{R}\\). Pokaż, że dla dowolnego \\(\\geq 0\\),\n\\[\n             \\int_{\\mathbb{R}} F(x+) - F(x) \\mathrm{d}x =.\n         \\]Niech \\(F\\) będzie dystrybuantą na \\(\\mathbb{R}\\). Pokaż, że dla dowolnego \\(\\geq 0\\),\n\\[\n             \\int_{\\mathbb{R}} F(x+) - F(x) \\mathrm{d}x =.\n         \\]Z urny, w której jest \\(6\\) kul czarnych \\(4\\) białe losujemy kolejno bez zwracania po\njednej kuli tak długo, aż wylosujemy kulę czarną. Obliczyć wartość oczekiwaną\nliczby wylosowanych kul białych.Z urny, w której jest \\(6\\) kul czarnych \\(4\\) białe losujemy kolejno bez zwracania po\njednej kuli tak długo, aż wylosujemy kulę czarną. Obliczyć wartość oczekiwaną\nliczby wylosowanych kul białych.Rzucamy sześcienną kostką aż momentu, gdy wypadną z rzędu dwie „szóstki”.\nZnaleźć wartość oczekiwaną liczby rzutów.Rzucamy sześcienną kostką aż momentu, gdy wypadną z rzędu dwie „szóstki”.\nZnaleźć wartość oczekiwaną liczby rzutów.Niech \\(A_1, A_2, \\ldots, A_n\\) będą niezależnymi zdarzeniami o jednakowym\nprawdopodobieństwie \\(p_n\\). Przy pomocy nierówności Boole’oraz\nnierówności Bonferroniego oszacować (z góry z dołu)\n\\[ \\mathbf{P}_n \\; = \\; \\frac{\\mathbb{P}[\\bigcup_{=1}^n A_i]}{np_n} \\; . \\]\nJak sprawdza się szacowanie dla \\(p_n = 1/n\\)? Wyznaczyć \\(\\lim_{n \\\\infty} \\mathbf{P}_n\\)\nw przypadku, gdy \\(\\lim_{n \\\\infty} n p_n = 0\\).Niech \\(A_1, A_2, \\ldots, A_n\\) będą niezależnymi zdarzeniami o jednakowym\nprawdopodobieństwie \\(p_n\\). Przy pomocy nierówności Boole’oraz\nnierówności Bonferroniego oszacować (z góry z dołu)\n\\[ \\mathbf{P}_n \\; = \\; \\frac{\\mathbb{P}[\\bigcup_{=1}^n A_i]}{np_n} \\; . \\]\nJak sprawdza się szacowanie dla \\(p_n = 1/n\\)? Wyznaczyć \\(\\lim_{n \\\\infty} \\mathbf{P}_n\\)\nw przypadku, gdy \\(\\lim_{n \\\\infty} n p_n = 0\\).","code":""},{"path":"lista-8-wektory-losowe.html","id":"lista-8-wektory-losowe","chapter":"Lista 8: wektory losowe","heading":"Lista 8: wektory losowe","text":"Zadania na ćwiczenia: 2025-04-14Lista zadań w formacie PDF","code":""},{"path":"lista-8-wektory-losowe.html","id":"zadania-do-samodzielnego-rozwiązania-7","chapter":"Lista 8: wektory losowe","heading":"Zadania do samodzielnego rozwiązania","text":"Niech \\(\\vec{X}=(X_1, X_2)\\) będzie dwuwymiarowym wektorem losowym o standardowym rozkładzie normalnym.\nZnajdź rozkłady brzegowe.\n\nOdpowiedź\n\n\n\nOba rozkłady brzegowe standardowe rozkłady normalne.\n\nNiech \\(\\vec{X}=(X_1, X_2)\\) będzie dwuwymiarowym wektorem losowym o standardowym rozkładzie normalnym.\nZnajdź rozkłady brzegowe.\n\nOdpowiedź\n\nOba rozkłady brzegowe standardowe rozkłady normalne.\nNiech \\(\\vec{X}=(X,Y)\\) będzie dwuwymiarowym wektorem losowym o gęstości\n\\[\nf(x, y) =\n\\begin{cases}\n4x^2y + 2y^5 & 0 \\leq x, y \\leq 1, \\\\\n0 & \\mbox{w przeciwnym razie}\n\\end{cases}\n\\]\nSprawdź, że \\(f\\) jest rzeczywiście gęstością.\nOblicz \\(\\mathbb{P}[1/2 \\leq X \\leq 3/4, 1/4 \\leq Y \\leq 1/2]\\).\nZnajdź rozkłady brzegowe \\(X\\) \\(Y\\). Czy są one absolutnie ciągłe?\nJeżeli tak, oblicz ich gęstości.\nNiech \\(\\vec{X}=(X,Y)\\) będzie dwuwymiarowym wektorem losowym o gęstości\n\\[\nf(x, y) =\n\\begin{cases}\n4x^2y + 2y^5 & 0 \\leq x, y \\leq 1, \\\\\n0 & \\mbox{w przeciwnym razie}\n\\end{cases}\n\\]Sprawdź, że \\(f\\) jest rzeczywiście gęstością.Oblicz \\(\\mathbb{P}[1/2 \\leq X \\leq 3/4, 1/4 \\leq Y \\leq 1/2]\\).Znajdź rozkłady brzegowe \\(X\\) \\(Y\\). Czy są one absolutnie ciągłe?\nJeżeli tak, oblicz ich gęstości.\n\\(\\int_0^1 \\int_0^1 f(x, y) = 1\\)\\(\\frac{629}{16384}\\)\\[F_X(t) =\n\\begin{cases}\n0 & t < 0 \\\\\n\\frac{2 t^3 + t}{3} & 0 \\le t \\le 1\\\\\n1 & t > 1\n\\end{cases}\\]\n\\(f_X(t) = (2 t^2 + 1/3) \\mathbf{1}_{[0,1]}(t)\\)\n\\[F_Y (t) =\n\\begin{cases}\n0 & t<0 \\\\\n\\frac{2t^2 + t^6}{3} & t \\[0,1] \\\\\n1 & t > 1\n\\end{cases}\\]\n\\(f_Y(t) = (4/3 t + 2 t^5) \\mathbf{1}_{[0,1]} (t)\\)Niech \\((X,Y)\\) będzie dwuwymiarowym wektorem losowym o gęstości\n\\[\nf(x, y) =\n\\begin{cases}\nCye^{-xy} & 0 \\leq x, y \\leq 1, \\\\\n0 & \\text{w przeciwnym razie}\n\\end{cases}\n\\]\nOblicz wartość stałej \\(C\\).\nOblicz \\(\\mathbb{P}[1/2 \\leq X \\leq 3/4, 1/4 \\leq Y \\leq 1/2]\\).\nZnajdź rozkłady brzegowe \\(X\\) \\(Y\\). Czy są one absolutnie ciągłe? Jeżeli tak, oblicz ich gęstości.\nNiech \\((X,Y)\\) będzie dwuwymiarowym wektorem losowym o gęstości\n\\[\nf(x, y) =\n\\begin{cases}\nCye^{-xy} & 0 \\leq x, y \\leq 1, \\\\\n0 & \\text{w przeciwnym razie}\n\\end{cases}\n\\]Oblicz wartość stałej \\(C\\).Oblicz \\(\\mathbb{P}[1/2 \\leq X \\leq 3/4, 1/4 \\leq Y \\leq 1/2]\\).Znajdź rozkłady brzegowe \\(X\\) \\(Y\\). Czy są one absolutnie ciągłe? Jeżeli tak, oblicz ich gęstości.\n\\(C = e\\)\\(e (2(e^{-3/8} - e^{-1/4}) + 4 (e^{-3/16} - e^{-1/8}))\\)\\[F_X (t) =\n\\begin{cases}\n0 & t<0 \\\\\ne(1+ e^{-t}/t - 1/t) & t \\[0,1] \\\\\n1 & t > 1\n\\end{cases}\\]\n\\(f_X(t) = e(e^{-t}/t - e^{-t}/t^2 + 1/t^2)\\)\n\\[F_Y (t) =\n\\begin{cases}\n0 & t<0 \\\\\ne((e^{-t} - 1) + t)& t \\[0,1] \\\\\n1 & t > 1\n\\end{cases}\\]\n\\(f_Y(t) = e(-e^{-t} + 1)\\)Udowodnij, że wektor losowy \\(\\vec{X}=(X_1, X_2)\\) ma rozkład dyskretny wtedy tylko wtedy, gdy\nzmienne losowe \\(X_1\\) \\(X_2\\) mają rozkłady dyskretne.\n\nOdpowiedź\n\n\n\nZałóżmy, że \\(\\vec{X}\\) ma rozkład dyskretny. Wówczas\nistnieje przeliczalny zbiór \\(S\\subseteq \\mathbb{R}^2\\) taki, że\n\\[\\begin{equation*}\n\\mathbb{P}\\left[\\vec{X}\\S\\right]=1.\n\\end{equation*}\\]\nPokażemy jedynie, że \\(X_1\\) ma rozkład dyskretny. Dyskretność rozkładu \\(X_2\\) będzie\nwynikała z analogicznego argumentu. Rozważmy \\(S_1\\) będący rzutem \\(S\\) na pierwszą oś, tj.\n\\[\\begin{equation*}\nS_1 = \\{ x \\\\mathbb{R} \\: : \\: \\exists y \\\\mathbb{R}, \\: (x,y) \\S\\}\n\\subseteq \\mathbb{R}.\n\\end{equation*}\\]\nWówczas \\(|S_1|\\leq |S|\\). W szczególności \\(S_1\\) jest zbiorem przeliczalnym. Mamy\n\\[\\begin{equation*}\n\\left\\{ \\vec{X} \\S \\right\\} \\subseteq \\{X_1 \\S_1\\}.\n\\end{equation*}\\]\nRzeczywiście, niech \\(\\omega \\\\{\\vec{X}\\S\\}\\). Wówczas \\(\\vec{X}(\\omega) \\S\\).\nMamy \\(\\vec{X}(\\omega) = (X_1(\\omega), X_2(\\omega))\\). Innymi słowy dla \\(y =X_2(\\omega)\\) mamy\n\\((X_1(\\omega), y) \\S\\). Oznacza , że \\(X_1(\\omega) \\S_1\\), czyli \\(\\omega \\\\{ X_1\\S\\}\\).\ndowodzi postulowanej inkluzji. Z monotoniczności prawdopodobieństwa\n\\[\\begin{equation*}\n1=\\mathbb{P}\\left[\\vec{X}\\S \\right] \\leq \\mathbb{P}[X_1 \\S_1].\n\\end{equation*}\\]\nCo kończy dowód jednej implikacji.\nZałóżmy teraz, że rozkłady \\(X_1\\) \\(X_2\\) są dyskretne. Istnieją zatem przeliczalne\n\\(S_1\\), \\(S_2 \\subseteq \\mathbb{R}\\) takie, że\n\\[\\begin{equation*}\n\\mathbb{P}\\left[X_1\\S_1\\right]=\\mathbb{P}[X_2\\S_2]=1.\n\\end{equation*}\\]\nRozważmy zbiór \\(S = S_1 \\times S_2 \\subseteq \\mathbb{R}^2\\). Wówczas \\(S\\) jest zbiorem przeliczalnym.\nMamy\n\\[\\begin{equation*}\n\\left\\{ \\vec{X} \\S \\right\\} = \\{(X_1, X_2) \\S_1\\times S_2\\}\n= \\bigcup_{x \\S_1} \\{X_1 = x, \\: X_2\\S_2 \\}.\n\\end{equation*}\\]\nSkoro powyższa suma jest przeliczalna, zbiory rozłączne\n\\[\\begin{equation*}\n\\mathbb{P}\\left[\\vec{X} \\S \\right] = \\sum_{x \\S_1} \\mathbb{P}[X_1 =x, \\: X_2 \\S_2].\n\\end{equation*}\\]\nZauważmy, że \\(\\mathbb{P}[X_1=x, \\: X_2 \\S_2] = \\mathbb{P}[X_1=x]\\), ponieważ\n\\[\\begin{multline*}\n0 \\leq \\mathbb{P}[X_1 =x]-\\mathbb{P}[X_1=x,\\: X_2 \\S_2] \\\\=\n\\mathbb{P}[X_1=x, \\: X_2 \\notin S_2] \\leq \\mathbb{P}[X_2 \\notin S_2]=0.\n\\end{multline*}\\]\nMamy zatem\n\\[\\begin{equation*}\n\\mathbb{P}\\left[\\vec{X} \\S \\right] = \\sum_{x \\S_1} \\mathbb{P}[X_1 =x] = \\mathbb{P}[X_1\\S_1] =1.\n\\end{equation*}\\]\n\n\nZałóżmy, że \\(\\vec{X}\\) ma rozkład dyskretny. Wówczas\nistnieje przeliczalny zbiór \\(S\\subseteq \\mathbb{R}^2\\) taki, że\n\\[\\begin{equation*}\n\\mathbb{P}\\left[\\vec{X}\\S\\right]=1.\n\\end{equation*}\\]\nPokażemy jedynie, że \\(X_1\\) ma rozkład dyskretny. Dyskretność rozkładu \\(X_2\\) będzie\nwynikała z analogicznego argumentu. Rozważmy \\(S_1\\) będący rzutem \\(S\\) na pierwszą oś, tj.\n\\[\\begin{equation*}\nS_1 = \\{ x \\\\mathbb{R} \\: : \\: \\exists y \\\\mathbb{R}, \\: (x,y) \\S\\}\n\\subseteq \\mathbb{R}.\n\\end{equation*}\\]\nWówczas \\(|S_1|\\leq |S|\\). W szczególności \\(S_1\\) jest zbiorem przeliczalnym. Mamy\n\\[\\begin{equation*}\n\\left\\{ \\vec{X} \\S \\right\\} \\subseteq \\{X_1 \\S_1\\}.\n\\end{equation*}\\]\nRzeczywiście, niech \\(\\omega \\\\{\\vec{X}\\S\\}\\). Wówczas \\(\\vec{X}(\\omega) \\S\\).\nMamy \\(\\vec{X}(\\omega) = (X_1(\\omega), X_2(\\omega))\\). Innymi słowy dla \\(y =X_2(\\omega)\\) mamy\n\\((X_1(\\omega), y) \\S\\). Oznacza , że \\(X_1(\\omega) \\S_1\\), czyli \\(\\omega \\\\{ X_1\\S\\}\\).\ndowodzi postulowanej inkluzji. Z monotoniczności prawdopodobieństwa\n\\[\\begin{equation*}\n1=\\mathbb{P}\\left[\\vec{X}\\S \\right] \\leq \\mathbb{P}[X_1 \\S_1].\n\\end{equation*}\\]\nCo kończy dowód jednej implikacji.\nZałóżmy teraz, że rozkłady \\(X_1\\) \\(X_2\\) są dyskretne. Istnieją zatem przeliczalne\n\\(S_1\\), \\(S_2 \\subseteq \\mathbb{R}\\) takie, że\n\\[\\begin{equation*}\n\\mathbb{P}\\left[X_1\\S_1\\right]=\\mathbb{P}[X_2\\S_2]=1.\n\\end{equation*}\\]\nRozważmy zbiór \\(S = S_1 \\times S_2 \\subseteq \\mathbb{R}^2\\). Wówczas \\(S\\) jest zbiorem przeliczalnym.\nMamy\n\\[\\begin{equation*}\n\\left\\{ \\vec{X} \\S \\right\\} = \\{(X_1, X_2) \\S_1\\times S_2\\}\n= \\bigcup_{x \\S_1} \\{X_1 = x, \\: X_2\\S_2 \\}.\n\\end{equation*}\\]\nSkoro powyższa suma jest przeliczalna, zbiory rozłączne\n\\[\\begin{equation*}\n\\mathbb{P}\\left[\\vec{X} \\S \\right] = \\sum_{x \\S_1} \\mathbb{P}[X_1 =x, \\: X_2 \\S_2].\n\\end{equation*}\\]\nZauważmy, że \\(\\mathbb{P}[X_1=x, \\: X_2 \\S_2] = \\mathbb{P}[X_1=x]\\), ponieważ\n\\[\\begin{multline*}\n0 \\leq \\mathbb{P}[X_1 =x]-\\mathbb{P}[X_1=x,\\: X_2 \\S_2] \\\\=\n\\mathbb{P}[X_1=x, \\: X_2 \\notin S_2] \\leq \\mathbb{P}[X_2 \\notin S_2]=0.\n\\end{multline*}\\]\nMamy zatem\n\\[\\begin{equation*}\n\\mathbb{P}\\left[\\vec{X} \\S \\right] = \\sum_{x \\S_1} \\mathbb{P}[X_1 =x] = \\mathbb{P}[X_1\\S_1] =1.\n\\end{equation*}\\]\n","code":""},{"path":"lista-8-wektory-losowe.html","id":"zadania-na-ćwiczenia-6","chapter":"Lista 8: wektory losowe","heading":"Zadania na ćwiczenia","text":"Niech \\((X,Y)\\) będzie dwuwymiarowym wektorem losowym o rozkładzie zadanym gęstością\n\\[\nf(x, y) = C(x+y)\n\\]\ndla \\(0 \\leq y \\leq x \\leq 1\\), \\(f(x, y) = 0\\) poza tym zbiorem.\nZnajdź wartość \\(C\\). Znajdź rozkłady brzegowe.Niech \\((X,Y)\\) będzie dwuwymiarowym wektorem losowym o rozkładzie zadanym gęstością\n\\[\nf(x, y) = C(x+y)\n\\]\ndla \\(0 \\leq y \\leq x \\leq 1\\), \\(f(x, y) = 0\\) poza tym zbiorem.\nZnajdź wartość \\(C\\). Znajdź rozkłady brzegowe.Zmienna losowa \\((X,Y)\\) ma rozkład z gęstością\n\\[\ng(x, y) = C \\cdot xy \\cdot \\mathbf{1}_{[0,1]^2}(x, y)\n\\]\nWyznaczyć \\(C\\).\nObliczyć \\(\\mathbb{P}(X + Y \\leq 1)\\).\nWyznaczyć rozkład zmiennej losowej \\(X/Y\\).\nZmienna losowa \\((X,Y)\\) ma rozkład z gęstością\n\\[\ng(x, y) = C \\cdot xy \\cdot \\mathbf{1}_{[0,1]^2}(x, y)\n\\]Wyznaczyć \\(C\\).Obliczyć \\(\\mathbb{P}(X + Y \\leq 1)\\).Wyznaczyć rozkład zmiennej losowej \\(X/Y\\).Rzucamy trzy razy kostką. Niech \\(X_1\\) będzie liczbą wyrzuconych jedynek. Niech \\(X_2\\) będzie liczbą\nwyrzuconych dwójek.\nZnajdź rozkład wektora losowego \\(\\vec{X}=(X_1, X_2)\\).\nZnajdź \\(\\mathbb{P}[X_1=1, \\: X_2=2 \\: | \\: X_1+X_2=3]\\).\nRzucamy trzy razy kostką. Niech \\(X_1\\) będzie liczbą wyrzuconych jedynek. Niech \\(X_2\\) będzie liczbą\nwyrzuconych dwójek.Znajdź rozkład wektora losowego \\(\\vec{X}=(X_1, X_2)\\).Znajdź \\(\\mathbb{P}[X_1=1, \\: X_2=2 \\: | \\: X_1+X_2=3]\\).Losujemy liczbę \\(\\omega\\) z odcinka \\([0,2\\pi]\\) w sposób jednostajny.\nNiech \\(X_1(\\omega)=\\cos(\\omega)\\), \\(X_2(\\omega) = \\sin(\\omega)\\).\nZnajdź rozkład zmiennej \\(X_1\\).\nNiech \\(\\vec{X} = (X_1, X_2)\\). Znajdź\n\\(\\mathbb{P}\\left[\\vec{X} \\[1/\\sqrt{2},1]\\times [-2,3]\\right]\\).\nLosujemy liczbę \\(\\omega\\) z odcinka \\([0,2\\pi]\\) w sposób jednostajny.\nNiech \\(X_1(\\omega)=\\cos(\\omega)\\), \\(X_2(\\omega) = \\sin(\\omega)\\).Znajdź rozkład zmiennej \\(X_1\\).Niech \\(\\vec{X} = (X_1, X_2)\\). Znajdź\n\\(\\mathbb{P}\\left[\\vec{X} \\[1/\\sqrt{2},1]\\times [-2,3]\\right]\\).Losujemy punkt z trójkąta równobocznego \\(ABC\\). niech \\(X_1\\) będzie odległością wylosowanego punktu\nod boku \\(AB\\), \\(X_2\\) odległością wylosowanego punktu od boku \\(CB\\).\nZnajdź \\(\\mathbb{P}[X_1\\leq t]\\) dla \\(t \\[0. \\sqrt{3}/2]\\).\nZnajdź \\(\\mathbb{P} [X_2 \\leq s \\: | \\: X_1 \\leq t]\\) dla \\(s,t \\[0, \\sqrt{3}/2]\\).\nZnajdź dystrybuantę wektora losowego \\(\\vec{X} =(X_1, X_2)\\).\nLosujemy punkt z trójkąta równobocznego \\(ABC\\). niech \\(X_1\\) będzie odległością wylosowanego punktu\nod boku \\(AB\\), \\(X_2\\) odległością wylosowanego punktu od boku \\(CB\\).Znajdź \\(\\mathbb{P}[X_1\\leq t]\\) dla \\(t \\[0. \\sqrt{3}/2]\\).Znajdź \\(\\mathbb{P} [X_2 \\leq s \\: | \\: X_1 \\leq t]\\) dla \\(s,t \\[0, \\sqrt{3}/2]\\).Znajdź dystrybuantę wektora losowego \\(\\vec{X} =(X_1, X_2)\\).Załóżmy, że wektor losowy \\(\\vec{X}=(X_1, X_2)\\) ma dwuwymiarowy standardowy\nrozkład normalny. Rozważmy zmienne losowe \\(Y_1 = ax_1+bX_2\\) oraz\n\\(Y_2=-X_1/+X_2/b\\) dla \\(, b>0\\).\nZnajdź rozkład wektora losowego \\(\\vec{Y} = (Y_1, Y_2)\\).\nZnajdź rozkład zmiennej losowej \\(Y_1\\).\nZałóżmy, że wektor losowy \\(\\vec{X}=(X_1, X_2)\\) ma dwuwymiarowy standardowy\nrozkład normalny. Rozważmy zmienne losowe \\(Y_1 = ax_1+bX_2\\) oraz\n\\(Y_2=-X_1/+X_2/b\\) dla \\(, b>0\\).Znajdź rozkład wektora losowego \\(\\vec{Y} = (Y_1, Y_2)\\).Znajdź rozkład zmiennej losowej \\(Y_1\\).Niech \\(X = (X_1, X_2)\\) będzie wektorem z dwuwymiarowym rozkładem normalnym o parametrach \\(\\vec{m}=(0,0)\\) oraz\n\\[\\begin{equation*}\n\\Sigma = \\left( \\begin{array}{cc} 1 & \\rho \\\\ \\rho & 1 \\end{array} \\right).\n\\end{equation*}\\]\nPokaż, że \\(\\Sigma\\) jest dodatnio określona wtedy tylko wtedy, gdy \\(\\rho \\(-1,1)\\).\nNiech \\(f_{\\vec{X}}\\) będzie gęstością \\(\\vec{X}\\). Wyznacz poziomice \\(f_{\\vec{X}}\\),\n\\[\\begin{equation*}\n\\left\\{ (x,y) \\\\mathbb{R}^2 \\: : \\: f_{\\vec{X}}(x,y) = c \\right\\}, \\qquad c>0\n\\end{equation*}\\]\nw zależności od parametru \\(\\rho\\(-1,1)\\).\nNiech \\(X = (X_1, X_2)\\) będzie wektorem z dwuwymiarowym rozkładem normalnym o parametrach \\(\\vec{m}=(0,0)\\) oraz\n\\[\\begin{equation*}\n\\Sigma = \\left( \\begin{array}{cc} 1 & \\rho \\\\ \\rho & 1 \\end{array} \\right).\n\\end{equation*}\\]Pokaż, że \\(\\Sigma\\) jest dodatnio określona wtedy tylko wtedy, gdy \\(\\rho \\(-1,1)\\).Niech \\(f_{\\vec{X}}\\) będzie gęstością \\(\\vec{X}\\). Wyznacz poziomice \\(f_{\\vec{X}}\\),\n\\[\\begin{equation*}\n\\left\\{ (x,y) \\\\mathbb{R}^2 \\: : \\: f_{\\vec{X}}(x,y) = c \\right\\}, \\qquad c>0\n\\end{equation*}\\]\nw zależności od parametru \\(\\rho\\(-1,1)\\).Niech \\(\\vec{X}=(X_1, X_2)\\) będzie wektorem losowym o dwuwymiarowym standardowym rozkładzie normalnym.\nRozważamy wektor \\(\\vec{X}\\) współrzędnych biegunowych. Niech \\(R = \\sqrt{X_1^2+X_2^2}\\) niech\n\\(S = \\mathrm{arccos}(X_1/R)\\). Znajdź rozkład wektora losowego \\((R, S)\\).Niech \\(\\vec{X}=(X_1, X_2)\\) będzie wektorem losowym o dwuwymiarowym standardowym rozkładzie normalnym.\nRozważamy wektor \\(\\vec{X}\\) współrzędnych biegunowych. Niech \\(R = \\sqrt{X_1^2+X_2^2}\\) niech\n\\(S = \\mathrm{arccos}(X_1/R)\\). Znajdź rozkład wektora losowego \\((R, S)\\).","code":""},{"path":"lista-8-wektory-losowe.html","id":"zadania-dodatkowe-5","chapter":"Lista 8: wektory losowe","heading":"Zadania dodatkowe","text":"Wektor losowy \\(\\vec{U} = (X, Y, Z)\\) ma następującą własność:\njeżeli\\[\n^2 + b^2 + c^2 = 1,\n\\]\nzmienna losowa\n\\[\naX + + cZ\n\\]\nma rozkład jednostajny na \\([-1, 1]\\). Jaki rozkład ma wektor \\(\\vec{U}\\)?","code":""},{"path":"lista-9-niezależne-zmienne.html","id":"lista-9-niezależne-zmienne","chapter":"Lista 9: Niezależne zmienne","heading":"Lista 9: Niezależne zmienne","text":"Zadania na ćwiczenia: 2025-04-28Lista zadań w formacie PDF","code":""},{"path":"lista-9-niezależne-zmienne.html","id":"zadania-do-samodzielnego-rozwiązania-8","chapter":"Lista 9: Niezależne zmienne","heading":"Zadania do samodzielnego rozwiązania","text":"Załóżmy, że \\(\\xi_1, \\ldots, \\xi_n\\) są niezależnymi zmiennymi losowymi Bernoulliego, dla których\n\\[\n\\mathbb{P}(\\xi_k = 1) = p, \\quad \\mathbb{P}(\\xi_k = 0) = 1 - p, \\quad \\text{dla } 1 \\leq k \\leq n.\n\\]\nWyznacz prawdopodobieństwo warunkowe, że pierwsza jedynka („sukces”) pojawi się w \\(m\\)-tym kroku,\npod warunkiem, że w ciągu \\(n\\) kroków sukces wystąpił dokładnie raz.\n\nOdpowiedź\n\n\n\n\\(1/n\\)\n\nZałóżmy, że \\(\\xi_1, \\ldots, \\xi_n\\) są niezależnymi zmiennymi losowymi Bernoulliego, dla których\n\\[\n\\mathbb{P}(\\xi_k = 1) = p, \\quad \\mathbb{P}(\\xi_k = 0) = 1 - p, \\quad \\text{dla } 1 \\leq k \\leq n.\n\\]\nWyznacz prawdopodobieństwo warunkowe, że pierwsza jedynka („sukces”) pojawi się w \\(m\\)-tym kroku,\npod warunkiem, że w ciągu \\(n\\) kroków sukces wystąpił dokładnie raz.\n\nOdpowiedź\n\n\\(1/n\\)\nZmienne \\(X\\) \\(Y\\) są niezależne. \\(X\\) ma rozkład jednostajny na przedziale \\([0,1]\\),\n\\(Y\\) ma rozkład zadany przez \\(\\mathbb{P}[Y = -1] = 1/3\\), \\(\\mathbb{P}[Y = 2] = 2/3\\).\nOblicz \\(\\mathbb{P}[3X < Y]\\).\nWyznacz rozkład zmiennej \\(XY\\).\n\nOdpowiedź\n\n\n\n\\(4/9\\), b jednostajny na \\([-1,2]\\)\n\n\nZmienne \\(X\\) \\(Y\\) są niezależne. \\(X\\) ma rozkład jednostajny na przedziale \\([0,1]\\),\n\\(Y\\) ma rozkład zadany przez \\(\\mathbb{P}[Y = -1] = 1/3\\), \\(\\mathbb{P}[Y = 2] = 2/3\\).Oblicz \\(\\mathbb{P}[3X < Y]\\).Wyznacz rozkład zmiennej \\(XY\\).\n\nOdpowiedź\n\n\n\n\\(4/9\\), b jednostajny na \\([-1,2]\\)\n\n\n\\(4/9\\), b jednostajny na \\([-1,2]\\)\nZmienne losowe \\(X\\) \\(Y\\) są niezależne mają rozkłady wykładnicze z parametrami odpowiednio\n\\(\\lambda\\) \\(\\mu\\). Znajdź rozkład zmiennej losowej \\(X + Y\\).\n\nOdpowiedź\n\n\n\nJeżeli \\(\\mu \\neq \\lambda\\), \njest rozkład o gęstości \\(\\mu\\lambda (e^{-\\mu x} -e^{-\\lambda x})\\mathbf{1}_{[0, +\\infty)}(x)/(\\lambda -\\mu)\\)\nJeżeli \\(\\mu=\\lambda\\), jest rozkład o gęstości\n\\(\\lambda^2x e^{-\\lambda x}\\mathbf{1}_{[0, +\\infty)}(x)\\).\n\nZmienne losowe \\(X\\) \\(Y\\) są niezależne mają rozkłady wykładnicze z parametrami odpowiednio\n\\(\\lambda\\) \\(\\mu\\). Znajdź rozkład zmiennej losowej \\(X + Y\\).\n\nOdpowiedź\n\nJeżeli \\(\\mu \\neq \\lambda\\), \njest rozkład o gęstości \\(\\mu\\lambda (e^{-\\mu x} -e^{-\\lambda x})\\mathbf{1}_{[0, +\\infty)}(x)/(\\lambda -\\mu)\\)\nJeżeli \\(\\mu=\\lambda\\), jest rozkład o gęstości\n\\(\\lambda^2x e^{-\\lambda x}\\mathbf{1}_{[0, +\\infty)}(x)\\).\nPodaj przykład dwóch zależnych zmiennych losowych \\(X\\) \\(Y\\),\ndla których \\(X^2\\) \\(Y^2\\) są niezależne.\n\nOdpowiedź\n\n\n\n\\(X=Y\\) takie, że \\(\\mathbb{P}[X=1=\\mathbb{P}[X=-1]=1/2\\).\n\nPodaj przykład dwóch zależnych zmiennych losowych \\(X\\) \\(Y\\),\ndla których \\(X^2\\) \\(Y^2\\) są niezależne.\n\nOdpowiedź\n\n\\(X=Y\\) takie, że \\(\\mathbb{P}[X=1=\\mathbb{P}[X=-1]=1/2\\).\nZałóżmy, że \\(\\xi_1, \\ldots, \\xi_n\\) są niezależnymi identycznie rozłożonymi zmiennymi losowymi,\ndla których:\n\\[\n\\mathbb{P}\\{\\xi_j = 1\\} = p, \\quad \\mathbb{P}\\{\\xi_j = 0\\} = 1 - p,\n\\]\ndla pewnego \\(0 < p < 1\\). Niech \\(S_k = \\xi_1 + \\ldots + \\xi_k\\), gdzie \\(k \\leq n\\).\nUdowodnij, że dla \\(1 \\leq m \\leq n\\) zachodzi:\n\\[\n\\mathbb{P}(S_m = k \\mid S_n = l) = \\frac{\\binom{m}{k} \\binom{n - m}{l - k}}{\\binom{n}{l}}.\n\\]\n\nOdpowiedź\n\n\n\nZauważmy, że \\(S_n-S_m = \\xi_{m+1}+\\ldots + \\xi_n\\). Wobec tego zmienne\n\\(S_m\\) oraz \\(S_n-S_m\\) są niezależne mają rozkłady odpowiednio \\(\\mathrm{Bin}(m,p)\\) oraz \\(\\mathrm{Bin}(n-m,p)\\). Mamy więc\n\\[\\begin{multline*}\n\\mathbb{P}[S_m=k, \\: S_n=l]\n= \\mathbb{P}[S_m=k, \\: S_n-S_m=l-k] \\\\\n= {m \\choose k}p^k(1-p)^{m-k}{n-m \\choose l-k} p^{l-k}(1-p)^{n-m-l+k}\n\\end{multline*}\\]\nPodstawiając powyższe wyliczenie wzoru na prawdopodobieństwo całkowite otrzymujemy tezę.\n\nZałóżmy, że \\(\\xi_1, \\ldots, \\xi_n\\) są niezależnymi identycznie rozłożonymi zmiennymi losowymi,\ndla których:\n\\[\n\\mathbb{P}\\{\\xi_j = 1\\} = p, \\quad \\mathbb{P}\\{\\xi_j = 0\\} = 1 - p,\n\\]\ndla pewnego \\(0 < p < 1\\). Niech \\(S_k = \\xi_1 + \\ldots + \\xi_k\\), gdzie \\(k \\leq n\\).\nUdowodnij, że dla \\(1 \\leq m \\leq n\\) zachodzi:\n\\[\n\\mathbb{P}(S_m = k \\mid S_n = l) = \\frac{\\binom{m}{k} \\binom{n - m}{l - k}}{\\binom{n}{l}}.\n\\]\n\nOdpowiedź\n\nZauważmy, że \\(S_n-S_m = \\xi_{m+1}+\\ldots + \\xi_n\\). Wobec tego zmienne\n\\(S_m\\) oraz \\(S_n-S_m\\) są niezależne mają rozkłady odpowiednio \\(\\mathrm{Bin}(m,p)\\) oraz \\(\\mathrm{Bin}(n-m,p)\\). Mamy więc\n\\[\\begin{multline*}\n\\mathbb{P}[S_m=k, \\: S_n=l]\n= \\mathbb{P}[S_m=k, \\: S_n-S_m=l-k] \\\\\n= {m \\choose k}p^k(1-p)^{m-k}{n-m \\choose l-k} p^{l-k}(1-p)^{n-m-l+k}\n\\end{multline*}\\]\nPodstawiając powyższe wyliczenie wzoru na prawdopodobieństwo całkowite otrzymujemy tezę.\n","code":""},{"path":"lista-9-niezależne-zmienne.html","id":"zadania-na-ćwiczenia-7","chapter":"Lista 9: Niezależne zmienne","heading":"Zadania na ćwiczenia","text":"Niech \\(X\\) będzie zmienną losową posiadającą wartość oczekiwaną. Pokaż, że dla zdarzenia\n\\(\\) o dodatnim prawdopodobieństwie\n\\[\\begin{equation*}\n\\mathbb{E}[X|] = \\frac{1}{\\mathbb{P}[]} \\mathbb{E}\\left[ X \\mathbf{1}_A \\right].\n  \\end{equation*}\\]Niech \\(X\\) będzie zmienną losową posiadającą wartość oczekiwaną. Pokaż, że dla zdarzenia\n\\(\\) o dodatnim prawdopodobieństwie\n\\[\\begin{equation*}\n\\mathbb{E}[X|] = \\frac{1}{\\mathbb{P}[]} \\mathbb{E}\\left[ X \\mathbf{1}_A \\right].\n  \\end{equation*}\\]Niech \\(\\vec{X}=(X_1, \\ldots , X_n)\\), gdzie zmienne \\(X_1, \\ldots , X_n\\) są niezależne z gęstościami odpowiednio\n\\(f_1, \\ldots , f_n\\). Pokaż, że \\(X_1, \\ldots, X_n\\) są niezależne wtedy tylko wtedy, gdy wektor losowy\n\\(\\vec{X}\\) ma rozkład o gęstości\n\\[\\begin{equation*}\nf_{\\vec{X}}(x_1,x_2 \\ldots , x_n) = f_1(x_1) \\cdot f_2(x_2) \\cdots f_n(x_n).\n\\end{equation*}\\]Niech \\(\\vec{X}=(X_1, \\ldots , X_n)\\), gdzie zmienne \\(X_1, \\ldots , X_n\\) są niezależne z gęstościami odpowiednio\n\\(f_1, \\ldots , f_n\\). Pokaż, że \\(X_1, \\ldots, X_n\\) są niezależne wtedy tylko wtedy, gdy wektor losowy\n\\(\\vec{X}\\) ma rozkład o gęstości\n\\[\\begin{equation*}\nf_{\\vec{X}}(x_1,x_2 \\ldots , x_n) = f_1(x_1) \\cdot f_2(x_2) \\cdots f_n(x_n).\n\\end{equation*}\\]Niech \\(X_1, \\ldots, X_n\\) będą niezależnymi zmiennymi losowymi o rozkładzie wykładniczym z parametrem \\(1\\). Znajdź rozkład \\(Y = \\min_{1 \\leq \\leq n} X_i\\). Czy \\(X_n\\) \\(Y\\) są niezależne?Niech \\(X_1, \\ldots, X_n\\) będą niezależnymi zmiennymi losowymi o rozkładzie wykładniczym z parametrem \\(1\\). Znajdź rozkład \\(Y = \\min_{1 \\leq \\leq n} X_i\\). Czy \\(X_n\\) \\(Y\\) są niezależne?Niech \\(X\\) \\(Y\\) będą niezależnymi zmiennymi losowymi o wartościach całkowitych. Pokaż, że dla każdej wartości \\(k\\) zachodzi\n\\[\n\\mathbb{P}[X + Y = k] = \\sum_{j = -\\infty}^{+\\infty} \\mathbb{P}[X = k - j] \\mathbb{P}[Y = j].\n\\]Niech \\(X\\) \\(Y\\) będą niezależnymi zmiennymi losowymi o wartościach całkowitych. Pokaż, że dla każdej wartości \\(k\\) zachodzi\n\\[\n\\mathbb{P}[X + Y = k] = \\sum_{j = -\\infty}^{+\\infty} \\mathbb{P}[X = k - j] \\mathbb{P}[Y = j].\n\\]Zmienne losowe \\(X_1, \\ldots, X_n\\) są niezależne mają rozkłady Poissona z parametrami \\(\\lambda_i\\). Pokaż, że \\(X_1 + \\ldots + X_n\\) ma rozkład Poissona z parametrem \\(\\lambda_1 + \\ldots + \\lambda_n\\).Zmienne losowe \\(X_1, \\ldots, X_n\\) są niezależne mają rozkłady Poissona z parametrami \\(\\lambda_i\\). Pokaż, że \\(X_1 + \\ldots + X_n\\) ma rozkład Poissona z parametrem \\(\\lambda_1 + \\ldots + \\lambda_n\\).Załóżmy, że \\(X_1\\) \\(X_2\\) są niezależnymi zmiennymi losowymi o rozkładach odpowiednio\n\\(\\mathcal{N}(m_1, \\sigma_1)\\) \\(\\mathcal{N}(m_2, \\sigma_2)\\).\nZnajdź rozkład zmiennej losowej \\(X_1 + X_2\\).Załóżmy, że \\(X_1\\) \\(X_2\\) są niezależnymi zmiennymi losowymi o rozkładach odpowiednio\n\\(\\mathcal{N}(m_1, \\sigma_1)\\) \\(\\mathcal{N}(m_2, \\sigma_2)\\).\nZnajdź rozkład zmiennej losowej \\(X_1 + X_2\\).Niech \\(E_1\\) \\(E_2\\) będą niezaleznymi zmiennymi losowymi o rozkładach Exp(\\(\\lambda\\)).\nZnajdź rozkład \\(E_1\\) pod warunkiem \\(\\{ E_1 \\leq t < E_1+E_2\\}\\) dla \\(t>0\\).Niech \\(E_1\\) \\(E_2\\) będą niezaleznymi zmiennymi losowymi o rozkładach Exp(\\(\\lambda\\)).\nZnajdź rozkład \\(E_1\\) pod warunkiem \\(\\{ E_1 \\leq t < E_1+E_2\\}\\) dla \\(t>0\\).Niech \\(\\Omega\\subseteq \\mathbb{R}\\) będzie kołem jednostkowym. Rozważmy \\(\\mathcal{F}=\\mathcal{B}(\\Omega)\\)\noraz \\(\\mathbb{P}\\) jako unormowaną dwuwymiarową miarę Lebesgue’. Rozważmy zmienne losowe\n\\(X(\\omega) = \\omega_1/\\sqrt{\\omega_1^2+\\omega_2^2}\\), \\(Y(\\omega)=\\omega_2/\\sqrt{\\omega_1^2+\\omega_2^2}\\) oraz\n\\(Z(\\omega) = \\omega_1^2+\\omega_2^2\\) dla \\(\\omega=(\\omega_1, \\omega_2) \\\\Omega\\).\nCzy zmienne \\(X\\) \\(Y\\) są niezależne?\nCzy zmienne \\(X\\) \\(Z\\) są niezależne?\nNiech \\(\\Omega\\subseteq \\mathbb{R}\\) będzie kołem jednostkowym. Rozważmy \\(\\mathcal{F}=\\mathcal{B}(\\Omega)\\)\noraz \\(\\mathbb{P}\\) jako unormowaną dwuwymiarową miarę Lebesgue’. Rozważmy zmienne losowe\n\\(X(\\omega) = \\omega_1/\\sqrt{\\omega_1^2+\\omega_2^2}\\), \\(Y(\\omega)=\\omega_2/\\sqrt{\\omega_1^2+\\omega_2^2}\\) oraz\n\\(Z(\\omega) = \\omega_1^2+\\omega_2^2\\) dla \\(\\omega=(\\omega_1, \\omega_2) \\\\Omega\\).Czy zmienne \\(X\\) \\(Y\\) są niezależne?Czy zmienne \\(X\\) \\(Z\\) są niezależne?Niech \\(Z\\) będzie zmienną losową o rozkładzie \\(\\mathrm{Exp}(1)\\).\nNiech \\(\\{Z\\}\\) oznacza część ułamkową zmiennej \\(Z\\), \\([Z]\\) — jej część całkowitą.\nUdowodnij, że \\(\\{Z\\}\\) \\([Z]\\) są niezależne oraz wyznacz ich rozkłady jawnie.\nRozważmy dodatnią zmienną losową \\(X\\), której rozkład jest absolutnie ciągły z\ngęstością \\(\\varphi\\)\ntaką, że \\(\\{X\\}\\) \\([X]\\) są niezależne \\(\\{X\\}\\) ma rozkład jednostajny na \\([0, 1]\\).\nZnajdź \\(\\varphi\\).\nNiech \\(Z\\) będzie zmienną losową o rozkładzie \\(\\mathrm{Exp}(1)\\).\nNiech \\(\\{Z\\}\\) oznacza część ułamkową zmiennej \\(Z\\), \\([Z]\\) — jej część całkowitą.Udowodnij, że \\(\\{Z\\}\\) \\([Z]\\) są niezależne oraz wyznacz ich rozkłady jawnie.Rozważmy dodatnią zmienną losową \\(X\\), której rozkład jest absolutnie ciągły z\ngęstością \\(\\varphi\\)\ntaką, że \\(\\{X\\}\\) \\([X]\\) są niezależne \\(\\{X\\}\\) ma rozkład jednostajny na \\([0, 1]\\).\nZnajdź \\(\\varphi\\).","code":""},{"path":"lista-9-niezależne-zmienne.html","id":"zadania-dodatkowe-6","chapter":"Lista 9: Niezależne zmienne","heading":"Zadania dodatkowe","text":"Z odcinka \\([0,1]\\) losujemy niezależnie w sposób jednostajny liczby \\(X_1, X_2, \\ldots\\).\nUzasadnij, że z prawdopodobieństwem \\(1\\), ciąg \\(\\{X_n\\}\\) jest gęsty w odcinku \\([0,1]\\).Z odcinka \\([0,1]\\) losujemy niezależnie w sposób jednostajny liczby \\(X_1, X_2, \\ldots\\).\nUzasadnij, że z prawdopodobieństwem \\(1\\), ciąg \\(\\{X_n\\}\\) jest gęsty w odcinku \\([0,1]\\).Scharakteryzuj gęstości doodatnich zmiennych losowych \\(X\\) dla których \\([X]\\) oraz \\(\\{X\\}\\) są\nniezależne.Scharakteryzuj gęstości doodatnich zmiennych losowych \\(X\\) dla których \\([X]\\) oraz \\(\\{X\\}\\) są\nniezależne.Niech \\(\\Gamma \\\\mathcal{F}\\) niech \\(\\mathcal{G} \\subseteq \\mathcal{F}\\) będzie \\(\\sigma\\)-ciałem.\nUdowodnij, że następujące warunki są równoważne:\n\\(\\Gamma\\) jest niezależny od \\(\\mathcal{G}\\) względem \\(\\mathbb{P}\\),\ndla każdego prawdopodobieństwa \\(\\mathbb{Q}\\) na \\((\\Omega, \\mathcal{F})\\),\nrównoważnego z \\(\\mathbb{P}\\), takiego że \\(\\left(\\frac{d\\mathbb{Q}}{d\\mathbb{P}}\\right)\\)\njest \\(\\mathcal{G}\\)-mierzalna, zachodzi \\(\\mathbb{Q}(\\Gamma) = \\mathbb{P}(\\Gamma)\\).\nNiech \\(\\Gamma \\\\mathcal{F}\\) niech \\(\\mathcal{G} \\subseteq \\mathcal{F}\\) będzie \\(\\sigma\\)-ciałem.\nUdowodnij, że następujące warunki są równoważne:\\(\\Gamma\\) jest niezależny od \\(\\mathcal{G}\\) względem \\(\\mathbb{P}\\),dla każdego prawdopodobieństwa \\(\\mathbb{Q}\\) na \\((\\Omega, \\mathcal{F})\\),\nrównoważnego z \\(\\mathbb{P}\\), takiego że \\(\\left(\\frac{d\\mathbb{Q}}{d\\mathbb{P}}\\right)\\)\njest \\(\\mathcal{G}\\)-mierzalna, zachodzi \\(\\mathbb{Q}(\\Gamma) = \\mathbb{P}(\\Gamma)\\).","code":""},{"path":"lista-10-wariancja.html","id":"lista-10-wariancja","chapter":"Lista 10: Wariancja","heading":"Lista 10: Wariancja","text":"Zadania na ćwiczenia: 2025-05-12Lista zadań w formacie PDF","code":""},{"path":"lista-10-wariancja.html","id":"zadania-do-samodzielnego-rozwiązania-9","chapter":"Lista 10: Wariancja","heading":"Zadania do samodzielnego rozwiązania","text":"Zmienne losowe \\(X, Y\\) spełniają warunki: \\(\\mathbb{V}ar[X] = 3\\), \\(\\mathrm{Cov}(X,Y) = -1\\), \\(\\mathbb{V}ar[Y] = 2\\).\nOblicz \\(\\mathbb{V}ar[4X - 3Y]\\) oraz \\(\\mathrm{Cov}(2X - Y, 2X + Y)\\).\n\nOdpowiedź\n\n\n\n\\(\\mathbb{V}ar[4X - 3Y]=90\\) oraz \\(\\mathrm{Cov}(2X - Y, 2X + Y)=10\\).\n\nZmienne losowe \\(X, Y\\) spełniają warunki: \\(\\mathbb{V}ar[X] = 3\\), \\(\\mathrm{Cov}(X,Y) = -1\\), \\(\\mathbb{V}ar[Y] = 2\\).\nOblicz \\(\\mathbb{V}ar[4X - 3Y]\\) oraz \\(\\mathrm{Cov}(2X - Y, 2X + Y)\\).\n\nOdpowiedź\n\n\\(\\mathbb{V}ar[4X - 3Y]=90\\) oraz \\(\\mathrm{Cov}(2X - Y, 2X + Y)=10\\).\nNiech \\(\\vec{X}= (X, Y)\\) będzie jednostajnie wylosowanym punktem kwadratu jednostkowego \\([0, 1]^2\\).\nZnajdź \\(\\mathbb{V}ar(X)\\), \\(\\mathbb{V}ar(Y)\\), \\(\\mathrm{Cov}(X, Y)\\).\n\nOdpowiedź\n\n\n\n\\(\\mathrm{Var}(X)= 1/12\\), \\(\\mathrm{Var}(Y)=1/12\\), \\(\\mathrm{Cov}(X, Y)=0\\).\n\nNiech \\(\\vec{X}= (X, Y)\\) będzie jednostajnie wylosowanym punktem kwadratu jednostkowego \\([0, 1]^2\\).\nZnajdź \\(\\mathbb{V}ar(X)\\), \\(\\mathbb{V}ar(Y)\\), \\(\\mathrm{Cov}(X, Y)\\).\n\nOdpowiedź\n\n\\(\\mathrm{Var}(X)= 1/12\\), \\(\\mathrm{Var}(Y)=1/12\\), \\(\\mathrm{Cov}(X, Y)=0\\).\nZnajdź wariancję dla zmiennej losowej \\(X\\) o rozkładzie\n\\(\\mathrm{Pois}(\\lambda)\\), \\(\\lambda>0\\).\n\\(\\mathcal{U}[,b]\\), \\(<b\\)\n\\(\\mathrm{Exp}(\\lambda)\\), \\(\\lambda>0\\).\nZnajdź wariancję dla zmiennej losowej \\(X\\) o rozkładzie\\(\\mathrm{Pois}(\\lambda)\\), \\(\\lambda>0\\).\\(\\mathcal{U}[,b]\\), \\(<b\\)\\(\\mathrm{Exp}(\\lambda)\\), \\(\\lambda>0\\).\n\\(\\lambda\\), b \\((b-)^2/12\\), c \\(1/\\lambda^2\\)\nWykaż, że jeżeli \\(X\\) jest zmienną losową całkowalną z kwadratem, :\n\\(\\mathbb{V}ar[cX] = c^2 \\mathbb{V}ar[X]\\) dla każdego \\(c \\\\mathbb{R}\\);\n\\(\\mathbb{V}ar[X + ] = \\mathbb{V}ar[X]\\) dla każdego \\(\\\\mathbb{R}\\);\n\\(\\mathbb{V}ar[X] = 0\\) wtedy tylko wtedy, gdy zmienna losowa \\(X\\) jest stała z prawdopodobieństwem 1.\nWykaż, że jeżeli \\(X\\) jest zmienną losową całkowalną z kwadratem, :\\(\\mathbb{V}ar[cX] = c^2 \\mathbb{V}ar[X]\\) dla każdego \\(c \\\\mathbb{R}\\);\\(\\mathbb{V}ar[X + ] = \\mathbb{V}ar[X]\\) dla każdego \\(\\\\mathbb{R}\\);\\(\\mathbb{V}ar[X] = 0\\) wtedy tylko wtedy, gdy zmienna losowa \\(X\\) jest stała z prawdopodobieństwem 1.\n\n\\[\\begin{align*}\n\\mathbb{V}ar(cX) & = \\mathbb{E}[(cX)^2] - (\\mathbb{E}[cX])^2 \\\\\n& = c^2 \\mathbb{E}[X^2] - c^2 (\\mathbb{E}[X])^2 \\\\\n& = c^2 (\\mathbb{E}[X^2] - (\\mathbb{E}[X])^2) \\\\\n&= c^2 \\mathbb{V}ar(X)\n\\end{align*}\\]\nb\n\\[\\begin{align*}\n\\mathbb{V}ar(X + ) & = \\mathbb{E}[(X + )^2] - (\\mathbb{E}[X + ])^2 \\\\\n&= \\mathbb{E}[X^2 + 2aX + ^2] - (\\mathbb{E}[X] + )^2\\\\\n&= \\mathbb{E}[X^2] + 2a\\mathbb{E}[X] + ^2 - (\\mathbb{E}[X]^2 + 2a\\mathbb{E}[X] + ^2)\\\\\n&= \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 = \\mathbb{V}ar(X)\n\\end{align*}\\]\nc (\\(\\Rightarrow\\)) Jeśli \\(\\text{Var}(X) = 0\\), całka z funkcji nieujemnej \\((X-\\mathbb{E}[X])^2\\) jest równa zero.\nOznacza , że owa funkcja jest równa zero p.w. Czyli \\(X =\\mathbb{E}[X]\\) p.w.\n(\\(\\Leftarrow\\)) Jeśli \\(X = c\\) z prawdopodobieństwem 1, :\n\\[\n\\text{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 = c^2 - c^2 = 0\n\\]\nNiech \\(X\\), \\(Y\\) \\(Z\\) będą zmiennymi losowymi całkowalnymi z kwadratem. Pokaż, że\n\\(\\mathrm{Cov}(X, Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y].\\)\n\\(\\mathrm{Cov}(X, X) = \\mathrm{Var}[X].\\)\n\\(\\mathrm{Cov}(X, Y) = \\mathrm{Cov}(Y, X).\\)\nKowariancja jest operatorem dwuliniowym:\n\\[\n\\mathrm{Cov}(aX + , Z) = \\,\\mathrm{Cov}(X, Z) + b\\,\\mathrm{Cov}(Y, Z).\n\\]\nNiech \\(X\\), \\(Y\\) \\(Z\\) będą zmiennymi losowymi całkowalnymi z kwadratem. Pokaż, że\\(\\mathrm{Cov}(X, Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y].\\)\\(\\mathrm{Cov}(X, X) = \\mathrm{Var}[X].\\)\\(\\mathrm{Cov}(X, Y) = \\mathrm{Cov}(Y, X).\\)Kowariancja jest operatorem dwuliniowym:\n\\[\n\\mathrm{Cov}(aX + , Z) = \\,\\mathrm{Cov}(X, Z) + b\\,\\mathrm{Cov}(Y, Z).\n\\]\nZ definicji kowariancji:\n\\[\\begin{align*}\n\\text{Cov}(X, Y) &  = \\mathbb{E}[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])]\\\\\n& = \\mathbb{E}[XY - X\\mathbb{E}[Y] - \\mathbb{E}[X]Y + \\mathbb{E}[X]\\mathbb{E}[Y]]\\\\\n& = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y] - \\mathbb{E}[X]\\mathbb{E}[Y] + \\mathbb{E}[X]\\mathbb{E}[Y] \\\\\n& = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]\n\\end{align*}\\]\nb\n\\[\\begin{align*}\n\\text{Cov}(X, X) & = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 = \\text{Var}(X)\n\\end{align*}\\]\nc Z definicji:\n\\[\\begin{align*}\n\\text{Cov}(X, Y) & = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]\\\\\n\\text{Cov}(Y, X) & = \\mathbb{E}[YX] - \\mathbb{E}[Y]\\mathbb{E}[X] \\\\\n& = \\text{Cov}(X, Y)\n\\end{align*}\\]\nd Niech \\(, b \\\\mathbb{R}\\). Udowodnijmy:\n\\[\\begin{align*}\n\\text{Cov}(aX + , Z) & = \\text{Cov}(X, Z) + b \\text{Cov}(Y, Z)\\\\\n& = \\mathbb{E}[(aX + )Z] - \\mathbb{E}[aX + ] \\mathbb{E}[Z]\\\\\n& = \\mathbb{E}[XZ] + b\\mathbb{E}[YZ] - (\\mathbb{E}[X] + b\\mathbb{E}[Y])\\mathbb{E}[Z]\\\\\n& = (\\mathbb{E}[XZ] - \\mathbb{E}[X]\\mathbb{E}[Z]) + b(\\mathbb{E}[YZ] - \\mathbb{E}[Y]\\mathbb{E}[Z])\\\\\n& = \\text{Cov}(X, Z) + b \\text{Cov}(Y, Z)\n\\end{align*}\\]\nLosujemy jednostajnie punkt \\(\\vec{X}=(X,Y)\\) z koła\n\\[ \\left\\{(x,y) \\\\mathbb{R}^2 \\: : \\: x^2+y^2-2x-4y\\leq -4  \\right\\}.\\]\nZnajdź \\(\\mathrm{Cov}(X,Y)\\).\n\\(0\\)\n","code":""},{"path":"lista-10-wariancja.html","id":"zadania-na-ćwiczenia-8","chapter":"Lista 10: Wariancja","heading":"Zadania na ćwiczenia","text":"Załóżmy, że wektor losowy \\(\\vec{X}= (X,Y)\\) ma dwuwymiarowy rozkład normalny.\nPokaż, że \\(X\\) \\(Y\\) są niezależne wtedy tylko wtedy, gdy \\(\\mathrm{Cov}(X,Y)=0\\).Załóżmy, że wektor losowy \\(\\vec{X}= (X,Y)\\) ma dwuwymiarowy rozkład normalny.\nPokaż, że \\(X\\) \\(Y\\) są niezależne wtedy tylko wtedy, gdy \\(\\mathrm{Cov}(X,Y)=0\\).Podaj przykład zmiennych losowych \\(X\\) \\(Y\\) takich, że \\(X\\) \\(Y\\) mają standardowy jednowymiarowy rozkład normalny,\n\\(\\mathrm{Cov}(X,Y)=0\\), ale \\(X\\) \\(Y\\) nie są niezależne.Podaj przykład zmiennych losowych \\(X\\) \\(Y\\) takich, że \\(X\\) \\(Y\\) mają standardowy jednowymiarowy rozkład normalny,\n\\(\\mathrm{Cov}(X,Y)=0\\), ale \\(X\\) \\(Y\\) nie są niezależne.Niech \\(X_1\\) \\(X_2\\) będą niezależnymi zmiennymi losowymi o rozkładzie \\(\\mathcal{N}(0,1)\\).\nWykaż, że zmienne losowe \\(\\frac{X_1 + X_2}{\\sqrt{2}}\\) \\(\\frac{X_1 - X_2}{\\sqrt{2}}\\)\nsą niezależne obie mają rozkład \\(\\mathcal{N}(0,1)\\).Niech \\(X_1\\) \\(X_2\\) będą niezależnymi zmiennymi losowymi o rozkładzie \\(\\mathcal{N}(0,1)\\).\nWykaż, że zmienne losowe \\(\\frac{X_1 + X_2}{\\sqrt{2}}\\) \\(\\frac{X_1 - X_2}{\\sqrt{2}}\\)\nsą niezależne obie mają rozkład \\(\\mathcal{N}(0,1)\\).Na płaszczyźnie zaznaczono punkty \\(p_1, \\ldots , p_n\\) w taki sposób, że żadne trzy nie są współliniowe.\nKażda para punktów została połączona odcinkiem z prawdopodobieństwem \\(p\\(0,1)\\). Niech \\(X\\)\noznacza liczbę powstałych trójkątów o wierzchołkach w punktach \\(p_1, \\ldots , p_n\\). Oblicz \\(\\mathbb{V}ar[X]\\).Na płaszczyźnie zaznaczono punkty \\(p_1, \\ldots , p_n\\) w taki sposób, że żadne trzy nie są współliniowe.\nKażda para punktów została połączona odcinkiem z prawdopodobieństwem \\(p\\(0,1)\\). Niech \\(X\\)\noznacza liczbę powstałych trójkątów o wierzchołkach w punktach \\(p_1, \\ldots , p_n\\). Oblicz \\(\\mathbb{V}ar[X]\\).Niech \\(X\\) będzie zmienną losową o rozkładzie \\(\\mathcal{N}(\\mu, \\sigma^2)\\). Pokaż, że dla każdej \\(\\theta \\\\mathbb{R}\\),\n\\[\\begin{equation*}\n\\mathbb{E}\\left[ e^{\\theta X} \\right] = e^{\\theta \\mu +\\theta^2\\sigma^2/2}.\n\\end{equation*}\\]Niech \\(X\\) będzie zmienną losową o rozkładzie \\(\\mathcal{N}(\\mu, \\sigma^2)\\). Pokaż, że dla każdej \\(\\theta \\\\mathbb{R}\\),\n\\[\\begin{equation*}\n\\mathbb{E}\\left[ e^{\\theta X} \\right] = e^{\\theta \\mu +\\theta^2\\sigma^2/2}.\n\\end{equation*}\\]Niech \\(\\vec{X}=(X,Y)\\) będzie wektorem losowym o dwuwymiarowym rozkładzie normalnym z parametrami \\(\\vec{m}=(0,0)\\) \n\\[\\begin{equation*}\n\\Sigma = \\left(\\begin{array}{cc} & \\rho \\\\ \\rho & b \\end{array} \\right)\n\\end{equation*}\\]\ndla \\(\\rho < \\sqrt{ab}\\). Znajdź rozkład zmiennej \\(X\\) wywnioskuj, że \\(\\mathbb{V}ar[X]=\\).Niech \\(\\vec{X}=(X,Y)\\) będzie wektorem losowym o dwuwymiarowym rozkładzie normalnym z parametrami \\(\\vec{m}=(0,0)\\) \n\\[\\begin{equation*}\n\\Sigma = \\left(\\begin{array}{cc} & \\rho \\\\ \\rho & b \\end{array} \\right)\n\\end{equation*}\\]\ndla \\(\\rho < \\sqrt{ab}\\). Znajdź rozkład zmiennej \\(X\\) wywnioskuj, że \\(\\mathbb{V}ar[X]=\\).Dla zmiennej losowej \\(X\\) całkowalnej z kwadratem zdarzenia \\(\\) o dodatnim prawdopodobieństwie definiujemy\nwariancję \\(X\\) pod warunkiem \\(\\) wzorem\n\\[\\begin{equation*}\n\\mathbb{V}ar[X|] = \\mathbb{E} \\left[ \\left. (X - \\mathbb{E}[X|])^2 \\right|\\right].\n\\end{equation*}\\]\nPokaż, że dla rozbicia \\(\\{A_j\\}_{j \\\\mathbb{N}}\\) zbioru \\(\\Omega\\) na zdarzenia o dodatnim prawdopodobieństwie zachodzi\n\\[\n\\mathbb{E}[X] = \\sum_i \\mathbb{P}[A_i] \\mathbb{E}[X|A_i]\n\\]\noraz\n\\[\n\\mathbb{V}ar(X) = \\sum_i \\mathbb{P}(A_i) \\cdot \\mathbb{V}ar(X \\mid A_i) + \\sum_i \\mathbb{P}(A_i) \\cdot\n\\left( \\mathbb{E}[X \\mid A_i] - \\mathbb{E}[X] \\right)^2.\n  \\]Dla zmiennej losowej \\(X\\) całkowalnej z kwadratem zdarzenia \\(\\) o dodatnim prawdopodobieństwie definiujemy\nwariancję \\(X\\) pod warunkiem \\(\\) wzorem\n\\[\\begin{equation*}\n\\mathbb{V}ar[X|] = \\mathbb{E} \\left[ \\left. (X - \\mathbb{E}[X|])^2 \\right|\\right].\n\\end{equation*}\\]\nPokaż, że dla rozbicia \\(\\{A_j\\}_{j \\\\mathbb{N}}\\) zbioru \\(\\Omega\\) na zdarzenia o dodatnim prawdopodobieństwie zachodzi\n\\[\n\\mathbb{E}[X] = \\sum_i \\mathbb{P}[A_i] \\mathbb{E}[X|A_i]\n\\]\noraz\n\\[\n\\mathbb{V}ar(X) = \\sum_i \\mathbb{P}(A_i) \\cdot \\mathbb{V}ar(X \\mid A_i) + \\sum_i \\mathbb{P}(A_i) \\cdot\n\\left( \\mathbb{E}[X \\mid A_i] - \\mathbb{E}[X] \\right)^2.\n  \\]Rzucamy kostką aż momentu otrzymania pierwszej jedynki. Niech \\(X\\) będzie sumą wyrzuconych oczek. Znajdź \\(\\mathbb{E}[X]\\)\noraz \\(\\mathbb{V}ar[X]\\).Rzucamy kostką aż momentu otrzymania pierwszej jedynki. Niech \\(X\\) będzie sumą wyrzuconych oczek. Znajdź \\(\\mathbb{E}[X]\\)\noraz \\(\\mathbb{V}ar[X]\\).","code":""},{"path":"lista-10-wariancja.html","id":"zadania-dodatkowe-7","chapter":"Lista 10: Wariancja","heading":"Zadania dodatkowe","text":"Niech \\(\\vec{X}\\) \\(\\vec{Y}\\) będą niezależnymi \\(d\\)-wymiarowymi wektorami losowymi z\nrozkładem normalnym o parametrach \\((0, \\ldots, 0)\\) macierzy kowariancji \\(I_d\\) (identyczność).Udowodnij, że dla dowolnych \\(f, g \\\\mathcal{C}_b^2(\\mathbb{R}^d)\\) zachodzi\n\\[\\begin{equation*}\n    \\mathrm{Cov}(f(X), g(X)) =\n    \\int_0^1 \\mathbb{E} \\left[\n    \\left\\langle \\nabla f(X), \\nabla g(\\alpha X + \\sqrt{1 - \\alpha^2} Y)\n    \\right\\rangle \\right] \\mathrm{d}\\alpha,\n    \\end{equation*}\\]\ngdzie \\(\\nabla f(x) = \\left( \\frac{\\partial f}{\\partial x_i}(x) \\right)\\).\nNajpierw sprawdź wzór dla \\(f(x) = e^{\\langle t, x \\rangle}\\) oraz \\(g(x) = e^{\\langle s, x \\rangle}\\),\ngdzie \\(s, t, x \\\\mathbb{R}^d\\).Udowodnij, że dla dowolnych \\(f, g \\\\mathcal{C}_b^2(\\mathbb{R}^d)\\) zachodzi\n\\[\\begin{equation*}\n    \\mathrm{Cov}(f(X), g(X)) =\n    \\int_0^1 \\mathbb{E} \\left[\n    \\left\\langle \\nabla f(X), \\nabla g(\\alpha X + \\sqrt{1 - \\alpha^2} Y)\n    \\right\\rangle \\right] \\mathrm{d}\\alpha,\n    \\end{equation*}\\]\ngdzie \\(\\nabla f(x) = \\left( \\frac{\\partial f}{\\partial x_i}(x) \\right)\\).\nNajpierw sprawdź wzór dla \\(f(x) = e^{\\langle t, x \\rangle}\\) oraz \\(g(x) = e^{\\langle s, x \\rangle}\\),\ngdzie \\(s, t, x \\\\mathbb{R}^d\\).Niech \\(\\mu_\\alpha\\) będzie miarą probabilistyczną na \\(\\mathbb{R}^{2d}\\),\nktóra jest rozkładem wektora\n\\[\n    \\left(X, \\alpha X + \\sqrt{1 - \\alpha^2} Y\\right),\n    \\]\nniech \\(\\mu\\) oznacza miarą probabilistyczną daną przez\n\\[\n    \\int_0^1 \\mu_\\alpha \\, \\mathrm{d}\\alpha.\n    \\]\nNiech \\(Z\\) będzie wektorem losowym w \\(\\mathbb{R}^d\\) takim, że wektor \\((X, Z)\\) w \\(\\mathbb{R}^{2d}\\) ma rozkład \\(\\mu\\).\nUdowodnij, że dla każdej funkcji Lipschitza \\(f\\), takiej że \\(\\|f\\|_{\\text{Lip}} \\leq 1\\) oraz\n\\(\\mathbb{E}[f(X)] = 0\\), zachodzi nierówność\n\\[\n    \\mathbb{E}\\left[f(X)e^{t f(X)}\\right] \\leq t \\mathbb{E}\\left[e^{t f(Z)}\\right],\n    \\]\ndla wszystkich \\(t \\geq 0\\). Wywnioskuj, że\n\\[\n    \\mathbb{E}\\left[e^{t f(X)}\\right] \\leq e^{\\frac{t^2}{2}}.\n    \\]Niech \\(\\mu_\\alpha\\) będzie miarą probabilistyczną na \\(\\mathbb{R}^{2d}\\),\nktóra jest rozkładem wektora\n\\[\n    \\left(X, \\alpha X + \\sqrt{1 - \\alpha^2} Y\\right),\n    \\]\nniech \\(\\mu\\) oznacza miarą probabilistyczną daną przez\n\\[\n    \\int_0^1 \\mu_\\alpha \\, \\mathrm{d}\\alpha.\n    \\]\nNiech \\(Z\\) będzie wektorem losowym w \\(\\mathbb{R}^d\\) takim, że wektor \\((X, Z)\\) w \\(\\mathbb{R}^{2d}\\) ma rozkład \\(\\mu\\).\nUdowodnij, że dla każdej funkcji Lipschitza \\(f\\), takiej że \\(\\|f\\|_{\\text{Lip}} \\leq 1\\) oraz\n\\(\\mathbb{E}[f(X)] = 0\\), zachodzi nierówność\n\\[\n    \\mathbb{E}\\left[f(X)e^{t f(X)}\\right] \\leq t \\mathbb{E}\\left[e^{t f(Z)}\\right],\n    \\]\ndla wszystkich \\(t \\geq 0\\). Wywnioskuj, że\n\\[\n    \\mathbb{E}\\left[e^{t f(X)}\\right] \\leq e^{\\frac{t^2}{2}}.\n    \\]Mówimy, że funkcja \\(f : \\mathbb{R}^n \\\\mathbb{R}\\) jest ograniczona wielomianowo,\njeśli istnieją liczby całkowite \\(k = (k_1, \\ldots, k_n)\\) oraz liczba rzeczywista \\(> 0\\) takie, że\n\\[\n|f(x)| \\leq |x_1|^{k_1} \\cdots |x_n|^{k_n}\n   \\]\ndla każdego \\(x = (x_1, \\ldots, x_n)\\) takiego, że \\(\\|x\\| \\geq \\).Udowodnij, że jeśli \\(G\\) jest zmienną losową o rozkładzie normalnym o średniej zero,\ndla każdej ograniczonej wielomianowo różniczkowalnej w sposób ciągły funkcji \\(\\Phi\\) zachodzi:\n\\[\n    \\mathbb{E}[G \\Phi(G)] = \\mathbb{E}\\left[G^2\\right] \\, \\mathbb{E}[\\Phi'(G)].\n    \\]Udowodnij, że jeśli \\(G\\) jest zmienną losową o rozkładzie normalnym o średniej zero,\ndla każdej ograniczonej wielomianowo różniczkowalnej w sposób ciągły funkcji \\(\\Phi\\) zachodzi:\n\\[\n    \\mathbb{E}[G \\Phi(G)] = \\mathbb{E}\\left[G^2\\right] \\, \\mathbb{E}[\\Phi'(G)].\n    \\]Udowodnij, że jeśli \\((G, G_1, G_2, \\ldots, G_n)\\) jest\n\\((n+1)\\)-wymiarowym wektorem losowym o rozkładzie normalnym, \\(\\mathbb{E}[G]=\\mathbb{E}[G_i]=0\\),\ndla każdej ograniczonej wielomianowo różniczkowalnej w sposób ciągły funkcji\n\\[\n    \\Phi : \\mathbb{R}^n \\\\mathbb{R},\n    \\]\nzachodzi\n\\[\n    \\mathbb{E}\\left[G \\Phi(G_1, \\ldots, G_n)\\right] =\n    \\sum_{l \\leq n} \\mathbb{E}[G G_l] \\, \\mathbb{E}\\left[\\frac{\\partial \\Phi}{\\partial x_l}(G_1, \\ldots, G_n)\\right].\n    \\]Udowodnij, że jeśli \\((G, G_1, G_2, \\ldots, G_n)\\) jest\n\\((n+1)\\)-wymiarowym wektorem losowym o rozkładzie normalnym, \\(\\mathbb{E}[G]=\\mathbb{E}[G_i]=0\\),\ndla każdej ograniczonej wielomianowo różniczkowalnej w sposób ciągły funkcji\n\\[\n    \\Phi : \\mathbb{R}^n \\\\mathbb{R},\n    \\]\nzachodzi\n\\[\n    \\mathbb{E}\\left[G \\Phi(G_1, \\ldots, G_n)\\right] =\n    \\sum_{l \\leq n} \\mathbb{E}[G G_l] \\, \\mathbb{E}\\left[\\frac{\\partial \\Phi}{\\partial x_l}(G_1, \\ldots, G_n)\\right].\n    \\]","code":""},{"path":"lista-11-momenty-wielowymiarowe-i-nierówności.html","id":"lista-11-momenty-wielowymiarowe-i-nierówności","chapter":"Lista 11: Momenty wielowymiarowe i nierówności","heading":"Lista 11: Momenty wielowymiarowe i nierówności","text":"Zadania na ćwiczenia: 2025-05-19Lista zadań w formacie PDF","code":""},{"path":"lista-11-momenty-wielowymiarowe-i-nierówności.html","id":"zadania-do-samodzielnego-rozwiązania-10","chapter":"Lista 11: Momenty wielowymiarowe i nierówności","heading":"Zadania do samodzielnego rozwiązania","text":"Znajdź \\(\\mathbb{E}[\\vec{X}]\\) oraz macierz kowariancji \\(Q^{\\vec{X}}\\),\ngdzie \\(\\vec{X}\\) jest wektorem wylosowanym jednostajnie z trójkąta\no wierzchołkach w punktach \\((0,0)\\), \\((0,1)\\) \\((1,0)\\).\no wierzchołkach w punktach \\((1,1)\\), \\((3,2)\\) \\((4,5)\\).\nZnajdź \\(\\mathbb{E}[\\vec{X}]\\) oraz macierz kowariancji \\(Q^{\\vec{X}}\\),\ngdzie \\(\\vec{X}\\) jest wektorem wylosowanym jednostajnie z trójkątao wierzchołkach w punktach \\((0,0)\\), \\((0,1)\\) \\((1,0)\\).o wierzchołkach w punktach \\((1,1)\\), \\((3,2)\\) \\((4,5)\\).\n\\[\\begin{align*}\n) & \\mathbb{E}[X] = (1/3,1/3), \\: Q =[ 1/27, -1/54; -1/54, 1/27] \\\\\nb) & \\mathbb{E}[X] = (8/3, 8/3), \\: Q = [7/27, 17/54; 17/54, 13/27]\n\\end{align*}\\]\nZnajdź funkcję tworzącą momenty dla zmiennej losowe \\(X\\) o rozkładzie\n\\(\\mathrm{Pois}(\\lambda)\\)\n\\(\\mathrm{Geo}(p)\\)\n\\(\\mathcal{U}(,b)\\)\n\\(\\mathcal{N}(\\mu, \\sigma^2)\\)\nZnajdź funkcję tworzącą momenty dla zmiennej losowe \\(X\\) o rozkładzie\\(\\mathrm{Pois}(\\lambda)\\)\\(\\mathrm{Geo}(p)\\)\\(\\mathcal{U}(,b)\\)\\(\\mathcal{N}(\\mu, \\sigma^2)\\)\n\\[\\begin{align*}\n) & M_X(t) = e^{\\lambda(e^t-1)}\\\\\nb) & M_X(t) = pe^{t}/(1-(1-p)e^t)\\\\\nc) & M_X(t) = e^{tb}-e^{ta}/(t(b-)), M_X(0)=1\\\\\nd) & M_X(t) = \\exp\\{ t\\mu t^2\\sigma^2/2\\}\n\\end{align*}\\]\nNiech \\(M_X(\\beta) = \\mathbb{E} \\left[e^{\\beta X}\\right]\\)\nbędzie funkcją tworzącą momenty dla zmiennej losowej \\(X\\).\nUdowodnij, że dla każdego \\(\\beta > 0\\) zachodzi\n\\[\n\\mathbb{P} [ X \\geq 0 ] \\leq M_X(\\beta).\n\\]\n\\[\\begin{equation*}\nM_X(\\beta)=\\mathbb[E][e^{\\beta X}] \\geq \\mathbb{E}[e^{\\beta X}\\mathbf{1}_{X \\geq 0}] \\geq \\mathbb{P}[X\\geq 0]\n\\end{equation*}\\]\nNiech \\(X\\) będzie zmienną losową. Pokaż, że\n\\[\\begin{equation*}\n\\mathbb{E}\\left[X^2 \\right] \\geq \\mathbb{E}[X]^2.\n\\end{equation*}\\]\n\\[\\begin{equation*}\n    0 \\leq \\mathbb{V}ar[X]  = \\mathbb{E}[X^2]-\\mathbb{E}[X]^2.\n\\end{equation*}\\]\nPokaż, że dla ustalonych \\(0 < b \\leq \\) istnieje zmienna losowa \\(X\\) taka, że\n\\(\\mathbb{E}[X^2] = b^2\\) \\(\\mathbb{P}[|X| \\geq ] = b^2 / ^2\\).\nW szczególności w nierówności Czebyszewa zachodzi równość.\n\\(\\mathbb{P}[X =] = b^2/^2\\) \\(\\mathbb{P}[X=0] = 1-b^2/^2\\).\nWłącz komputer. Na podstawie 1000 najwyżej ocenianych filmów z IMDB\nmetodą regresji liniowej znajdź estymator \\(\\hat{Y}\\) zmiennej \\(Y\\)= ocena filmu względem \\(X\\) = czas trwania filmu w minutach.\n\\(\\hat{Y} = 7.656+0.002384\\cdot X\\)\n","code":""},{"path":"lista-11-momenty-wielowymiarowe-i-nierówności.html","id":"zadania-na-ćwiczenia-9","chapter":"Lista 11: Momenty wielowymiarowe i nierówności","heading":"Zadania na ćwiczenia","text":"Niech \\(\\vec{X}\\) będzie wektorem wylosowanym jednostajnie ze zbioru\n\\[\n\\Omega=\\left\\{ (x,y) \\\\mathbb{R}^2 \\: : \\: 13x^2 - 10xy + 13y^2 + 82x - 98y \\leq -129 \\right\\}\n\\]\nZnajdź \\(\\mathbb{E}[\\vec{X}]\\).Niech \\(\\vec{X}\\) będzie wektorem wylosowanym jednostajnie ze zbioru\n\\[\n\\Omega=\\left\\{ (x,y) \\\\mathbb{R}^2 \\: : \\: 13x^2 - 10xy + 13y^2 + 82x - 98y \\leq -129 \\right\\}\n\\]\nZnajdź \\(\\mathbb{E}[\\vec{X}]\\).Niech \\(X\\) \\(Y\\) będą zmiennymi losowymi całkowalnymi z kwadratem. Pokaż, że\n\\[\\begin{equation*}\n\\mathbb{E}[XY] \\leq \\mathbb{E}[X^2]^{1/2}\\mathbb{E}[Y^2]^{1/2}.\n\\end{equation*}\\]Niech \\(X\\) \\(Y\\) będą zmiennymi losowymi całkowalnymi z kwadratem. Pokaż, że\n\\[\\begin{equation*}\n\\mathbb{E}[XY] \\leq \\mathbb{E}[X^2]^{1/2}\\mathbb{E}[Y^2]^{1/2}.\n\\end{equation*}\\]Niech \\(X\\) będzie zmienną losową \\(\\varphi \\colon \\mathbb{R} \\\\mathbb{R}\\) funkcją wypukła taką, że\n\\(\\mathbb{E}[|\\varphi(X)|] <\\infty\\). Pokaż, że\n\\[\\begin{equation*}\n\\varphi(\\mathbb{E}[X]) \\leq \\mathbb{E}[\\varphi(X)].\n\\end{equation*}\\]Niech \\(X\\) będzie zmienną losową \\(\\varphi \\colon \\mathbb{R} \\\\mathbb{R}\\) funkcją wypukła taką, że\n\\(\\mathbb{E}[|\\varphi(X)|] <\\infty\\). Pokaż, że\n\\[\\begin{equation*}\n\\varphi(\\mathbb{E}[X]) \\leq \\mathbb{E}[\\varphi(X)].\n\\end{equation*}\\]Niech \\(X\\) będzie nieujemną zmienną losową całkowalną z kwadratem. Pokaż, że dla \\(\\theta \\(0,1)\\),\n\\[\\begin{equation*}\n\\mathbb{P}[X \\geq \\theta \\mathbb{E}[X]] \\geq (1-\\theta)^2 \\frac{\\mathbb{E}[X]^2}{\\mathbb{E}[X^2]}.\n   \\end{equation*}\\]Niech \\(X\\) będzie nieujemną zmienną losową całkowalną z kwadratem. Pokaż, że dla \\(\\theta \\(0,1)\\),\n\\[\\begin{equation*}\n\\mathbb{P}[X \\geq \\theta \\mathbb{E}[X]] \\geq (1-\\theta)^2 \\frac{\\mathbb{E}[X]^2}{\\mathbb{E}[X^2]}.\n   \\end{equation*}\\]Rzucamy monetą aż momentu otrzymania dwóch orłów z rzędu.\nZakładamy, że prawdopodobieństwo otrzymania orła w pojedynczym rzucie wynosi \\(p \\(0,1)\\). Niech\n\\[\n   \\Omega = \\{ OO, ROO, RROO, OROO, RRROO, ROROO, ORROO, \\ldots \\}.\n   \\]\nRozważmy zmienną losową \\(X(\\omega) = |\\omega|\\) oznaczająca liczbę wykonanych rzutów.\nCelem zadania jest znalezienie \\(\\mathbb{E}[X]\\).\nUzasadnij, że dla pewnej \\(\\beta>0\\),\n\\[\\begin{equation*}\nM_X(\\beta) = \\mathbb{E}\\left[ e^{\\beta X} \\right]\n= \\sum_{\\omega \\\\Omega} e^{\\beta|\\omega|} p^{O(\\omega)}q^{R(\\omega)}<\\infty\n\\end{equation*}\\]\ngdzie \\(R(\\omega)\\) oraz \\(O(\\omega)\\) oznaczają odpowiednio liczbę reszek orłów w ciągu \\(\\omega\\).\nZauważ, że\n\\[\\begin{equation*}\n\\Omega = \\{OO\\} \\cup R\\Omega \\cup \\Omega,\n\\end{equation*}\\]\ngdzie powyższe zbiory są rozłączne. Wywnioskuj\n\\[\\begin{equation*}\nM_X(\\beta) = p^2 e^{2\\beta} +qe^{\\beta} M_X(\\beta) +pqe^{2\\beta}M_X(\\beta).\n\\end{equation*}\\]\nRzucamy monetą aż momentu otrzymania dwóch orłów z rzędu.\nZakładamy, że prawdopodobieństwo otrzymania orła w pojedynczym rzucie wynosi \\(p \\(0,1)\\). Niech\n\\[\n   \\Omega = \\{ OO, ROO, RROO, OROO, RRROO, ROROO, ORROO, \\ldots \\}.\n   \\]\nRozważmy zmienną losową \\(X(\\omega) = |\\omega|\\) oznaczająca liczbę wykonanych rzutów.\nCelem zadania jest znalezienie \\(\\mathbb{E}[X]\\).Uzasadnij, że dla pewnej \\(\\beta>0\\),\n\\[\\begin{equation*}\nM_X(\\beta) = \\mathbb{E}\\left[ e^{\\beta X} \\right]\n= \\sum_{\\omega \\\\Omega} e^{\\beta|\\omega|} p^{O(\\omega)}q^{R(\\omega)}<\\infty\n\\end{equation*}\\]\ngdzie \\(R(\\omega)\\) oraz \\(O(\\omega)\\) oznaczają odpowiednio liczbę reszek orłów w ciągu \\(\\omega\\).Zauważ, że\n\\[\\begin{equation*}\n\\Omega = \\{OO\\} \\cup R\\Omega \\cup \\Omega,\n\\end{equation*}\\]\ngdzie powyższe zbiory są rozłączne. Wywnioskuj\n\\[\\begin{equation*}\nM_X(\\beta) = p^2 e^{2\\beta} +qe^{\\beta} M_X(\\beta) +pqe^{2\\beta}M_X(\\beta).\n\\end{equation*}\\]Znajdź \\(\\mathbb{E}[X]\\).Niech \\(X\\) \\(Y\\) będą dwiema zmiennymi losowymi, dla których \\(\\mathbb{E}[X] = \\mathbb{E}[Y] = 0\\)\noraz \\(\\mathbb{V}ar[X] = \\mathbb{V}ar[Y] = 1\\) oraz \\(\\mathrm{cov}(X,Y)=\\rho\\). Udowodnij, że\n\\[\n   \\mathbb{E}\\left[ \\max(X^2, Y^2) \\right] \\leq 1 + \\sqrt{1 - \\rho^2}.\n   \\]\nUżyj tożsamości\n\\[\n   \\max(X^2, Y^2) = \\frac{1}{2} \\left( X^2 + Y^2 + |X^2 - Y^2| \\right).\n   \\]\nPrzypuśćmy, że \\(X\\) \\(Y\\) są dwiema zmiennymi losowymi o współczynniku korelacji\n\\[\n   \\rho = \\frac{ \\mathrm{Cov}(X,Y)}{\\sqrt{\\mathbb{V}ar[X]\\mathbb{V}ar[Y]}}.\n   \\]\nZweryfikuj następujący dwuwymiarowy analog nierówności Czebyszewa:\\[\n   \\mathbb{P} \\left[ |X - \\mathbb{E}[X]| \\geq \\varepsilon \\sqrt{\\mathbb{V}ar[X]} \\text{ lub }\n   |Y - \\mathbb{E}[Y]| \\geq \\varepsilon \\sqrt{\\mathbb{V}ar[Y]} \\right]\\\\\n   \\leq \\frac{1}{\\varepsilon^2} \\left(1 + \\sqrt{1 - \\rho^2} \\right).\n   \\]\nBez straty ogólności można założyć, że \\(\\mathbb{E}[X] = \\mathbb{E}[Y] = 0\\) oraz\n\\(\\mathbb{V}ar[X] = \\mathbb{V}ar[Y] = 1\\). Wtedy\n\\[\n   \\mathbb{P} \\left[ |X| \\geq \\varepsilon \\text{ lub } |Y| \\geq \\varepsilon \\right]\n   = \\mathbb{P} \\left[ \\max(X^2, Y^2) \\geq \\varepsilon^2 \\right].\n   \\]\nNiech \\(\\varphi\\) oraz \\(\\Phi\\) będą odpowiednio gęstością dystrybuantą jednowymiarowego\nstandardowego rozkładu normalnego.\nUdowodnij, że dla dowolnego \\(x > 0\\) zachodzi nierówność\n\\[\n   \\frac{x}{1 + x^2} \\, \\varphi(x) < 1 - \\Phi(x) < \\frac{\\varphi(x)}{x}.\n   \\]\nOblicz pochodne \\(\\varphi'(x)\\) oraz \\(\\left(x^{-1} \\varphi(x)\\right)'\\).\n","code":""},{"path":"lista-11-momenty-wielowymiarowe-i-nierówności.html","id":"zadania-dodatkowe-8","chapter":"Lista 11: Momenty wielowymiarowe i nierówności","heading":"Zadania dodatkowe","text":"Niech \\(\\xi\\) \\(\\eta\\) będą dowolnymi niezależnymi zmiennymi losowymi, dla których \\(\\mathbb{E} [\\xi] = 0\\). Udowodnij, że\n\\[\n   \\mathbb{E} |\\xi - \\eta| \\geq \\mathbb{E} |\\eta|.\n   \\]Niech \\(\\xi\\) \\(\\eta\\) będą dowolnymi niezależnymi zmiennymi losowymi, dla których \\(\\mathbb{E} [\\xi] = 0\\). Udowodnij, że\n\\[\n   \\mathbb{E} |\\xi - \\eta| \\geq \\mathbb{E} |\\eta|.\n   \\]Niech \\(\\xi_1\\) \\(\\xi_2\\) będą dowolnymi dwoma zmiennymi losowymi takimi,\nże rozkład wektora losowego \\((\\xi_1, \\xi_2)\\) pokrywa się z rozkładem \\((\\xi_2, \\xi_1)\\).\nUdowodnij, że jeśli \\(f\\) oraz \\(g\\) są dowolnymi nieujemnymi niemalejącymi funkcjami, \n\\[\n   \\mathbb{E} \\left[ f(\\xi_1) g(\\xi_1) \\right] \\geq \\mathbb{E} \\left[ f(\\xi_1) g(\\xi_2) \\right].\n   \\]Niech \\(\\xi_1\\) \\(\\xi_2\\) będą dowolnymi dwoma zmiennymi losowymi takimi,\nże rozkład wektora losowego \\((\\xi_1, \\xi_2)\\) pokrywa się z rozkładem \\((\\xi_2, \\xi_1)\\).\nUdowodnij, że jeśli \\(f\\) oraz \\(g\\) są dowolnymi nieujemnymi niemalejącymi funkcjami, \n\\[\n   \\mathbb{E} \\left[ f(\\xi_1) g(\\xi_1) \\right] \\geq \\mathbb{E} \\left[ f(\\xi_1) g(\\xi_2) \\right].\n   \\]Niech \\(\\vec{X}\\) będzie wektorem o wielowymiarowym standardowym rozkładzie normalnym.\nUdowodnij, że dla każdej funkcji Lipschitza \\(f \\colon \\mathbb{R}^d \\\\mathbb{R}\\)\ntakiej, że \\(\\|f\\|_{\\text{Lip}} \\leq 1\\), zachodzi nierówność\n\\[\n   \\mathbb{P}\\left[ f(X) - \\mathbb{E}[f(X)] \\geq \\lambda \\right] \\leq e^{-\\lambda^2 / 2}, \\quad \\text{dla każdego } \\lambda \\geq 0.\n   \\]Niech \\(\\vec{X}\\) będzie wektorem o wielowymiarowym standardowym rozkładzie normalnym.\nUdowodnij, że dla każdej funkcji Lipschitza \\(f \\colon \\mathbb{R}^d \\\\mathbb{R}\\)\ntakiej, że \\(\\|f\\|_{\\text{Lip}} \\leq 1\\), zachodzi nierówność\n\\[\n   \\mathbb{P}\\left[ f(X) - \\mathbb{E}[f(X)] \\geq \\lambda \\right] \\leq e^{-\\lambda^2 / 2}, \\quad \\text{dla każdego } \\lambda \\geq 0.\n   \\]Niech \\(\\vec{X}\\) będzie \\(m\\)-wymiarowym wektorm losowym, \\(Q^{\\vec{X}}\\) — jego macierzą kowariancji.Niech \\(\\vec{X}\\) będzie \\(m\\)-wymiarowym wektorm losowym, \\(Q^{\\vec{X}}\\) — jego macierzą kowariancji.Udowodnij, że jeśli \\(\\vec{X}\\) jest scentrowany (tzn. \\(\\mathbb{E}[\\vec{X}] = 0\\)), \n\\[\n    \\mathbb{P}\\left[ \\vec{X} \\\\mathrm{Im}\\,Q^{\\vec{X}}\\right] = 1\n    \\]\n(gdzie \\(\\mathrm{Im}\\, Q^{\\vec{X}}\\) oznacza obraz macierzy \\(Q^{\\vec{X}}\\)).Udowodnij, że jeśli \\(\\vec{X}\\) jest scentrowany (tzn. \\(\\mathbb{E}[\\vec{X}] = 0\\)), \n\\[\n    \\mathbb{P}\\left[ \\vec{X} \\\\mathrm{Im}\\,Q^{\\vec{X}}\\right] = 1\n    \\]\n(gdzie \\(\\mathrm{Im}\\, Q^{\\vec{X}}\\) oznacza obraz macierzy \\(Q^{\\vec{X}}\\)).Wywnioskuj, że jeśli macierz kowariancji zmiennej losowej nie jest odwracalna, rozkład tej zmiennej\nnie może mieć gęstości.Wywnioskuj, że jeśli macierz kowariancji zmiennej losowej nie jest odwracalna, rozkład tej zmiennej\nnie może mieć gęstości.","code":""},{"path":"lista-12-zbieżność.html","id":"lista-12-zbieżność","chapter":"Lista 12: Zbieżność","heading":"Lista 12: Zbieżność","text":"Zadania na ćwiczenia: 2025-05-26Lista zadań w formacie PDF","code":""},{"path":"lista-12-zbieżność.html","id":"zadania-do-samodzielnego-rozwiązania-11","chapter":"Lista 12: Zbieżność","heading":"Zadania do samodzielnego rozwiązania","text":"Niech \\(\\Omega = [0,1]\\), \\(\\mathcal{F}=\\mathcal{B}([0,1])\\) \n\\[\\mathbb{P} = \\frac 13\\delta_0 + \\frac 13\\delta_{1/3} + \\frac 13 \\delta_1.\\]\nRozważmy \\(X(\\omega) = 0\\), \\(Y(\\omega) = \\mathbf{1}_{\\{1\\}}(\\omega)\\) oraz \\(Z(\\omega) = \\mathbf{1}_{[1/2,1)}(\\omega)\\).\nCzy \\(X=Y\\) p. n?\nCzy \\(X=Z\\) p. n?\nNiech \\(\\Omega = [0,1]\\), \\(\\mathcal{F}=\\mathcal{B}([0,1])\\) \n\\[\\mathbb{P} = \\frac 13\\delta_0 + \\frac 13\\delta_{1/3} + \\frac 13 \\delta_1.\\]\nRozważmy \\(X(\\omega) = 0\\), \\(Y(\\omega) = \\mathbf{1}_{\\{1\\}}(\\omega)\\) oraz \\(Z(\\omega) = \\mathbf{1}_{[1/2,1)}(\\omega)\\).Czy \\(X=Y\\) p. n?Czy \\(X=Z\\) p. n?\nnie, b. takNech \\(\\Omega =\\mathbb{R}\\) \\(\\mathcal{F}=\\mathcal{B}(\\mathbb{R})\\).\nRozważmy \\(X_n(\\omega) = \\cos(\\omega)^n\\). Zbadaj zbieżność prawie na\npewno ciągu \\(\\{X_n\\}_{n \\\\mathbb{N}}\\)\njeżeli\\(\\mathbb{P}\\) jest jednowymiarową miarą Lebesgue’s obciętą odcinka \\([0,1]\\).\\(\\mathbb{P}\\) jest rozkładem normalnym o średniej zero wariancji \\(1\\).\\(\\mathbb{P} = \\sum_{k=1}^{\\infty}2^{-k}\\delta_{k\\pi}\\).\\(\\mathbb{P} = \\sum_{k=1}^{\\infty}2^{-k}\\delta_{2k\\pi}\\).\nb. ciąg jest zbieżny zera p.n. c. ciąg nie jest zbieżny p.n. d. ciąg jest zbieżny \\(1\\) p.n.Rozważmy zmienne losowe \\(X\\), \\(Y\\) \\(Z\\). Pokaż, że jeżeli \\(X=Y\\) p.n. oraz \\(Y=Z\\) p.n, \\(X=Z\\) p. n.\nNiech\n\\[\\begin{equation*}\n    = \\{ \\omega \\: : \\: X(\\omega) = Y(\\omega)\\}\n\\end{equation*}\\]\noraz\n\\[\\begin{equation*}\n    B = \\{ \\omega \\: : \\: Y(\\omega) = Z(\\omega)\\}.\n\\end{equation*}\\]\nZ założenia \\(\\mathbb{P}[] = \\mathbb{P}[B]=1\\). Dla \\(\\omega \\\\cap B\\),\n\\[\\begin{equation*}\n    X(\\omega) = Y(\\omega) = Z(\\omega)\n\\end{equation*}\\]\nprzy czym \\(\\mathbb{P}[\\cap B]=1\\).\nNiech \\(\\{X_n\\}_{n\\\\mathbb{N}}\\) będzie ciągiem niezależnych zmiennych losowych taki, że \\(X_{2n}\\) ma rozkład\n\\(\\mathrm{Exp}(1)\\), \\(X_{2n+1}\\) ma rozkład \\(\\mathrm{Pois}(3)\\). Znajdź granicę według prawdopodobieństwa\n\\[\\begin{equation*}\n\\frac{X_1+X_2+ \\ldots + X_n}{n}.\n\\end{equation*}\\]\n2\nNiech \\(X_n \\^\\mathbb{P} X\\) oraz \\(Y_n \\^\\mathbb{P} Y\\) załóżmy, że \\(X=Y\\) p.n.\nPokaż, że \\(X_n-Y_n \\^\\mathbb{P} 0\\).\nMamy\n\\[\\begin{equation*}\n\\mathbb{P}[|X_n-Y_n|>\\epsilon] \\leq \\mathbb{P}[|X_n-X|>\\epsilon/2] + \\mathbb{P}[|Y_n-Y| >\\epsilon/2].\n\\end{equation*}\\]\n","code":""},{"path":"lista-12-zbieżność.html","id":"zadania-na-ćwiczenia-10","chapter":"Lista 12: Zbieżność","heading":"Zadania na ćwiczenia","text":"Pokaż, że jeżeli \\(X_n \\^\\mathbb{P}X\\) oraz \\(X_n \\^{\\mathbb{P}} Y\\), \\(X = Y\\) p.n.Pokaż, że jeżeli \\(X_n \\^\\mathbb{P}X\\) oraz \\(X_n \\^{\\mathbb{P}} Y\\), \\(X = Y\\) p.n.Pokaż, że jeżeli \\(\\vec{X}_n \\^\\mathbb{P} \\vec{X}\\) \\(\\vec{Y}_n \\^{\\mathbb{P}} \\vec{Y}\\),\n\\(\\vec{X}_n+b\\vec{Y}_n \\\\vec{X}+b\\vec{Y}\\) dla dowolnych rzeczywistych \\(, b\\).Pokaż, że jeżeli \\(\\vec{X}_n \\^\\mathbb{P} \\vec{X}\\) \\(\\vec{Y}_n \\^{\\mathbb{P}} \\vec{Y}\\),\n\\(\\vec{X}_n+b\\vec{Y}_n \\\\vec{X}+b\\vec{Y}\\) dla dowolnych rzeczywistych \\(, b\\).Załóżmy, że \\(\\sigma\\)-ciała \\(\\{\\mathcal{F}_n\\}_{n\\\\mathbb{N}}\\) są niezależne. Pokaż, że jeżeli \\(X\\)\njest zmienną losową \\(\\mathcal{F}_\\infty\\)-mierzalną, \\(\\mathbb{P}[X=c]=1\\) dla pewnej stałej \\(c\\\\mathbb{R}\\).Załóżmy, że \\(\\sigma\\)-ciała \\(\\{\\mathcal{F}_n\\}_{n\\\\mathbb{N}}\\) są niezależne. Pokaż, że jeżeli \\(X\\)\njest zmienną losową \\(\\mathcal{F}_\\infty\\)-mierzalną, \\(\\mathbb{P}[X=c]=1\\) dla pewnej stałej \\(c\\\\mathbb{R}\\).Dany jest ciąg \\(\\{X_n\\}\\) niezależnych zmiennych losowych,\nprzy czym dla \\(n \\geq 1\\) zmienna \\(X_n\\) ma rozkład zadany przez\n\\(\\mathbb{P}[X_n = n] = \\frac{1}{n} = 1 - \\mathbb{P}[X_n = 0]\\). Czy ten ciąg jest zbieżny p.w.?\nCzy jest zbieżny według prawdopodobieństwa? Czy jest zbieżny w \\(L^p\\)?Dany jest ciąg \\(\\{X_n\\}\\) niezależnych zmiennych losowych,\nprzy czym dla \\(n \\geq 1\\) zmienna \\(X_n\\) ma rozkład zadany przez\n\\(\\mathbb{P}[X_n = n] = \\frac{1}{n} = 1 - \\mathbb{P}[X_n = 0]\\). Czy ten ciąg jest zbieżny p.w.?\nCzy jest zbieżny według prawdopodobieństwa? Czy jest zbieżny w \\(L^p\\)?Niech \\(X_1, X_2, \\ldots\\) będą niezależnymi zmiennymi losowymi,\nktóre mają rozkład jednostajny na przedziale \\([0, 1]\\). Udowodnij, że\n\\[\n\\mathbb{E} \\left[ \\sum_{k=1}^{n} X_k^2 \\left( \\sum_{k=1}^{n} X_k \\right)^{-1} \\right]\n\\\\frac{2}{3}.\n\\]Niech \\(X_1, X_2, \\ldots\\) będą niezależnymi zmiennymi losowymi,\nktóre mają rozkład jednostajny na przedziale \\([0, 1]\\). Udowodnij, że\n\\[\n\\mathbb{E} \\left[ \\sum_{k=1}^{n} X_k^2 \\left( \\sum_{k=1}^{n} X_k \\right)^{-1} \\right]\n\\\\frac{2}{3}.\n\\]Załóżmy, że \\(\\vec{X}_n \\^\\mathbb{P} \\vec{X}\\).\nPokaż, że dla każdego \\(\\epsilon>0\\) istnieje \\(T>0\\) takie, że\n\\[\\begin{equation*}\n\\sup_{n \\\\mathbb{N}} \\mathbb{P}\\left[\\left\\| \\vec{X}_n\\right\\|>T\\right] <\\epsilon.\n\\end{equation*}\\]\nPokaż, że jeżeli \\(f \\colon \\mathbb{R}^d \\\\mathbb{R}^k\\) jest funkcją ciągłą,\n\\(f(\\vec{X}_n) \\^\\mathbb{P} f(\\vec{X})\\).\nZałóżmy, że \\(\\vec{X}_n \\^\\mathbb{P} \\vec{X}\\).Pokaż, że dla każdego \\(\\epsilon>0\\) istnieje \\(T>0\\) takie, że\n\\[\\begin{equation*}\n\\sup_{n \\\\mathbb{N}} \\mathbb{P}\\left[\\left\\| \\vec{X}_n\\right\\|>T\\right] <\\epsilon.\n\\end{equation*}\\]Pokaż, że jeżeli \\(f \\colon \\mathbb{R}^d \\\\mathbb{R}^k\\) jest funkcją ciągłą,\n\\(f(\\vec{X}_n) \\^\\mathbb{P} f(\\vec{X})\\).Niech \\(\\{X_n\\}_n\\) będzie ciągiem niezależnych zmiennych losowych takich, że \\(X_n\\) ma rozkład Poissona z parametrem\n\\(n\\). Pokaż, że\n\\[\\begin{equation*}\n\\frac{X_n}{n} \\1 \\quad p.n.\n\\end{equation*}\\]Niech \\(\\{X_n\\}_n\\) będzie ciągiem niezależnych zmiennych losowych takich, że \\(X_n\\) ma rozkład Poissona z parametrem\n\\(n\\). Pokaż, że\n\\[\\begin{equation*}\n\\frac{X_n}{n} \\1 \\quad p.n.\n\\end{equation*}\\]Rozwazmy \\(\\Omega=[0,1]\\) z jednowymiarową miarą Lebesgue’.\nNiech \\(X_n(\\omega)= \\mathbf{1}_A([n\\omega])\\), gdzie \\(\\) zbiór kwadratów liczb naturalnych.\nPokaż, że zbiega według miary ale nie zbiega p.n. wskaż podciąg zbieżny p.n.Rozwazmy \\(\\Omega=[0,1]\\) z jednowymiarową miarą Lebesgue’.\nNiech \\(X_n(\\omega)= \\mathbf{1}_A([n\\omega])\\), gdzie \\(\\) zbiór kwadratów liczb naturalnych.\nPokaż, że zbiega według miary ale nie zbiega p.n. wskaż podciąg zbieżny p.n.","code":""},{"path":"lista-12-zbieżność.html","id":"zadania-dodatkowe-9","chapter":"Lista 12: Zbieżność","heading":"Zadania dodatkowe","text":"Niech \\((X_n)_{n \\geq 1}\\) będzie ciągiem zmiennych losowych.\nNiech \\(S_n=X_1+\\ldots + X_n\\). Udowodnij, że:Jeżeli \\(X_n \\0\\) p. n, \\(S_n/n \\0\\) p.n.Ogólnie, \\(X_n \\^\\mathbb{P} 0\\) nie implikuje \\(S_n/n \\^{\\mathbb{P}} 0\\)\\(S_n/n \\0\\) p.n. wtedy tylko wtedy, gdy \\(S_n/n \\^\\mathbb{P} 0\\) oraz \\(S_{2n}/2^n \\0\\) p.n.Niech \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) będzie przestrzenią probabilistyczną,\nna której \\(X_n \\^\\mathbb{P} X\\).\nUdowodnij, że jeśli \\(\\Omega\\) jest przeliczalna, \\(X_n \\X\\) p.n.Niech \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) będzie przestrzenią probabilistyczną,\nna której \\(X_n \\^\\mathbb{P} X\\).\nUdowodnij, że jeśli \\(\\Omega\\) jest przeliczalna, \\(X_n \\X\\) p.n.Niech \\((X_n)_{n \\geq 1}\\) będzie ciągiem niezależnych zmiennych losowych,\ndla którego \\(X_n \\^\\mathbb{P} X\\), gdzie \\(X\\) jest pewną zmienną losową.\nUdowodnij, że \\(X\\) musi być zmienną losową zdegenerowaną.Niech \\((X_n)_{n \\geq 1}\\) będzie ciągiem niezależnych zmiennych losowych,\ndla którego \\(X_n \\^\\mathbb{P} X\\), gdzie \\(X\\) jest pewną zmienną losową.\nUdowodnij, że \\(X\\) musi być zmienną losową zdegenerowaną.","code":""},{"path":"lista-13-słaba-zbieżność.html","id":"lista-13-słaba-zbieżność","chapter":"Lista 13: Słaba zbieżność","heading":"Lista 13: Słaba zbieżność","text":"Zadania na ćwiczenia: 2025-06-02Lista zadań w formacie PDF","code":""},{"path":"lista-13-słaba-zbieżność.html","id":"zadania-do-samodzielnego-rozwiązania-12","chapter":"Lista 13: Słaba zbieżność","heading":"Zadania do samodzielnego rozwiązania","text":"Niech \\(\\{X_n\\}\\) będzie ciągiem zmiennych losowych takim, że\n\\(X_n\\) ma rozkład \\(\\text{Exp}(\\lambda_n)\\).\nPokaż, że jeżeli \\(\\lambda_n \\\\lambda >0\\),\n\\(X_n \\Rightarrow\\text{Exp}(\\lambda)\\).\n\nOdpowiedź\n\n\n\nWystarczy zbadać zbieżność dystrybuant w punktach ciągłości dystrybuanty granicznej.\nDla \\(t \\geq 0\\) mamy\n\\[\\begin{equation*}\n\\mathbb{P}[X_n\\leq t] = 1 - e^{-\\lambda_nt} \\1-e^{-\\lambda t}.  \n\\end{equation*}\\]\n\n\nWystarczy zbadać zbieżność dystrybuant w punktach ciągłości dystrybuanty granicznej.\nDla \\(t \\geq 0\\) mamy\n\\[\\begin{equation*}\n\\mathbb{P}[X_n\\leq t] = 1 - e^{-\\lambda_nt} \\1-e^{-\\lambda t}.  \n\\end{equation*}\\]\nCzy zmienne losowe posiadające gęstość mogą słabo zbiegać rozkładu dyskretnego?\nCzy zmienne losowe o rozkładach dyskretnych mogą słabo zbiegać rozkładu posiadającego gęstość?\n\nOdpowiedź\n\n\n\n\\(\\frac{1}{n} \\sum_{k=1}^n \\delta_{k/n} \\Rightarrow \\lambda_{1|[0,1]}\\) oraz\n\\(\\mathcal{N}(0,1/n) \\Rightarrow \\delta_0\\).\n\n\n\\(\\frac{1}{n} \\sum_{k=1}^n \\delta_{k/n} \\Rightarrow \\lambda_{1|[0,1]}\\) oraz\n\\(\\mathcal{N}(0,1/n) \\Rightarrow \\delta_0\\).\nNiech \\(\\{X_n\\}_{n=1}^{\\infty}\\) będzie ciągiem niezależnych zmiennych losowych o\njednakowym rozkładzie jednostajnym \\(U[0,1]\\).\nNiech \\(Y_n = \\min_{1 \\le \\le n} X_i\\).\nPokaż, że \\(Y_n\\) zbiega słabo rozkładu wykładniczego \\(\\text{Exp}(1)\\).\n\nOdpowiedź\n\n\n\nMamy\n\\[\\begin{equation*}\n\\mathbb{P}[Y_n>t] = (1-t/n)^{n} \\e^{-t}\n\\end{equation*}\\]\n\n\nMamy\n\\[\\begin{equation*}\n\\mathbb{P}[Y_n>t] = (1-t/n)^{n} \\e^{-t}\n\\end{equation*}\\]\nNiech \\(F \\subseteq \\mathbb{R}^d\\).\nPokaż, że dla każdych \\(x,y \\\\mathbb{R}^d\\)\n\\[ |\\mathrm{dist}(x,F) - \\mathrm{dist}(y,F)| \\leq \\|x-y\\|.\\]\nWywnioskuj, że dla \\(\\epsilon>0\\) funkcja \\(f(x) = (1-\\mathrm{dist}(x,F)/\\epsilon)_+\\)\njest jednostajnie ciągła.\n\nOdpowiedź\n\n\n\nNiech \\(f\\) będzie dowolnym elementem \\(F\\). Z nierówności trójkąta\n\\[\\begin{equation*}\n\\| x-f\\| + \\|x-y\\|\\leq \\|y-f\\|\n\\end{equation*}\\]\nbiorąc kres dolny po \\(f \\F\\),\n\\[\\begin{equation*}\n\\mathrm{dist}(x,F) + \\|x-y\\|\\leq \\mathrm{dist}(y,F).\n\\end{equation*}\\]\nZamieniając \\(x\\) \\(y\\) miejscami stosując ten sam argument otrzymujemy pierwszą tezę.\nDruga teza wynika teraz z nierówności \\(|a_+ -b_+|\\leq |-b|\\) dla \\(,b \\\\mathbb{R}\\).\n\n\nNiech \\(f\\) będzie dowolnym elementem \\(F\\). Z nierówności trójkąta\n\\[\\begin{equation*}\n\\| x-f\\| + \\|x-y\\|\\leq \\|y-f\\|\n\\end{equation*}\\]\nbiorąc kres dolny po \\(f \\F\\),\n\\[\\begin{equation*}\n\\mathrm{dist}(x,F) + \\|x-y\\|\\leq \\mathrm{dist}(y,F).\n\\end{equation*}\\]\nZamieniając \\(x\\) \\(y\\) miejscami stosując ten sam argument otrzymujemy pierwszą tezę.\nDruga teza wynika teraz z nierówności \\(|a_+ -b_+|\\leq |-b|\\) dla \\(,b \\\\mathbb{R}\\).\nNiech \\(n \\\\mathbb{N}\\). Z liczb \\(\\{1, \\ldots , n\\}\\) losujemy trzy liczby. Niech \\(X_n\\) będzie ich\nmedianą (środkową wartością). Pokaż, że \\(X_n/n\\) przy \\(n \\\\infty\\) zbiega\nwedług rozkładu rozkładu o gęstości \\(6x(1-x) \\mathbf{1}_{[0,1]}(x)\\).\n\nOdpowiedź\n\n\n\nMamy \\[\\mathbb{P}[X_n=k] = 6(k-1)(n-k)/(n(n-1)(n-2)).\\]\nDla każdej \\(f\\C_b(\\mathbb{R})\\) mamy\n\\[\\begin{align*}\n\\mathbb{E}[f(X_n)] & = \\sum_{k} f(k/n) 6 (k-1)(n-k)/(n(n-1)(n-2)) \\\\\n& \\\\int_0^1 f(x) 6 x(1-x) \\mathrm{d}x\n\\end{align*}\\]\n\n\nMamy \\[\\mathbb{P}[X_n=k] = 6(k-1)(n-k)/(n(n-1)(n-2)).\\]\nDla każdej \\(f\\C_b(\\mathbb{R})\\) mamy\n\\[\\begin{align*}\n\\mathbb{E}[f(X_n)] & = \\sum_{k} f(k/n) 6 (k-1)(n-k)/(n(n-1)(n-2)) \\\\\n& \\\\int_0^1 f(x) 6 x(1-x) \\mathrm{d}x\n\\end{align*}\\]\n","code":""},{"path":"lista-13-słaba-zbieżność.html","id":"zadania-na-ćwiczenia-11","chapter":"Lista 13: Słaba zbieżność","heading":"Zadania na ćwiczenia","text":"Wykaż, że dodatnie zmienne losowe \\(\\{X_n\\}_{n=1}^{\\infty}\\) zbiegają słabo \nrozkładu \\(\\mathcal{U}[0,1]\\) wtedy tylko wtedy, gdy zmienne\nlosowe \\(Y_n = -2 \\log X_n\\) zbiegają słabo rozkładu wykładniczego \\(\\text{Exp}(1/2)\\).Wykaż, że dodatnie zmienne losowe \\(\\{X_n\\}_{n=1}^{\\infty}\\) zbiegają słabo \nrozkładu \\(\\mathcal{U}[0,1]\\) wtedy tylko wtedy, gdy zmienne\nlosowe \\(Y_n = -2 \\log X_n\\) zbiegają słabo rozkładu wykładniczego \\(\\text{Exp}(1/2)\\).Pokaż, że jeśli \\(X,\\{X_n\\}_{n=1}^{\\infty}\\) są zmiennymi losowymi\noraz \\(X_n \\^\\mathbb{P} X\\), \\(X_n \\Rightarrow X\\).Pokaż, że jeśli \\(X,\\{X_n\\}_{n=1}^{\\infty}\\) są zmiennymi losowymi\noraz \\(X_n \\^\\mathbb{P} X\\), \\(X_n \\Rightarrow X\\).Pokaż, że jeśli \\(X,\\{X_n\\}_{n=1}^{\\infty}\\) są zmiennymi losowymi oraz\n\\(X_n \\Rightarrow c \\\\mathbb{R}\\), \\(X_n \\^\\mathbb{P} c\\).Pokaż, że jeśli \\(X,\\{X_n\\}_{n=1}^{\\infty}\\) są zmiennymi losowymi oraz\n\\(X_n \\Rightarrow c \\\\mathbb{R}\\), \\(X_n \\^\\mathbb{P} c\\).Niech \\(X\\), \\(\\{X_n\\}_{n\\\\mathbb{N}}\\), \\(Y\\), \\(\\{Y_n\\}_{n \\\\mathbb{N}}\\) będą zmiennymi losowymi takimi, że\n\\[\\begin{equation*}\nX_n \\Rightarrow X \\quad \\mbox{oraz} \\quad Y_n \\Rightarrow Y.\n\\tag{25.1}\n\\end{equation*}\\]\nPokaż, że jeżeli \\(X_n\\) \\(Y_n\\) są niezależne oraz \\(X\\) \\(Y\\) są niezależne, \nma miejsce słaba zbieżność dwuwymiarowych wektorów losowych\n\\[\\begin{equation*}\n(X_n, Y_n) \\Rightarrow (X,Y).\n\\end{equation*}\\]\nPokaż, że jeżeli \\((X_n,Y_n) \\Rightarrow (X,Y)\\), \n\\[\\begin{equation*}\nX_n + Y_n \\Rightarrow X+Y.\n\\end{equation*}\\]\nPodaj przykład zmiennych losowych dla których (25.1) ale nie jest prawdą, że \\(X_n+Y_n \\Rightarrow X+Y\\).\nNiech \\(X\\), \\(\\{X_n\\}_{n\\\\mathbb{N}}\\), \\(Y\\), \\(\\{Y_n\\}_{n \\\\mathbb{N}}\\) będą zmiennymi losowymi takimi, że\n\\[\\begin{equation*}\nX_n \\Rightarrow X \\quad \\mbox{oraz} \\quad Y_n \\Rightarrow Y.\n\\tag{25.1}\n\\end{equation*}\\]Pokaż, że jeżeli \\(X_n\\) \\(Y_n\\) są niezależne oraz \\(X\\) \\(Y\\) są niezależne, \nma miejsce słaba zbieżność dwuwymiarowych wektorów losowych\n\\[\\begin{equation*}\n(X_n, Y_n) \\Rightarrow (X,Y).\n\\end{equation*}\\]Pokaż, że jeżeli \\(X_n\\) \\(Y_n\\) są niezależne oraz \\(X\\) \\(Y\\) są niezależne, \nma miejsce słaba zbieżność dwuwymiarowych wektorów losowych\n\\[\\begin{equation*}\n(X_n, Y_n) \\Rightarrow (X,Y).\n\\end{equation*}\\]Pokaż, że jeżeli \\((X_n,Y_n) \\Rightarrow (X,Y)\\), \n\\[\\begin{equation*}\nX_n + Y_n \\Rightarrow X+Y.\n\\end{equation*}\\]Pokaż, że jeżeli \\((X_n,Y_n) \\Rightarrow (X,Y)\\), \n\\[\\begin{equation*}\nX_n + Y_n \\Rightarrow X+Y.\n\\end{equation*}\\]Podaj przykład zmiennych losowych dla których (25.1) ale nie jest prawdą, że \\(X_n+Y_n \\Rightarrow X+Y\\).Podaj przykład zmiennych losowych dla których (25.1) ale nie jest prawdą, że \\(X_n+Y_n \\Rightarrow X+Y\\).Pokaż, że jeśli \\(X_n \\Rightarrow X\\) oraz \\(Y_n \\Rightarrow c \\\\mathbb{R}\\), \n\\(X_n + Y_n \\Rightarrow X + c\\);Pokaż, że jeśli \\(X_n \\Rightarrow X\\) oraz \\(Y_n \\Rightarrow c \\\\mathbb{R}\\), \n\\(X_n + Y_n \\Rightarrow X + c\\);Niech \\(\\{X_n\\}_{n=1}^{\\infty}\\) będzie ciągiem niezależnych zmiennych losowych o\njednakowym standardowym rozkładzie normalnym \\(\\mathcal{N}(0,1)\\).\nNiech \\(Y_n = n \\min_{1 \\le \\le n} |X_i|\\).\nPokaż, że \\(Y_n\\) zbiega słabo rozkładu wykładniczego. Z jakim parametrem?Niech \\(\\{X_n\\}_{n=1}^{\\infty}\\) będzie ciągiem niezależnych zmiennych losowych o\njednakowym standardowym rozkładzie normalnym \\(\\mathcal{N}(0,1)\\).\nNiech \\(Y_n = n \\min_{1 \\le \\le n} |X_i|\\).\nPokaż, że \\(Y_n\\) zbiega słabo rozkładu wykładniczego. Z jakim parametrem?Niech dla \\(n\\\\mathbb{N}\\) \\(X_n\\) będzie liczbą punktów stałych jednostajnie wylosowanej\npermutacji liczb \\(\\{1, 2, \\ldots , n\\}\\). Znajdź granicę według rozkładu ciągu zmiennych\n\\(\\{X_n\\}_{n \\\\mathbb{N}}\\).Niech dla \\(n\\\\mathbb{N}\\) \\(X_n\\) będzie liczbą punktów stałych jednostajnie wylosowanej\npermutacji liczb \\(\\{1, 2, \\ldots , n\\}\\). Znajdź granicę według rozkładu ciągu zmiennych\n\\(\\{X_n\\}_{n \\\\mathbb{N}}\\).Niech \\(\\mu_\\alpha = \\mathcal{N}(m_\\alpha, \\sigma^2_\\alpha)\\).\nUdowodnić, że rodzina \\(\\{\\mu_\\alpha : \\alpha \\\\Lambda\\}\\) jest ciasna wtedy tylko wtedy,\ngdy istnieje takie \\(K > 0\\), że dla wszystkich \\(\\alpha \\\\Lambda\\) jest\n\\[\n|m_\\alpha| \\leq K, \\quad \\sigma^2_\\alpha \\leq K.\n\\]\n\nWskazówka\n\n\n\nPokaż, że rodzina jest ciasna wtedy tylko wtedy, gdy\n\\[\\begin{equation*}\n\\lim_{t \\\\infty} \\sup_{\\alpha \\\\Lambda} \\frac{t -m_{\\alpha}}{ \\sigma_\\alpha} = \\infty.\n\\end{equation*}\\]\n\nNiech \\(\\mu_\\alpha = \\mathcal{N}(m_\\alpha, \\sigma^2_\\alpha)\\).\nUdowodnić, że rodzina \\(\\{\\mu_\\alpha : \\alpha \\\\Lambda\\}\\) jest ciasna wtedy tylko wtedy,\ngdy istnieje takie \\(K > 0\\), że dla wszystkich \\(\\alpha \\\\Lambda\\) jest\n\\[\n|m_\\alpha| \\leq K, \\quad \\sigma^2_\\alpha \\leq K.\n\\]\n\nWskazówka\n\nPokaż, że rodzina jest ciasna wtedy tylko wtedy, gdy\n\\[\\begin{equation*}\n\\lim_{t \\\\infty} \\sup_{\\alpha \\\\Lambda} \\frac{t -m_{\\alpha}}{ \\sigma_\\alpha} = \\infty.\n\\end{equation*}\\]\n","code":""},{"path":"lista-13-słaba-zbieżność.html","id":"zadania-dodatkowe-10","chapter":"Lista 13: Słaba zbieżność","heading":"Zadania dodatkowe","text":"Niech \\(c_{j,n}\\) dla \\(j\\leq n\\) będzie kolekcją liczb rzeczywistych. Pokaz, że jeśli\n\\[\n\\max_{1 \\leq j \\leq n} |c_{j,n}| \\0, \\quad \\lim_{n\\\\infty} \\sum_{j=1}^{n} c_{j,n} = \\lambda,\n\\quad \\text{oraz} \\quad \\sup_n \\sum_{j=1}^{n} |c_{j,n}| < \\infty,\n\\]\n\n\\[\n\\lim_{n \\\\infty}\\prod_{j=1}^{n} (1 + c_{j,n}) = e^{\\lambda}.\n\\]Niech \\(c_{j,n}\\) dla \\(j\\leq n\\) będzie kolekcją liczb rzeczywistych. Pokaz, że jeśli\n\\[\n\\max_{1 \\leq j \\leq n} |c_{j,n}| \\0, \\quad \\lim_{n\\\\infty} \\sum_{j=1}^{n} c_{j,n} = \\lambda,\n\\quad \\text{oraz} \\quad \\sup_n \\sum_{j=1}^{n} |c_{j,n}| < \\infty,\n\\]\n\n\\[\n\\lim_{n \\\\infty}\\prod_{j=1}^{n} (1 + c_{j,n}) = e^{\\lambda}.\n\\]Niech \\(\\{X_n\\}_{n \\\\mathbb{N}}\\) będą niezależnymi zmiennymi losowymi o rozkładzie jednostajnym na zbiorze \\(\\{1, 2, \\ldots, N\\}\\). Oznaczmy przez \\(T_N = \\min \\{n : X_n = X_m \\text{ dla pewnego } m < n\\}\\). Oblicz granicę według rozkładu ciągu \\(T_N/\\sqrt{N}\\), gdy \\(N \\\\infty\\). Wykorzystaj otrzymany wynik rozwiązania problemu urodzin: oszacuj prawdopodobieństwo, że w gronie 23 osób są dwie mające urodziny tego samego dnia.Niech \\(\\{X_n\\}_{n \\\\mathbb{N}}\\) będą niezależnymi zmiennymi losowymi o rozkładzie jednostajnym na zbiorze \\(\\{1, 2, \\ldots, N\\}\\). Oznaczmy przez \\(T_N = \\min \\{n : X_n = X_m \\text{ dla pewnego } m < n\\}\\). Oblicz granicę według rozkładu ciągu \\(T_N/\\sqrt{N}\\), gdy \\(N \\\\infty\\). Wykorzystaj otrzymany wynik rozwiązania problemu urodzin: oszacuj prawdopodobieństwo, że w gronie 23 osób są dwie mające urodziny tego samego dnia.Niech \\(\\mu\\) będzie miarą \\(\\sigma\\)-skończoną, \\(f_n, f\\) — funkcjami nieujemnymi takimi,\nże miary \\(\\nu_n() = \\int_A f_n\\, d\\mu\\), \\(\\nu() = \\int_A f\\, d\\mu\\) są miarami probabilistycznymi.\nNiech \\(f_n \\f\\) \\(mu\\)-p.w. Udowodnić, że\n\\[\n\\sup_A |\\nu() - \\nu_n()| \\leq \\int_\\Omega |f - f_n|\\, d\\mu \\0.\n\\]\nWywnioskuj, że \\(\\nu_n \\Rightarrow \\nu\\).Niech \\(\\mu\\) będzie miarą \\(\\sigma\\)-skończoną, \\(f_n, f\\) — funkcjami nieujemnymi takimi,\nże miary \\(\\nu_n() = \\int_A f_n\\, d\\mu\\), \\(\\nu() = \\int_A f\\, d\\mu\\) są miarami probabilistycznymi.\nNiech \\(f_n \\f\\) \\(mu\\)-p.w. Udowodnić, że\n\\[\n\\sup_A |\\nu() - \\nu_n()| \\leq \\int_\\Omega |f - f_n|\\, d\\mu \\0.\n\\]\nWywnioskuj, że \\(\\nu_n \\Rightarrow \\nu\\).Niech \\(X_n\\) będzie pierwszą współrzędną zmiennej losowej o rozkładzie\njednostajnym na kuli jednostkowej w \\(\\mathbb{R}^n\\). Wykazać, że\n\\[\n\\sqrt{n} X_n \\Rightarrow \\mathcal{N}(0,1).\n\\]Niech \\(X_n\\) będzie pierwszą współrzędną zmiennej losowej o rozkładzie\njednostajnym na kuli jednostkowej w \\(\\mathbb{R}^n\\). Wykazać, że\n\\[\n\\sqrt{n} X_n \\Rightarrow \\mathcal{N}(0,1).\n\\]Niech \\(\\xi_1, \\xi_2, \\ldots\\) będzie ciągiem niezależnych zmiennych losowych o wspólnym rozkładzie \\(\\mathbb{P}[\\xi_k = \\pm 1] = 1/2\\). Niech \\(S_n = \\sum_{k=1}^n \\xi_k\\). Pokaż, że nie istnieje zmienna losowa \\(\\eta\\) taka, że \\(S_n/\\sqrt{n} \\\\eta\\) według prawdopodobieństwa.Niech \\(\\xi_1, \\xi_2, \\ldots\\) będzie ciągiem niezależnych zmiennych losowych o wspólnym rozkładzie \\(\\mathbb{P}[\\xi_k = \\pm 1] = 1/2\\). Niech \\(S_n = \\sum_{k=1}^n \\xi_k\\). Pokaż, że nie istnieje zmienna losowa \\(\\eta\\) taka, że \\(S_n/\\sqrt{n} \\\\eta\\) według prawdopodobieństwa.","code":""},{"path":"lista-14-powtórka-przed-kolokwium.html","id":"lista-14-powtórka-przed-kolokwium","chapter":"Lista 14: Powtórka przed kolokwium","heading":"Lista 14: Powtórka przed kolokwium","text":"","code":""},{"path":"lista-14-powtórka-przed-kolokwium.html","id":"zadania-do-samodzielnego-rozwiązania-13","chapter":"Lista 14: Powtórka przed kolokwium","heading":"Zadania do samodzielnego rozwiązania","text":"Pokazać, że jeśli \\(\\mathbb{V}ar[X] = \\sigma^2 < \\infty\\), \n\\(\\mathbb{P}[|X - \\mathbb{E}[X]| > n\\sigma] \\leq n^{-2}\\)Pokazać, że jeśli \\(\\mathbb{V}ar[X] = \\sigma^2 < \\infty\\), \n\\(\\mathbb{P}[|X - \\mathbb{E}[X]| > n\\sigma] \\leq n^{-2}\\)Niech \\(X\\), \\(Y\\) będą niezależnymi zmiennymi losowymi o jednakowym\nrozkładzie z dystrybuantą \\(F\\) . Udowodnić, że\n\\(\\mathbb{E}|X − Y | = 2\\int_{-\\infty}^{\\infty} F(t)(1-F(t)) \\: \\mathrm{d}t\\).Niech \\(X\\), \\(Y\\) będą niezależnymi zmiennymi losowymi o jednakowym\nrozkładzie z dystrybuantą \\(F\\) . Udowodnić, że\n\\(\\mathbb{E}|X − Y | = 2\\int_{-\\infty}^{\\infty} F(t)(1-F(t)) \\: \\mathrm{d}t\\).Znaleźć rozkład łączny \\(Y_1 = \\frac{X_1 + X_2}{\\sqrt{2}}, \\, Y_2 = \\frac{X_1 - X_2}{\\sqrt{2}}\\), jeśli:Znaleźć rozkład łączny \\(Y_1 = \\frac{X_1 + X_2}{\\sqrt{2}}, \\, Y_2 = \\frac{X_1 - X_2}{\\sqrt{2}}\\), jeśli:\\((X_1, X_2) \\sim \\mathcal{N} \\left( 0, \\begin{pmatrix} 1 & 0 \\\\ 0 & 4 \\end{pmatrix} \\right)\\),\\((X_1, X_2) \\sim \\mathcal{N} \\left( 0, \\begin{pmatrix} 2 & 1 \\\\ 1 & 4 \\end{pmatrix} \\right)\\).Wykazać, że jeśli \\(X > 0\\) \\(\\mathbb{E}[X] > 0\\), \n\\[\n\\frac{1}{\\mathbb{E}[X]} \\leq \\mathbb{E}\\left[ \\frac{1}{X} \\right].\n\\]Wykazać, że jeśli \\(X > 0\\) \\(\\mathbb{E}[X] > 0\\), \n\\[\n\\frac{1}{\\mathbb{E}[X]} \\leq \\mathbb{E}\\left[ \\frac{1}{X} \\right].\n\\]Niech \\(X, Y\\) będą niezależnymi zmiennymi losowymi. Wykazać, że\n\\[\n\\mathbb{P}[X \\, (X, Y) \\B] = \\int_A \\mathbb{P}((u, Y) \\B)\\, \\mu_X(\\mathrm{d}u).\n\\]Niech \\(X, Y\\) będą niezależnymi zmiennymi losowymi. Wykazać, że\n\\[\n\\mathbb{P}[X \\, (X, Y) \\B] = \\int_A \\mathbb{P}((u, Y) \\B)\\, \\mu_X(\\mathrm{d}u).\n\\]Wyznaczyć rozkład sumy trzech niezależnych zmiennych losowych o rozkładzie \\(\\mathcal{U}[0,1]\\).Wyznaczyć rozkład sumy trzech niezależnych zmiennych losowych o rozkładzie \\(\\mathcal{U}[0,1]\\).Niech \\(U = \\min(X, Y), \\, V = \\max(X, Y) - \\min(X, Y)\\), gdzie \\(X, Y\\) są niezależne\nmają ten sam rozkład wykładniczy z parametrem \\(\\lambda\\).\nWykazać, że \\(U\\) \\(V\\) są niezależne.Niech \\(U = \\min(X, Y), \\, V = \\max(X, Y) - \\min(X, Y)\\), gdzie \\(X, Y\\) są niezależne\nmają ten sam rozkład wykładniczy z parametrem \\(\\lambda\\).\nWykazać, że \\(U\\) \\(V\\) są niezależne.Wykazać, że ciąg \\(\\{X_n\\}_{n \\\\mathbb{N}}\\) jest zbieżny według prawdopodobieństwa pewnej zmiennej losowej \\(X\\) wtedy tylko wtedy, gdy\n\\[\n\\forall \\varepsilon > 0 \\quad \\lim_{n,m \\\\infty} \\mathbb{P}(|X_n - X_m| > \\varepsilon) = 0\n\\]Wykazać, że ciąg \\(\\{X_n\\}_{n \\\\mathbb{N}}\\) jest zbieżny według prawdopodobieństwa pewnej zmiennej losowej \\(X\\) wtedy tylko wtedy, gdy\n\\[\n\\forall \\varepsilon > 0 \\quad \\lim_{n,m \\\\infty} \\mathbb{P}(|X_n - X_m| > \\varepsilon) = 0\n\\](Twierdzenie Pratta)\nUdowodnij, że jeżli \\(X_n, Y_n, Z_n, X, Y, Z\\) są takimi zmiennymi losowymi, że\n\\(X_n \\^\\mathbb{P} X\\), \\(Y_n \\^\\mathbb{P} Y\\), \\(Z_n \\^\\mathbb{P} Z\\)\noraz \\(X_n \\leq Y_n \\leq Z_n\\) p.n. dla wszystkich \\(n\\),\n\\(\\mathbb{E}X_n \\\\mathbb{E}X\\), \\(\\mathbb{E}Z_n \\\\mathbb{E}Z\\),\n\\(\\mathbb{E}Z, \\mathbb{E}X\\) są skończone, wtedy\n\\(\\mathbb{E}Y_n \\\\mathbb{E}Y\\) \\(\\mathbb{E}Y\\) jest skończona.(Twierdzenie Pratta)\nUdowodnij, że jeżli \\(X_n, Y_n, Z_n, X, Y, Z\\) są takimi zmiennymi losowymi, że\n\\(X_n \\^\\mathbb{P} X\\), \\(Y_n \\^\\mathbb{P} Y\\), \\(Z_n \\^\\mathbb{P} Z\\)\noraz \\(X_n \\leq Y_n \\leq Z_n\\) p.n. dla wszystkich \\(n\\),\n\\(\\mathbb{E}X_n \\\\mathbb{E}X\\), \\(\\mathbb{E}Z_n \\\\mathbb{E}Z\\),\n\\(\\mathbb{E}Z, \\mathbb{E}X\\) są skończone, wtedy\n\\(\\mathbb{E}Y_n \\\\mathbb{E}Y\\) \\(\\mathbb{E}Y\\) jest skończona.Udowodnić, że jeśli ciąg zmiennych losowych \\((\\sqrt{X_n})\\) jest zbieżny w \\(L^2\\),\nciąg \\((X_n)\\) jest zbieżny w \\(L^1\\).Udowodnić, że jeśli ciąg zmiennych losowych \\((\\sqrt{X_n})\\) jest zbieżny w \\(L^2\\),\nciąg \\((X_n)\\) jest zbieżny w \\(L^1\\).Niech \\(X_n\\) będą niezależnymi zmiennymi losowymi o jednakowym rozkładzie takim, że\n\\(\\mathbb{E}|X_1| <\\infty\\). Udowodnić, że\n\\[\n\\frac{1}{n} \\max_{\\leq n} |X_i| \\xrightarrow{P} 0.\n\\]Niech \\(X_n\\) będą niezależnymi zmiennymi losowymi o jednakowym rozkładzie takim, że\n\\(\\mathbb{E}|X_1| <\\infty\\). Udowodnić, że\n\\[\n\\frac{1}{n} \\max_{\\leq n} |X_i| \\xrightarrow{P} 0.\n\\]Niech \\((X_n)\\) mają ten sam rozkład: \\(\\mathbb{P}(X_n = 1) =\n\\mathbb{P}(X_n = -1) = \\frac{1}{2}\\).\nUdowodnić, że jeśli \\(\\sum_{n=1}^{\\infty} a_n^2 < \\infty\\), \\(\\sum_{n=1}^{\\infty} a_n X_n\\) jest zbieżny p.n.Niech \\((X_n)\\) mają ten sam rozkład: \\(\\mathbb{P}(X_n = 1) =\n\\mathbb{P}(X_n = -1) = \\frac{1}{2}\\).\nUdowodnić, że jeśli \\(\\sum_{n=1}^{\\infty} a_n^2 < \\infty\\), \\(\\sum_{n=1}^{\\infty} a_n X_n\\) jest zbieżny p.n.Niech \\(\\mathbb{P}(X_n = n) = \\frac{1}{n^3} = \\mathbb{P}(X_n = -n), \\,\n\\mathbb{P}(X_n = 0) = 1 - \\frac{2}{n^3}\\).\nWykazać, że \\(\\sum_{n=1}^{\\infty} X_n\\) jest zbieżny p.n.,\nchociaż \\(\\sum_{n=1}^{\\infty} \\mathbb{V}ar[ X_n] = \\infty\\).Niech \\(\\mathbb{P}(X_n = n) = \\frac{1}{n^3} = \\mathbb{P}(X_n = -n), \\,\n\\mathbb{P}(X_n = 0) = 1 - \\frac{2}{n^3}\\).\nWykazać, że \\(\\sum_{n=1}^{\\infty} X_n\\) jest zbieżny p.n.,\nchociaż \\(\\sum_{n=1}^{\\infty} \\mathbb{V}ar[ X_n] = \\infty\\).Rozważmy niesymetryczne błądzenie losowe. Niech \\(S_n = \\sum_{j=1}^nX_j\\), gdzie zmienne \\(X_j\\) są niezależne takie, że \\(\\mathbb{P}[X_j=1]=p\\)\n\\(\\mathbb{P}[X_j=-1]=1-p\\) dla \\(p\\(0,1)\\). Pokaz, że dla \\(p > \\frac{1}{2}\\),\n\\(\\mathbb{P}\\left[ \\lim S_n = \\infty \\right] = 1\\), dla \\(p < \\frac{1}{2}\\) jest\n\\(\\mathbb{P}\\left[ \\lim S_n = -\\infty \\right] = 1\\).Rozważmy niesymetryczne błądzenie losowe. Niech \\(S_n = \\sum_{j=1}^nX_j\\), gdzie zmienne \\(X_j\\) są niezależne takie, że \\(\\mathbb{P}[X_j=1]=p\\)\n\\(\\mathbb{P}[X_j=-1]=1-p\\) dla \\(p\\(0,1)\\). Pokaz, że dla \\(p > \\frac{1}{2}\\),\n\\(\\mathbb{P}\\left[ \\lim S_n = \\infty \\right] = 1\\), dla \\(p < \\frac{1}{2}\\) jest\n\\(\\mathbb{P}\\left[ \\lim S_n = -\\infty \\right] = 1\\).Niech \\(\\{ X_n \\}_{n=1}^\\infty\\)\nbędzie ciągiem niezależnych zmiennych losowych o jednakowym rozkładzie.\nZnaleźć\n\\[ \\lim_{n \\\\infty} \\sqrt[n]{\\Pi_{=1}^n X_i} \\; , \\]\njeżeli\n\\(X_1\\) ma rozkład jednostajny \\(\\mathcal{U}[0,1]\\);\n\\(X_1\\) ma rozkład o gęstości postaci \\(f(x) = \\frac{1}{2\\sqrt{x}} \\: \\mathbf{1}_{(0,1)}(x)\\).\nNiech \\(\\{ X_n \\}_{n=1}^\\infty\\)\nbędzie ciągiem niezależnych zmiennych losowych o jednakowym rozkładzie.\nZnaleźć\n\\[ \\lim_{n \\\\infty} \\sqrt[n]{\\Pi_{=1}^n X_i} \\; , \\]\njeżeli\\(X_1\\) ma rozkład jednostajny \\(\\mathcal{U}[0,1]\\);\\(X_1\\) ma rozkład o gęstości postaci \\(f(x) = \\frac{1}{2\\sqrt{x}} \\: \\mathbf{1}_{(0,1)}(x)\\).Niech \\(f \\colon [0,1] \\\\mathbb{R}\\) będzie funkcją ciągłą. Obliczyć granice:\n\\[\\begin{align*}\n\\text{)}\\quad & \\lim_{n \\\\infty} \\frac{1}{\\sqrt{n}} \\int_0^1 \\cdots \\int_0^1 \\sqrt{x_1^2 + \\dots + x_n^2} \\, dx_1 \\dots dx_n; \\\\\n\\text{b)}\\quad & \\lim_{n \\\\infty} \\int_0^1 \\cdots \\int_0^1 f\\left( \\frac{x_1 + \\dots + x_n}{n} \\right) \\, dx_1 \\dots dx_n; \\\\\n\\text{c)}\\quad & \\lim_{n \\\\infty} \\int_0^1 \\cdots \\int_0^1 f\\left( \\sqrt[n]{x_1 x_2 \\cdots x_n} \\right) \\, dx_1 \\dots dx_n.\n\\end{align*}\\]Niech \\(f \\colon [0,1] \\\\mathbb{R}\\) będzie funkcją ciągłą. Obliczyć granice:\n\\[\\begin{align*}\n\\text{)}\\quad & \\lim_{n \\\\infty} \\frac{1}{\\sqrt{n}} \\int_0^1 \\cdots \\int_0^1 \\sqrt{x_1^2 + \\dots + x_n^2} \\, dx_1 \\dots dx_n; \\\\\n\\text{b)}\\quad & \\lim_{n \\\\infty} \\int_0^1 \\cdots \\int_0^1 f\\left( \\frac{x_1 + \\dots + x_n}{n} \\right) \\, dx_1 \\dots dx_n; \\\\\n\\text{c)}\\quad & \\lim_{n \\\\infty} \\int_0^1 \\cdots \\int_0^1 f\\left( \\sqrt[n]{x_1 x_2 \\cdots x_n} \\right) \\, dx_1 \\dots dx_n.\n\\end{align*}\\]Zmienne losowe \\(X_1\\) \\(X_2\\) są niezależne mają ten sam rozkład geometryczny\n\\[ \\mathbb{P}[X_j=k] = q^k p,\\]\ngdzie \\(k = 0, 1, 2, \\ldots\\).\nOkreślmy \\(Z\\) jako większy z \\(X_1, X_2\\) (\\(Z = \\max(X_1, X_2)\\)).\nZnaleźć łączny rozkład \\((X_1, Z)\\) oraz rozkład \\(Z\\).Zmienne losowe \\(X_1\\) \\(X_2\\) są niezależne mają ten sam rozkład geometryczny\n\\[ \\mathbb{P}[X_j=k] = q^k p,\\]\ngdzie \\(k = 0, 1, 2, \\ldots\\).\nOkreślmy \\(Z\\) jako większy z \\(X_1, X_2\\) (\\(Z = \\max(X_1, X_2)\\)).\nZnaleźć łączny rozkład \\((X_1, Z)\\) oraz rozkład \\(Z\\).Niech \\(X_1\\) \\(X_2\\) będą dwoma niezależnymi zmiennymi losowymi o rozkładach Poissona\nz parametrami odpowiednio \\(\\lambda_1\\) \\(\\lambda_2\\).\nUdowodnić, że warunkowy rozkład \\(X_1\\) pod warunkiem \\(\\{X_1 + X_2 =n\\}\\) jest dwumianowy\nz parametrami \\(n\\) \\(\\lambda_1/(\\lambda_1+\\lambda_2)\\).Niech \\(X_1\\) \\(X_2\\) będą dwoma niezależnymi zmiennymi losowymi o rozkładach Poissona\nz parametrami odpowiednio \\(\\lambda_1\\) \\(\\lambda_2\\).\nUdowodnić, że warunkowy rozkład \\(X_1\\) pod warunkiem \\(\\{X_1 + X_2 =n\\}\\) jest dwumianowy\nz parametrami \\(n\\) \\(\\lambda_1/(\\lambda_1+\\lambda_2)\\).Niech \\(X_1\\) \\(X_2\\) będą niezależne mają jednakowy rozkład geometryczny\n\\(\\mathbb{P}[X_j=k] = q^k p\\}\\).\nPokazać bez rachunków, że warunkowy rozkład \\(X_1\\)\npod warunkiem \\(\\{ X_1 + X_2 =n \\}\\) jest jednostajny.Niech \\(X_1\\) \\(X_2\\) będą niezależne mają jednakowy rozkład geometryczny\n\\(\\mathbb{P}[X_j=k] = q^k p\\}\\).\nPokazać bez rachunków, że warunkowy rozkład \\(X_1\\)\npod warunkiem \\(\\{ X_1 + X_2 =n \\}\\) jest jednostajny.Niech \\(\\xi_1, \\ldots, \\xi_n\\) będą niezależnymi zmiennymi losowymi o rozkładach wykładniczych z parametrami odpowiednio \\(\\lambda_1, \\ldots, \\lambda_n\\).Niech \\(\\xi_1, \\ldots, \\xi_n\\) będą niezależnymi zmiennymi losowymi o rozkładach wykładniczych z parametrami odpowiednio \\(\\lambda_1, \\ldots, \\lambda_n\\).Udowodnij, że \\(\\mathbb{P} \\{ \\xi_1 < \\xi_2 \\} = \\frac{\\lambda_1}{\\lambda_1 + \\lambda_2}\\).Udowodnij, że \\(\\mathbb{P} \\{ \\xi_1 < \\xi_2 \\} = \\frac{\\lambda_1}{\\lambda_1 + \\lambda_2}\\).Udowodnij, że \\(\\min_{1 \\leq k \\leq n} \\xi_k\\) ma rozkład wykładniczy z parametrem \\(\\lambda = \\sum_{k=1}^n \\lambda_k\\) wywnioskuj z punktu (), że\n\\[\n  \\mathbb{P} \\left\\{ \\xi_j = \\min_{1 \\leq k \\leq n} \\xi_k \\right\\} = \\frac{\\lambda_j}{\\sum_{k=1}^n \\lambda_k}.\n  \\]Udowodnij, że \\(\\min_{1 \\leq k \\leq n} \\xi_k\\) ma rozkład wykładniczy z parametrem \\(\\lambda = \\sum_{k=1}^n \\lambda_k\\) wywnioskuj z punktu (), że\n\\[\n  \\mathbb{P} \\left\\{ \\xi_j = \\min_{1 \\leq k \\leq n} \\xi_k \\right\\} = \\frac{\\lambda_j}{\\sum_{k=1}^n \\lambda_k}.\n  \\]Zakładając, że \\(\\lambda_i \\ne \\lambda_j\\) dla \\(\\ne j\\), znajdź gęstość zmiennej losowej \\(\\xi_1 + \\cdots + \\xi_n\\)\n(dla przypadku \\(n = 2\\), zobacz Problem 2.8.26).Zakładając, że \\(\\lambda_i \\ne \\lambda_j\\) dla \\(\\ne j\\), znajdź gęstość zmiennej losowej \\(\\xi_1 + \\cdots + \\xi_n\\)\n(dla przypadku \\(n = 2\\), zobacz Problem 2.8.26).Udowodnij, że \\(\\mathbb{E} \\min(\\xi_1, \\xi_2) = \\frac{1}{\\lambda_1 + \\lambda_2}\\) znajdź \\(\\mathbb{E} \\max(\\xi_1, \\xi_2)\\).Udowodnij, że \\(\\mathbb{E} \\min(\\xi_1, \\xi_2) = \\frac{1}{\\lambda_1 + \\lambda_2}\\) znajdź \\(\\mathbb{E} \\max(\\xi_1, \\xi_2)\\).Znajdź gęstość rozkładu zmiennej losowej \\(\\xi_1 - \\xi_2\\).Znajdź gęstość rozkładu zmiennej losowej \\(\\xi_1 - \\xi_2\\).Udowodnij, że zmienne losowe \\(\\min(\\xi_1, \\xi_2)\\) oraz \\(\\xi_1 - \\xi_2\\) są niezależne.Udowodnij, że zmienne losowe \\(\\min(\\xi_1, \\xi_2)\\) oraz \\(\\xi_1 - \\xi_2\\) są niezależne.Udowodnić, że jeśli \\(X_n Y_n \\xrightarrow{D} X\\), \\(Y_n \\xrightarrow{D} 0\\), \\(f\\) jest funkcją różniczkowalną w zerze,\n\n\\[\nX_n \\left(f(Y_n) - f(0)\\right) \\xrightarrow{D} f'(0)X.\n\\]Udowodnić, że jeśli \\(X_n Y_n \\xrightarrow{D} X\\), \\(Y_n \\xrightarrow{D} 0\\), \\(f\\) jest funkcją różniczkowalną w zerze,\n\n\\[\nX_n \\left(f(Y_n) - f(0)\\right) \\xrightarrow{D} f'(0)X.\n\\]Niech \\(X_1, X_2, \\ldots\\) będą niezależnymi, nieujemnymi zmiennymi losowymi o takim samym rozkładzie takim, że\n\\[\\begin{equation*}\n\\mathbb{P}[X_k>t] = 1 \\wedge t^{-\\alpha}, \\qquad t >0,\n\\end{equation*}\\]\ndla pewnej \\(\\alpha >0\\). Rozważmy \\(M_n = \\max \\{ X_1, \\ldots, X_n \\}\\).\nPokaż, że ciąg zmiennych losowych \\(n^{-1/\\alpha}M_n\\) jest zbieżny według rozkładu.Niech \\(X_1, X_2, \\ldots\\) będą niezależnymi, nieujemnymi zmiennymi losowymi o takim samym rozkładzie takim, że\n\\[\\begin{equation*}\n\\mathbb{P}[X_k>t] = 1 \\wedge t^{-\\alpha}, \\qquad t >0,\n\\end{equation*}\\]\ndla pewnej \\(\\alpha >0\\). Rozważmy \\(M_n = \\max \\{ X_1, \\ldots, X_n \\}\\).\nPokaż, że ciąg zmiennych losowych \\(n^{-1/\\alpha}M_n\\) jest zbieżny według rozkładu.","code":""},{"path":"twierdzenie-fubiniego.html","id":"twierdzenie-fubiniego","chapter":"Twierdzenie Fubiniego","heading":"Twierdzenie Fubiniego","text":"Przedstawimy pokrótce elementarne własności przykłady miar produktowych.\nNastępnie zobaczymy kilka zastosowań twierdzenia Fubiniego.\nWszystkie twierdzenia będą przytoczone bez dowodów.","code":""},{"path":"twierdzenie-fubiniego.html","id":"miary-sigma-skończone","chapter":"Twierdzenie Fubiniego","heading":"26.1 Miary \\(\\sigma\\)-skończone","text":"Rozważać będziemy przestrzeń miarową \\((, \\mathcal{}, \\mu)\\).\nPrzypomnijmy, że \\(\\mathcal{}\\) jest \\(\\sigma\\)-ciałem podzbiorów \\(\\) oraz \\(\\mu \\colon \\mathcal{} \\[0, +\\infty]\\)\njest funkcją przeliczalnie addytywną. Dokładniej, dla każdych parami rozłącznych \\(\\{A_n\\}_{n \\\\mathbb{N}}\\) z \\(\\mathcal{}\\),\n\\[\\begin{equation*}\n    \\mu \\left( \\bigcup_{n \\\\mathbb{N}} A_n \\right) = \\sum_{n \\\\mathbb{N}} \\mu(A_n).\n\\end{equation*}\\]\nBędziemy zakładać, że przestrzeń jest \\(\\sigma\\)-skończona. Oznacza , że istnieją zbiory \\(\\{A_n\\}_{n \\\\mathbb{N}}\\) w \\(\\mathcal{}\\)\ntakie, że\n\\[\\begin{equation*}\n    \\bigcup_{n \\\\mathbb{N}} A_n = \n\\end{equation*}\\]\noraz \\(\\mu(A_n) <\\infty\\) dla każdego \\(n \\\\mathbb{N}\\).Przykład 26.1  Rozważmy \\((\\mathbb{N}, 2^{\\mathbb{N}}, \\#)\\), gdzie \\(\\#\\) jest miarą liczącą,\n\\[\\begin{equation*}\n    \\# = \\mbox{liczba elementów w $$}.\n\\end{equation*}\\]\nDla \\(A_n = \\{n\\}\\) mamy\n\\[\\begin{equation*}\n    \\bigcup_{n \\\\mathbb{N}} \\{n\\} = \\mathbb{N}\n\\end{equation*}\\]\noraz \\(\\# A_n =1\\). Zbiór liczb naturalnych z miarą liczącą jest zatem \\(\\sigma\\)-skończony.Przykład 26.2  Rozważmy przestrzeń \\(\\mathbb{R}, \\mathcal{B}(\\mathbb{R}), \\lambda_1)\\).\nRozważmy \\(A_n = [-n, n]\\). Wówczas\n\\[\\begin{equation*}\n    \\bigcup_{n \\\\mathbb{N}} [-n,n] = \\mathbb{R}\n\\end{equation*}\\]\noraz \\(\\lambda_1([-n,n]) = 2n <\\infty\\). Prosta rzeczywista z miarą Lebesgue’jest zatem \\(\\sigma\\)-skończona.Przykład 26.3  Rozważmy \\((\\mathbb{R}, 2^{\\mathbb{R}}, \\mu_\\mathbb{Q})\\), gdzie miara \\(\\mu_{\\mathbb{Q}}\\) jest zadana przez\n\\[\\begin{equation*}\n    \\mu_\\mathbb{Q}() = \\# \\cap \\mathbb{Q}\n\\end{equation*}\\]\nliczbę liczb wymiernych w \\(\\). Zauważmy, że dla każdego otwartego przedziału \\((,b)\\),\n\\[\\begin{equation*}\n\\mu_\\mathbb{Q}(,b) = \\infty.\n\\end{equation*}\\]\nJednak przestrzeń ta jest \\(\\sigma\\)-skończona.\nNiech \\(A_1 = \\mathbb{R}\\setminus \\mathbb{Q}\\) będzie zbiorem liczb niewymiernych.\nWówczas \\(\\mu_{\\mathbb{Q}}(A_0)=0\\). Ustawmy liczby wymierne w ciąg \\(\\mathbb{Q} = \\{q_2, q_3, \\ldots\\}\\) połóżmy\n\\(A_n = \\{q_n\\}\\). Wtedy \\(\\mu_{\\mathbb{Q}}(A_n) = 1\\) dla \\(n \\geq 2\\).\nZbiory \\(\\{A_n\\}_n\\) są świadkami na , że rozważana przestrzeń jest \\(\\sigma\\)-skończona.Przykład 26.4  Rozważmy modyfikację ostatniej przestrzeni \\((\\mathbb{R}, 2^{\\mathbb{R}}, \\mu_{\\mathbb{R}\\setminus \\mathbb{Q}})\\), gdzie miara \\(\\mu_{\\mathbb{R}\\setminus \\mathbb{Q}}\\) jest zadana przez\n\\[\\begin{equation*}\n    \\mu_{ \\mathbb{R}\\setminus \\mathbb{Q}}() = \\# \\cap (\\mathbb{R}\\setminus \\mathbb{Q})\n\\end{equation*}\\]\njest liczbą liczb niewymiernych w \\(\\). Taka przestrzeń nie jest \\(\\sigma\\)-skończona. Załóżmy nie wprost, że\n\\[\n\\bigcup_{n \\\\mathbb{N}} A_n = \\mathbb{R},\n\\]\ngdzie każdy \\(A_n\\) zawiera skończenie wiele liczb wymiernych. Wówczas\n\\[\\begin{equation*}\n\\mathbb{R} \\setminus \\mathbb{Q} = \\bigcup_{n \\\\mathbb{N}} A_n \\cap (\\mathbb{R}\\setminus \\mathbb{Q}).\n\\end{equation*}\\]\nSkoro każdy ze zbiorów \\(A_n \\cap(\\mathbb{R} \\setminus \\mathbb{Q})\\) jest skończony, przeliczalna suma zbiorów po prawej stronie jest przeliczalna.\nOznacza , że \\(\\mathbb{R}\\setminus \\mathbb{Q}\\) jest przeliczalny. Sprzeczność pokazuje, że rozważana przez nasz przestrzeń nie jest \\(\\sigma\\)-skończona.","code":""},{"path":"twierdzenie-fubiniego.html","id":"miary-produktowe","chapter":"Twierdzenie Fubiniego","heading":"26.2 Miary produktowe","text":"Rozważmy teraz dwie \\(\\sigma\\)-skończone przestrzenie miarowe \\((, \\mathcal{}, \\mu)\\) oraz \\((B, \\mathcal{B}, \\nu)\\).\nNiech \\(\\Omega = \\times B\\) niech\n\\[\\begin{equation*}\n    \\mathcal{S} = \\left\\{ \\times B \\: : \\: \\\\mathcal{}, \\: B \\\\mathcal{B} \\right\\}.\n\\end{equation*}\\]\nWówczas \\(\\mathcal{S}\\) jest zbiorem prostokątów o bokach w \\(\\mathcal{}\\) \\(\\mathcal{B}\\).\nRodzina ta jest zamknięta na przekroje\n\\[\\begin{equation*}\n    (\\times B) \\cap (C \\times D) = (\\cap C) \\times (B \\cap D).\n\\end{equation*}\\]Dopełnienie prostokąta jest sumą prostokątów.Niech \\(\\mathcal{F}\\) będzie najmniejszym \\(\\sigma\\)-ciałem podzbiorów \\(\\Omega\\) zawierającym \\(\\mathcal{S}\\).Twierdzenie 26.1  Załóżmy, że przestrzenie miarowe \\((, \\mathcal{}, \\mu)\\) oraz \\((B, \\mathcal{B}, \\nu)\\) są \\(\\sigma\\)-skończone.\nWówczas istnieje dokładnie jedna miary \\(\\eta\\) na \\((\\Omega, \\mathcal{F})\\) taka, że\n\\[\\begin{equation*}\n    \\eta(\\times B) = \\mu() \\cdot \\nu(B).\n\\end{equation*}\\]\nBędziemy korzystać z oznaczenia \\(\\mu\\otimes \\nu = \\eta\\).Przykład 26.5  Rozważmy dwie kopie liczb naturalnych wraz z miarami liczącymi. Dokładniej niech\n\\((, \\mathcal{}, \\mu)= (B, \\mathcal{B}, \\nu) = (\\mathbb{N}, 2^{\\mathbb{N}}, \\#)\\),\ngdzie \\(\\#\\) jest miarą liczącą. Wówczas \\(\\Omega = \\mathbb{N}\\times \\mathbb{N}\\) jest kratą liczb naturalnych.\nJak wygląda \\(\\mu \\otimes \\nu\\)?\nDla każdych \\(, B \\subseteq \\mathbb{N}\\), mamy\n\\[\\begin{equation*}\n\\mu\\otimes \\nu(\\times B) = \\mu() \\cdot \\nu(B) = \\# () \\cdot \\# (B)\n\\end{equation*}\\]\nZauważmy, że\n\\[\\begin{equation*}\n    \\# () \\cdot \\# (B) = \\# (\\times B).\n\\end{equation*}\\]\nW tym przypadku \\(\\mu\\otimes \\nu\\) jest po prostu miarą liczącą na produkcie \\(\\mathbb{N}^2\\).Przykład 26.6  Niech \\((, \\mathcal{}, \\mu) = (B, \\mathcal{B}, \\nu) = (\\mathbb{R}, \\mathcal{B}(\\mathbb{R}), \\lambda_1)\\).\nJak wygląda \\(\\lambda_1 \\otimes \\lambda1\\) na \\(\\mathbb{R}^2\\)? Dla dowolnych borelowskich \\(\\), \\(B\\)\n\\[\\begin{equation*}\n\\lambda_1\\otimes \\lambda_1(\\times B) = \\lambda_1() \\cdot \\lambda_1(B).\n\\end{equation*}\\]\nZauważmy, że\n\\[\\begin{equation*}\n    \\lambda_1 () \\cdot \\lambda_1 (B) = \\lambda_2(\\times B),\n\\end{equation*}\\]\ngdzie \\(\\lambda_2\\) jest dwuwymiarową miarą Lebesgue’.\nCzyli \\(\\lambda_1\\otimes \\lambda_1 = \\lambda_2\\).Przykład 26.7  Niech \\((, \\mathcal{}, \\mu) = (\\mathbb{R}, \\mathcal{B}(\\mathbb{R}), \\lambda_1)\\).\n\\((B, \\mathcal{B}, \\nu) = (\\mathbb{Z}, 2^{\\mathbb{Z}}, \\#)\\). Wówczas \\(\\Omega = \\mathbb{R} \\times \\mathbb{Z} \\subseteq \\mathbb{R}^2\\)\njest nieskończoną kolekcją poziomych prostych. Wówczas każdy prostokąt postaci $ {k}$ jest kopią \\(\\)\nna poziomie \\(k\\). May\n\\[\\begin{equation*}\n    \\mu \\otimes \\nu(\\times \\{ k\\}) = \\lambda_1().\n\\end{equation*}\\]\nJeżeli więc rozważymy okrąg\n\\[\\begin{equation*}\n    C = \\{ ( x, j) \\\\mathbb{R} \\times \\mathbb{Z} \\: : \\:  x^2+j^2\\leq 10 \\}\n\\end{equation*}\\]\n\n\\[\\begin{equation*}\n\\mu \\otimes \\nu(C) = \\sum_{j=-3}^3 \\lambda_1( x \\: : \\: x^2 \\leq 10-j^2 ) = 16+4\\sqrt{6} +2 \\sqrt{10}\n\\end{equation*}\\]","code":""},{"path":"twierdzenie-fubiniego.html","id":"całki-względem-miar-produktowych","chapter":"Twierdzenie Fubiniego","heading":"26.3 Całki względem miar produktowych","text":"Twierdzenie 26.2  Jeżeli \\(f\\colon \\times B \\\\mathbb{R}\\) jest taka, że\n\\(f \\geq 0\\) lub \\(\\int_{\\times B} f \\mathrm{d}\\mu\\otimes \\nu <\\infty\\), \n\\[\\begin{equation*}\n\\int_A \\int_B f(x,y) \\nu(\\mathrm{d}y) \\mu(\\mathrm{d}x) =\n\\int_B \\int_A f(x,y) \\mu(\\mathrm{d}x) \\nu(\\mathrm{d}y) =\n\\int_{\\times B} f   \\mathrm{d} \\mu \\otimes \\nu.\n\\end{equation*}\\]","code":""}]
